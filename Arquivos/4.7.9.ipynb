{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -938.00, Win Rate: 0.52, Wins: 1472, Losses: 1351, Epsilon: 0.4950, Steps: 36754, Time: 156.09s\n",
      "Ações: Manter=12015, Comprar=12729, Vender=12010\n",
      "Ganhos Totais: 38386.75, Perdas Totais: -39324.75\n",
      "Modelo e log do episódio 1 salvos em: 4.7.9\\model_episode_1.pth e 4.7.9\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -3085.00, Win Rate: 0.52, Wins: 1567, Losses: 1433, Epsilon: 0.4900, Steps: 36754, Time: 150.50s\n",
      "Ações: Manter=10448, Comprar=12507, Vender=13799\n",
      "Ganhos Totais: 38466.00, Perdas Totais: -41551.00\n",
      "Modelo e log do episódio 2 salvos em: 4.7.9\\model_episode_2.pth e 4.7.9\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: 2629.00, Win Rate: 0.54, Wins: 1591, Losses: 1337, Epsilon: 0.4851, Steps: 36754, Time: 143.56s\n",
      "Ações: Manter=11666, Comprar=12113, Vender=12975\n",
      "Ganhos Totais: 39459.25, Perdas Totais: -36830.25\n",
      "Modelo e log do episódio 3 salvos em: 4.7.9\\model_episode_3.pth e 4.7.9\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 185.25, Win Rate: 0.54, Wins: 1496, Losses: 1298, Epsilon: 0.4803, Steps: 36754, Time: 170.55s\n",
      "Ações: Manter=11801, Comprar=12662, Vender=12291\n",
      "Ganhos Totais: 38790.75, Perdas Totais: -38605.50\n",
      "Modelo e log do episódio 4 salvos em: 4.7.9\\model_episode_4.pth e 4.7.9\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -1330.00, Win Rate: 0.53, Wins: 1440, Losses: 1294, Epsilon: 0.4755, Steps: 36754, Time: 167.53s\n",
      "Ações: Manter=12307, Comprar=12427, Vender=12020\n",
      "Ganhos Totais: 38149.25, Perdas Totais: -39479.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.9\\model_episode_5.pth e 4.7.9\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -4355.25, Win Rate: 0.52, Wins: 1449, Losses: 1340, Epsilon: 0.4707, Steps: 36754, Time: 169.39s\n",
      "Ações: Manter=11966, Comprar=11917, Vender=12871\n",
      "Ganhos Totais: 36922.75, Perdas Totais: -41278.00\n",
      "Modelo e log do episódio 6 salvos em: 4.7.9\\model_episode_6.pth e 4.7.9\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -705.25, Win Rate: 0.52, Wins: 1404, Losses: 1276, Epsilon: 0.4660, Steps: 36754, Time: 173.78s\n",
      "Ações: Manter=12171, Comprar=12446, Vender=12137\n",
      "Ganhos Totais: 37229.50, Perdas Totais: -37934.75\n",
      "Modelo e log do episódio 7 salvos em: 4.7.9\\model_episode_7.pth e 4.7.9\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -2053.00, Win Rate: 0.51, Wins: 1424, Losses: 1384, Epsilon: 0.4614, Steps: 36754, Time: 175.46s\n",
      "Ações: Manter=11895, Comprar=12074, Vender=12785\n",
      "Ganhos Totais: 37531.50, Perdas Totais: -39584.50\n",
      "Modelo e log do episódio 8 salvos em: 4.7.9\\model_episode_8.pth e 4.7.9\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -3306.00, Win Rate: 0.53, Wins: 1456, Losses: 1306, Epsilon: 0.4568, Steps: 36754, Time: 172.47s\n",
      "Ações: Manter=12093, Comprar=12256, Vender=12405\n",
      "Ganhos Totais: 36068.50, Perdas Totais: -39374.50\n",
      "Modelo e log do episódio 9 salvos em: 4.7.9\\model_episode_9.pth e 4.7.9\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -268.00, Win Rate: 0.53, Wins: 1428, Losses: 1257, Epsilon: 0.4522, Steps: 36754, Time: 174.35s\n",
      "Ações: Manter=12776, Comprar=11920, Vender=12058\n",
      "Ganhos Totais: 37517.75, Perdas Totais: -37785.75\n",
      "Modelo e log do episódio 10 salvos em: 4.7.9\\model_episode_10.pth e 4.7.9\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 1184.25, Win Rate: 0.54, Wins: 1534, Losses: 1302, Epsilon: 0.4477, Steps: 36754, Time: 181.36s\n",
      "Ações: Manter=12578, Comprar=11727, Vender=12449\n",
      "Ganhos Totais: 39386.75, Perdas Totais: -38202.50\n",
      "Modelo e log do episódio 11 salvos em: 4.7.9\\model_episode_11.pth e 4.7.9\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 3915.25, Win Rate: 0.55, Wins: 1593, Losses: 1280, Epsilon: 0.4432, Steps: 36754, Time: 168.42s\n",
      "Ações: Manter=11593, Comprar=12487, Vender=12674\n",
      "Ganhos Totais: 40372.25, Perdas Totais: -36457.00\n",
      "Modelo e log do episódio 12 salvos em: 4.7.9\\model_episode_12.pth e 4.7.9\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 1445.75, Win Rate: 0.51, Wins: 1412, Losses: 1340, Epsilon: 0.4388, Steps: 36754, Time: 174.46s\n",
      "Ações: Manter=12010, Comprar=12329, Vender=12415\n",
      "Ganhos Totais: 39219.75, Perdas Totais: -37774.00\n",
      "Modelo e log do episódio 13 salvos em: 4.7.9\\model_episode_13.pth e 4.7.9\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -687.25, Win Rate: 0.54, Wins: 1494, Losses: 1271, Epsilon: 0.4344, Steps: 36754, Time: 177.64s\n",
      "Ações: Manter=12613, Comprar=12156, Vender=11985\n",
      "Ganhos Totais: 37403.00, Perdas Totais: -38090.25\n",
      "Modelo e log do episódio 14 salvos em: 4.7.9\\model_episode_14.pth e 4.7.9\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: 513.00, Win Rate: 0.54, Wins: 1420, Losses: 1210, Epsilon: 0.4300, Steps: 36754, Time: 167.10s\n",
      "Ações: Manter=12178, Comprar=12351, Vender=12225\n",
      "Ganhos Totais: 37806.25, Perdas Totais: -37293.25\n",
      "Modelo e log do episódio 15 salvos em: 4.7.9\\model_episode_15.pth e 4.7.9\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: 1065.50, Win Rate: 0.54, Wins: 1417, Losses: 1213, Epsilon: 0.4257, Steps: 36754, Time: 167.09s\n",
      "Ações: Manter=12622, Comprar=12763, Vender=11369\n",
      "Ganhos Totais: 37703.50, Perdas Totais: -36638.00\n",
      "Modelo e log do episódio 16 salvos em: 4.7.9\\model_episode_16.pth e 4.7.9\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -1532.25, Win Rate: 0.52, Wins: 1396, Losses: 1266, Epsilon: 0.4215, Steps: 36754, Time: 166.87s\n",
      "Ações: Manter=12412, Comprar=12199, Vender=12143\n",
      "Ganhos Totais: 37602.00, Perdas Totais: -39134.25\n",
      "Episode 18/100, Total Reward: -1880.25, Win Rate: 0.54, Wins: 1414, Losses: 1201, Epsilon: 0.4173, Steps: 36754, Time: 167.11s\n",
      "Ações: Manter=12308, Comprar=12117, Vender=12329\n",
      "Ganhos Totais: 36982.75, Perdas Totais: -38863.00\n",
      "Episode 19/100, Total Reward: -1567.75, Win Rate: 0.53, Wins: 1409, Losses: 1244, Epsilon: 0.4131, Steps: 36754, Time: 166.87s\n",
      "Ações: Manter=12896, Comprar=12603, Vender=11255\n",
      "Ganhos Totais: 37143.00, Perdas Totais: -38710.75\n",
      "Episode 20/100, Total Reward: -1378.25, Win Rate: 0.54, Wins: 1419, Losses: 1218, Epsilon: 0.4090, Steps: 36754, Time: 167.45s\n",
      "Ações: Manter=12756, Comprar=12521, Vender=11477\n",
      "Ganhos Totais: 36219.75, Perdas Totais: -37598.00\n",
      "Episode 21/100, Total Reward: 775.00, Win Rate: 0.53, Wins: 1377, Losses: 1199, Epsilon: 0.4049, Steps: 36754, Time: 167.18s\n",
      "Ações: Manter=13959, Comprar=11900, Vender=10895\n",
      "Ganhos Totais: 36803.75, Perdas Totais: -36028.75\n",
      "Modelo e log do episódio 21 salvos em: 4.7.9\\model_episode_21.pth e 4.7.9\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: 3810.25, Win Rate: 0.55, Wins: 1559, Losses: 1278, Epsilon: 0.4008, Steps: 36754, Time: 167.82s\n",
      "Ações: Manter=12210, Comprar=11441, Vender=13103\n",
      "Ganhos Totais: 39060.75, Perdas Totais: -35250.50\n",
      "Modelo e log do episódio 22 salvos em: 4.7.9\\model_episode_22.pth e 4.7.9\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: -3254.00, Win Rate: 0.53, Wins: 1395, Losses: 1243, Epsilon: 0.3968, Steps: 36754, Time: 167.67s\n",
      "Ações: Manter=13153, Comprar=11663, Vender=11938\n",
      "Ganhos Totais: 35897.75, Perdas Totais: -39151.75\n",
      "Episode 24/100, Total Reward: 2091.50, Win Rate: 0.54, Wins: 1469, Losses: 1239, Epsilon: 0.3928, Steps: 36754, Time: 167.69s\n",
      "Ações: Manter=12776, Comprar=11826, Vender=12152\n",
      "Ganhos Totais: 38592.50, Perdas Totais: -36501.00\n",
      "Modelo e log do episódio 24 salvos em: 4.7.9\\model_episode_24.pth e 4.7.9\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 4910.75, Win Rate: 0.55, Wins: 1445, Losses: 1170, Epsilon: 0.3889, Steps: 36754, Time: 168.20s\n",
      "Ações: Manter=12631, Comprar=13075, Vender=11048\n",
      "Ganhos Totais: 40078.00, Perdas Totais: -35167.25\n",
      "Modelo e log do episódio 25 salvos em: 4.7.9\\model_episode_25.pth e 4.7.9\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: 481.00, Win Rate: 0.54, Wins: 1429, Losses: 1204, Epsilon: 0.3850, Steps: 36754, Time: 167.91s\n",
      "Ações: Manter=12826, Comprar=12179, Vender=11749\n",
      "Ganhos Totais: 38538.50, Perdas Totais: -38057.50\n",
      "Episode 27/100, Total Reward: 181.50, Win Rate: 0.53, Wins: 1335, Losses: 1196, Epsilon: 0.3812, Steps: 36754, Time: 168.90s\n",
      "Ações: Manter=13619, Comprar=12008, Vender=11127\n",
      "Ganhos Totais: 37341.25, Perdas Totais: -37159.75\n",
      "Episode 28/100, Total Reward: -1227.25, Win Rate: 0.52, Wins: 1304, Losses: 1200, Epsilon: 0.3774, Steps: 36754, Time: 167.92s\n",
      "Ações: Manter=13605, Comprar=10665, Vender=12484\n",
      "Ganhos Totais: 35232.75, Perdas Totais: -36460.00\n",
      "Episode 29/100, Total Reward: -809.50, Win Rate: 0.52, Wins: 1317, Losses: 1205, Epsilon: 0.3736, Steps: 36754, Time: 168.81s\n",
      "Ações: Manter=14105, Comprar=11029, Vender=11620\n",
      "Ganhos Totais: 36512.75, Perdas Totais: -37322.25\n",
      "Episode 30/100, Total Reward: 2286.75, Win Rate: 0.54, Wins: 1403, Losses: 1218, Epsilon: 0.3699, Steps: 36754, Time: 167.89s\n",
      "Ações: Manter=12571, Comprar=11767, Vender=12416\n",
      "Ganhos Totais: 38877.25, Perdas Totais: -36590.50\n",
      "Modelo e log do episódio 30 salvos em: 4.7.9\\model_episode_30.pth e 4.7.9\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -1680.25, Win Rate: 0.53, Wins: 1371, Losses: 1222, Epsilon: 0.3662, Steps: 36754, Time: 168.87s\n",
      "Ações: Manter=11625, Comprar=11524, Vender=13605\n",
      "Ganhos Totais: 36581.25, Perdas Totais: -38261.50\n",
      "Episode 32/100, Total Reward: 1010.00, Win Rate: 0.53, Wins: 1427, Losses: 1245, Epsilon: 0.3625, Steps: 36754, Time: 175.86s\n",
      "Ações: Manter=13037, Comprar=12242, Vender=11475\n",
      "Ganhos Totais: 36791.25, Perdas Totais: -35781.25\n",
      "Modelo e log do episódio 32 salvos em: 4.7.9\\model_episode_32.pth e 4.7.9\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: -849.50, Win Rate: 0.52, Wins: 1372, Losses: 1266, Epsilon: 0.3589, Steps: 36754, Time: 157.96s\n",
      "Ações: Manter=13099, Comprar=11194, Vender=12461\n",
      "Ganhos Totais: 36647.25, Perdas Totais: -37496.75\n",
      "Episode 34/100, Total Reward: -1198.25, Win Rate: 0.53, Wins: 1309, Losses: 1141, Epsilon: 0.3553, Steps: 36754, Time: 146.45s\n",
      "Ações: Manter=13075, Comprar=12069, Vender=11610\n",
      "Ganhos Totais: 35242.00, Perdas Totais: -36440.25\n",
      "Episode 35/100, Total Reward: -3272.25, Win Rate: 0.52, Wins: 1329, Losses: 1242, Epsilon: 0.3517, Steps: 36754, Time: 146.46s\n",
      "Ações: Manter=11732, Comprar=12968, Vender=12054\n",
      "Ganhos Totais: 34942.75, Perdas Totais: -38215.00\n",
      "Episode 36/100, Total Reward: -367.75, Win Rate: 0.53, Wins: 1342, Losses: 1182, Epsilon: 0.3482, Steps: 36754, Time: 146.07s\n",
      "Ações: Manter=13203, Comprar=11543, Vender=12008\n",
      "Ganhos Totais: 36421.25, Perdas Totais: -36789.00\n",
      "Episode 37/100, Total Reward: -744.50, Win Rate: 0.53, Wins: 1284, Losses: 1131, Epsilon: 0.3447, Steps: 36754, Time: 145.91s\n",
      "Ações: Manter=13124, Comprar=11369, Vender=12261\n",
      "Ganhos Totais: 34244.00, Perdas Totais: -34988.50\n",
      "Episode 38/100, Total Reward: -3303.75, Win Rate: 0.54, Wins: 1338, Losses: 1128, Epsilon: 0.3413, Steps: 36754, Time: 145.84s\n",
      "Ações: Manter=12984, Comprar=10882, Vender=12888\n",
      "Ganhos Totais: 34616.50, Perdas Totais: -37920.25\n",
      "Episode 39/100, Total Reward: -2140.00, Win Rate: 0.53, Wins: 1331, Losses: 1163, Epsilon: 0.3379, Steps: 36754, Time: 146.26s\n",
      "Ações: Manter=13134, Comprar=11478, Vender=12142\n",
      "Ganhos Totais: 35484.25, Perdas Totais: -37624.25\n",
      "Episode 40/100, Total Reward: -3411.50, Win Rate: 0.52, Wins: 1272, Losses: 1159, Epsilon: 0.3345, Steps: 36754, Time: 146.10s\n",
      "Ações: Manter=13121, Comprar=12385, Vender=11248\n",
      "Ganhos Totais: 33519.75, Perdas Totais: -36931.25\n",
      "Episode 41/100, Total Reward: -787.50, Win Rate: 0.55, Wins: 1343, Losses: 1095, Epsilon: 0.3311, Steps: 36754, Time: 146.01s\n",
      "Ações: Manter=14167, Comprar=11359, Vender=11228\n",
      "Ganhos Totais: 35555.00, Perdas Totais: -36342.50\n",
      "Episode 42/100, Total Reward: -157.50, Win Rate: 0.55, Wins: 1263, Losses: 1040, Epsilon: 0.3278, Steps: 36754, Time: 145.84s\n",
      "Ações: Manter=12634, Comprar=13210, Vender=10910\n",
      "Ganhos Totais: 36455.50, Perdas Totais: -36613.00\n",
      "Episode 43/100, Total Reward: -2513.25, Win Rate: 0.54, Wins: 1276, Losses: 1091, Epsilon: 0.3246, Steps: 36754, Time: 145.89s\n",
      "Ações: Manter=11823, Comprar=14488, Vender=10443\n",
      "Ganhos Totais: 35342.00, Perdas Totais: -37855.25\n",
      "Episode 44/100, Total Reward: 1323.25, Win Rate: 0.52, Wins: 1223, Losses: 1109, Epsilon: 0.3213, Steps: 36754, Time: 146.20s\n",
      "Ações: Manter=12282, Comprar=12839, Vender=11633\n",
      "Ganhos Totais: 36116.00, Perdas Totais: -34792.75\n",
      "Modelo e log do episódio 44 salvos em: 4.7.9\\model_episode_44.pth e 4.7.9\\log_episode_44.csv\n",
      "\n",
      "Episode 45/100, Total Reward: 1556.75, Win Rate: 0.55, Wins: 1291, Losses: 1065, Epsilon: 0.3181, Steps: 36754, Time: 145.84s\n",
      "Ações: Manter=12926, Comprar=12002, Vender=11826\n",
      "Ganhos Totais: 37697.75, Perdas Totais: -36141.00\n",
      "Modelo e log do episódio 45 salvos em: 4.7.9\\model_episode_45.pth e 4.7.9\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: -563.50, Win Rate: 0.54, Wins: 1185, Losses: 1013, Epsilon: 0.3149, Steps: 36754, Time: 146.20s\n",
      "Ações: Manter=12421, Comprar=11475, Vender=12858\n",
      "Ganhos Totais: 34769.00, Perdas Totais: -35332.50\n",
      "Episode 47/100, Total Reward: 40.50, Win Rate: 0.55, Wins: 1231, Losses: 1022, Epsilon: 0.3118, Steps: 36754, Time: 146.22s\n",
      "Ações: Manter=12353, Comprar=12128, Vender=12273\n",
      "Ganhos Totais: 36405.00, Perdas Totais: -36364.50\n",
      "Episode 48/100, Total Reward: -5179.75, Win Rate: 0.54, Wins: 1230, Losses: 1061, Epsilon: 0.3086, Steps: 36754, Time: 146.23s\n",
      "Ações: Manter=11996, Comprar=11185, Vender=13573\n",
      "Ganhos Totais: 33313.00, Perdas Totais: -38492.75\n",
      "Episode 49/100, Total Reward: -3691.00, Win Rate: 0.53, Wins: 1136, Losses: 1004, Epsilon: 0.3056, Steps: 36754, Time: 146.25s\n",
      "Ações: Manter=13328, Comprar=12371, Vender=11055\n",
      "Ganhos Totais: 33279.00, Perdas Totais: -36970.00\n",
      "Episode 50/100, Total Reward: -3727.25, Win Rate: 0.53, Wins: 1148, Losses: 1025, Epsilon: 0.3025, Steps: 36754, Time: 146.16s\n",
      "Ações: Manter=13152, Comprar=11094, Vender=12508\n",
      "Ganhos Totais: 33487.00, Perdas Totais: -37214.25\n",
      "Episode 51/100, Total Reward: -3947.50, Win Rate: 0.53, Wins: 1128, Losses: 987, Epsilon: 0.2995, Steps: 36754, Time: 147.52s\n",
      "Ações: Manter=12125, Comprar=12251, Vender=12378\n",
      "Ganhos Totais: 33682.25, Perdas Totais: -37629.75\n",
      "Episode 52/100, Total Reward: 531.00, Win Rate: 0.54, Wins: 1097, Losses: 942, Epsilon: 0.2965, Steps: 36754, Time: 146.71s\n",
      "Ações: Manter=12955, Comprar=11205, Vender=12594\n",
      "Ganhos Totais: 35352.00, Perdas Totais: -34821.00\n",
      "Episode 53/100, Total Reward: 723.50, Win Rate: 0.55, Wins: 1117, Losses: 928, Epsilon: 0.2935, Steps: 36754, Time: 146.70s\n",
      "Ações: Manter=14490, Comprar=11184, Vender=11080\n",
      "Ganhos Totais: 35014.50, Perdas Totais: -34291.00\n",
      "Episode 54/100, Total Reward: 510.00, Win Rate: 0.53, Wins: 1090, Losses: 954, Epsilon: 0.2906, Steps: 36754, Time: 146.57s\n",
      "Ações: Manter=13420, Comprar=12653, Vender=10681\n",
      "Ganhos Totais: 34875.00, Perdas Totais: -34365.00\n",
      "Episode 55/100, Total Reward: -1015.75, Win Rate: 0.54, Wins: 1029, Losses: 894, Epsilon: 0.2877, Steps: 36754, Time: 146.87s\n",
      "Ações: Manter=13789, Comprar=13184, Vender=9781\n",
      "Ganhos Totais: 32163.75, Perdas Totais: -33179.50\n",
      "Episode 56/100, Total Reward: -1697.50, Win Rate: 0.52, Wins: 1055, Losses: 973, Epsilon: 0.2848, Steps: 36754, Time: 147.07s\n",
      "Ações: Manter=12254, Comprar=12749, Vender=11751\n",
      "Ganhos Totais: 32919.75, Perdas Totais: -34617.25\n",
      "Episode 57/100, Total Reward: 2960.50, Win Rate: 0.55, Wins: 1067, Losses: 885, Epsilon: 0.2820, Steps: 36754, Time: 146.61s\n",
      "Ações: Manter=12993, Comprar=12924, Vender=10837\n",
      "Ganhos Totais: 35532.50, Perdas Totais: -32572.00\n",
      "Modelo e log do episódio 57 salvos em: 4.7.9\\model_episode_57.pth e 4.7.9\\log_episode_57.csv\n",
      "\n",
      "Episode 58/100, Total Reward: 1482.00, Win Rate: 0.55, Wins: 1110, Losses: 895, Epsilon: 0.2791, Steps: 36754, Time: 146.48s\n",
      "Ações: Manter=14572, Comprar=11820, Vender=10362\n",
      "Ganhos Totais: 35314.25, Perdas Totais: -33832.25\n",
      "Modelo e log do episódio 58 salvos em: 4.7.9\\model_episode_58.pth e 4.7.9\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: -2456.25, Win Rate: 0.52, Wins: 1034, Losses: 965, Epsilon: 0.2763, Steps: 36754, Time: 146.68s\n",
      "Ações: Manter=14270, Comprar=12459, Vender=10025\n",
      "Ganhos Totais: 33168.25, Perdas Totais: -35624.50\n",
      "Episode 60/100, Total Reward: 1691.75, Win Rate: 0.56, Wins: 1186, Losses: 932, Epsilon: 0.2736, Steps: 36754, Time: 146.70s\n",
      "Ações: Manter=13874, Comprar=11398, Vender=11482\n",
      "Ganhos Totais: 35097.50, Perdas Totais: -33405.75\n",
      "Modelo e log do episódio 60 salvos em: 4.7.9\\model_episode_60.pth e 4.7.9\\log_episode_60.csv\n",
      "\n",
      "Episode 61/100, Total Reward: 3833.75, Win Rate: 0.54, Wins: 1074, Losses: 906, Epsilon: 0.2708, Steps: 36754, Time: 146.59s\n",
      "Ações: Manter=11632, Comprar=13297, Vender=11825\n",
      "Ganhos Totais: 36401.75, Perdas Totais: -32568.00\n",
      "Modelo e log do episódio 61 salvos em: 4.7.9\\model_episode_61.pth e 4.7.9\\log_episode_61.csv\n",
      "\n",
      "Episode 62/100, Total Reward: 3433.50, Win Rate: 0.54, Wins: 989, Losses: 830, Epsilon: 0.2681, Steps: 36754, Time: 146.54s\n",
      "Ações: Manter=13882, Comprar=10363, Vender=12509\n",
      "Ganhos Totais: 34936.25, Perdas Totais: -31502.75\n",
      "Modelo e log do episódio 62 salvos em: 4.7.9\\model_episode_62.pth e 4.7.9\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: -4374.00, Win Rate: 0.51, Wins: 934, Losses: 909, Epsilon: 0.2655, Steps: 36754, Time: 146.24s\n",
      "Ações: Manter=13097, Comprar=12343, Vender=11314\n",
      "Ganhos Totais: 31798.50, Perdas Totais: -36172.50\n",
      "Episode 64/100, Total Reward: -2274.50, Win Rate: 0.53, Wins: 916, Losses: 828, Epsilon: 0.2628, Steps: 36754, Time: 147.01s\n",
      "Ações: Manter=14306, Comprar=10766, Vender=11682\n",
      "Ganhos Totais: 31375.25, Perdas Totais: -33649.75\n",
      "Episode 65/100, Total Reward: -455.25, Win Rate: 0.53, Wins: 965, Losses: 854, Epsilon: 0.2602, Steps: 36754, Time: 146.88s\n",
      "Ações: Manter=12399, Comprar=12868, Vender=11487\n",
      "Ganhos Totais: 34099.75, Perdas Totais: -34555.00\n",
      "Episode 66/100, Total Reward: -2445.50, Win Rate: 0.52, Wins: 917, Losses: 853, Epsilon: 0.2576, Steps: 36754, Time: 146.13s\n",
      "Ações: Manter=12100, Comprar=11448, Vender=13206\n",
      "Ganhos Totais: 31787.75, Perdas Totais: -34233.25\n",
      "Episode 67/100, Total Reward: 2970.75, Win Rate: 0.53, Wins: 914, Losses: 819, Epsilon: 0.2550, Steps: 36754, Time: 146.96s\n",
      "Ações: Manter=12866, Comprar=11933, Vender=11955\n",
      "Ganhos Totais: 34666.75, Perdas Totais: -31696.00\n",
      "Modelo e log do episódio 67 salvos em: 4.7.9\\model_episode_67.pth e 4.7.9\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: -2882.00, Win Rate: 0.51, Wins: 926, Losses: 887, Epsilon: 0.2524, Steps: 36754, Time: 147.12s\n",
      "Ações: Manter=12353, Comprar=13119, Vender=11282\n",
      "Ganhos Totais: 32664.50, Perdas Totais: -35546.50\n",
      "Episode 69/100, Total Reward: -1797.00, Win Rate: 0.53, Wins: 996, Losses: 898, Epsilon: 0.2499, Steps: 36754, Time: 146.69s\n",
      "Ações: Manter=12926, Comprar=11676, Vender=12152\n",
      "Ganhos Totais: 33702.00, Perdas Totais: -35499.00\n",
      "Episode 70/100, Total Reward: 1380.75, Win Rate: 0.54, Wins: 990, Losses: 860, Epsilon: 0.2474, Steps: 36754, Time: 147.15s\n",
      "Ações: Manter=11730, Comprar=13172, Vender=11852\n",
      "Ganhos Totais: 35372.75, Perdas Totais: -33992.00\n",
      "Episode 71/100, Total Reward: 639.00, Win Rate: 0.55, Wins: 1033, Losses: 839, Epsilon: 0.2449, Steps: 36754, Time: 146.99s\n",
      "Ações: Manter=13541, Comprar=13158, Vender=10055\n",
      "Ganhos Totais: 34062.00, Perdas Totais: -33423.00\n",
      "Episode 72/100, Total Reward: -1935.25, Win Rate: 0.54, Wins: 905, Losses: 778, Epsilon: 0.2425, Steps: 36754, Time: 147.49s\n",
      "Ações: Manter=12945, Comprar=11684, Vender=12125\n",
      "Ganhos Totais: 32442.00, Perdas Totais: -34377.25\n",
      "Episode 73/100, Total Reward: 2046.50, Win Rate: 0.52, Wins: 879, Losses: 805, Epsilon: 0.2401, Steps: 36754, Time: 147.08s\n",
      "Ações: Manter=11688, Comprar=14657, Vender=10409\n",
      "Ganhos Totais: 33649.00, Perdas Totais: -31602.50\n",
      "Episode 74/100, Total Reward: -1658.00, Win Rate: 0.53, Wins: 941, Losses: 829, Epsilon: 0.2377, Steps: 36754, Time: 147.49s\n",
      "Ações: Manter=11917, Comprar=11423, Vender=13414\n",
      "Ganhos Totais: 32821.50, Perdas Totais: -34479.50\n",
      "Episode 75/100, Total Reward: 1951.00, Win Rate: 0.56, Wins: 982, Losses: 775, Epsilon: 0.2353, Steps: 36754, Time: 148.34s\n",
      "Ações: Manter=10930, Comprar=14701, Vender=11123\n",
      "Ganhos Totais: 33936.50, Perdas Totais: -31985.50\n",
      "Episode 76/100, Total Reward: 4988.50, Win Rate: 0.55, Wins: 1026, Losses: 824, Epsilon: 0.2329, Steps: 36754, Time: 147.30s\n",
      "Ações: Manter=12233, Comprar=12856, Vender=11665\n",
      "Ganhos Totais: 35900.00, Perdas Totais: -30911.50\n",
      "Modelo e log do episódio 76 salvos em: 4.7.9\\model_episode_76.pth e 4.7.9\\log_episode_76.csv\n",
      "\n",
      "Episode 77/100, Total Reward: 35.50, Win Rate: 0.54, Wins: 859, Losses: 742, Epsilon: 0.2306, Steps: 36754, Time: 147.56s\n",
      "Ações: Manter=14148, Comprar=13309, Vender=9297\n",
      "Ganhos Totais: 31465.00, Perdas Totais: -31429.50\n",
      "Episode 78/100, Total Reward: 55.00, Win Rate: 0.55, Wins: 907, Losses: 730, Epsilon: 0.2283, Steps: 36754, Time: 147.18s\n",
      "Ações: Manter=10607, Comprar=12940, Vender=13207\n",
      "Ganhos Totais: 32804.25, Perdas Totais: -32749.25\n",
      "Episode 79/100, Total Reward: -2817.00, Win Rate: 0.54, Wins: 965, Losses: 817, Epsilon: 0.2260, Steps: 36754, Time: 147.29s\n",
      "Ações: Manter=10499, Comprar=12349, Vender=13906\n",
      "Ganhos Totais: 32044.00, Perdas Totais: -34861.00\n",
      "Episode 80/100, Total Reward: -1477.75, Win Rate: 0.53, Wins: 850, Losses: 767, Epsilon: 0.2238, Steps: 36754, Time: 147.93s\n",
      "Ações: Manter=12352, Comprar=12627, Vender=11775\n",
      "Ganhos Totais: 31648.00, Perdas Totais: -33125.75\n",
      "Episode 81/100, Total Reward: -2553.25, Win Rate: 0.53, Wins: 908, Losses: 814, Epsilon: 0.2215, Steps: 36754, Time: 149.33s\n",
      "Ações: Manter=11303, Comprar=14080, Vender=11371\n",
      "Ganhos Totais: 30892.00, Perdas Totais: -33445.25\n",
      "Episode 82/100, Total Reward: -1540.50, Win Rate: 0.52, Wins: 870, Losses: 789, Epsilon: 0.2193, Steps: 36754, Time: 147.63s\n",
      "Ações: Manter=11136, Comprar=14950, Vender=10668\n",
      "Ganhos Totais: 32426.25, Perdas Totais: -33966.75\n",
      "Episode 83/100, Total Reward: 2012.75, Win Rate: 0.55, Wins: 884, Losses: 716, Epsilon: 0.2171, Steps: 36754, Time: 147.51s\n",
      "Ações: Manter=10985, Comprar=15435, Vender=10334\n",
      "Ganhos Totais: 33232.50, Perdas Totais: -31219.75\n",
      "Episode 84/100, Total Reward: 1934.25, Win Rate: 0.56, Wins: 914, Losses: 725, Epsilon: 0.2149, Steps: 36754, Time: 148.23s\n",
      "Ações: Manter=9429, Comprar=14918, Vender=12407\n",
      "Ganhos Totais: 33211.00, Perdas Totais: -31276.75\n",
      "Episode 85/100, Total Reward: -107.75, Win Rate: 0.54, Wins: 857, Losses: 722, Epsilon: 0.2128, Steps: 36754, Time: 147.59s\n",
      "Ações: Manter=10169, Comprar=14815, Vender=11770\n",
      "Ganhos Totais: 33163.50, Perdas Totais: -33271.25\n",
      "Episode 86/100, Total Reward: -3643.75, Win Rate: 0.53, Wins: 826, Losses: 747, Epsilon: 0.2107, Steps: 36754, Time: 147.69s\n",
      "Ações: Manter=10874, Comprar=14330, Vender=11550\n",
      "Ganhos Totais: 30401.50, Perdas Totais: -34045.25\n",
      "Episode 87/100, Total Reward: 826.75, Win Rate: 0.54, Wins: 843, Losses: 712, Epsilon: 0.2086, Steps: 36754, Time: 147.48s\n",
      "Ações: Manter=10950, Comprar=16594, Vender=9210\n",
      "Ganhos Totais: 32466.50, Perdas Totais: -31639.75\n",
      "Episode 88/100, Total Reward: -1223.75, Win Rate: 0.54, Wins: 848, Losses: 708, Epsilon: 0.2065, Steps: 36754, Time: 147.86s\n",
      "Ações: Manter=11256, Comprar=14677, Vender=10821\n",
      "Ganhos Totais: 30901.25, Perdas Totais: -32125.00\n",
      "Episode 89/100, Total Reward: 2371.75, Win Rate: 0.54, Wins: 852, Losses: 730, Epsilon: 0.2044, Steps: 36754, Time: 147.49s\n",
      "Ações: Manter=10840, Comprar=14354, Vender=11560\n",
      "Ganhos Totais: 33559.50, Perdas Totais: -31187.75\n",
      "Modelo e log do episódio 89 salvos em: 4.7.9\\model_episode_89.pth e 4.7.9\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: 2763.00, Win Rate: 0.56, Wins: 899, Losses: 715, Epsilon: 0.2024, Steps: 36754, Time: 143.68s\n",
      "Ações: Manter=11698, Comprar=10949, Vender=14107\n",
      "Ganhos Totais: 33915.00, Perdas Totais: -31152.00\n",
      "Modelo e log do episódio 90 salvos em: 4.7.9\\model_episode_90.pth e 4.7.9\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: 2403.50, Win Rate: 0.54, Wins: 854, Losses: 731, Epsilon: 0.2003, Steps: 36754, Time: 141.17s\n",
      "Ações: Manter=11371, Comprar=14243, Vender=11140\n",
      "Ganhos Totais: 32833.75, Perdas Totais: -30430.25\n",
      "Episode 92/100, Total Reward: -1511.50, Win Rate: 0.54, Wins: 765, Losses: 647, Epsilon: 0.1983, Steps: 36754, Time: 140.81s\n",
      "Ações: Manter=10713, Comprar=15493, Vender=10548\n",
      "Ganhos Totais: 29932.25, Perdas Totais: -31443.75\n",
      "Episode 93/100, Total Reward: -4352.75, Win Rate: 0.51, Wins: 793, Losses: 759, Epsilon: 0.1964, Steps: 36754, Time: 153.35s\n",
      "Ações: Manter=7877, Comprar=11626, Vender=17251\n",
      "Ganhos Totais: 29962.00, Perdas Totais: -34314.75\n",
      "Episode 94/100, Total Reward: 8047.00, Win Rate: 0.57, Wins: 922, Losses: 690, Epsilon: 0.1944, Steps: 36754, Time: 157.90s\n",
      "Ações: Manter=9222, Comprar=17012, Vender=10520\n",
      "Ganhos Totais: 36524.75, Perdas Totais: -28477.75\n",
      "Modelo e log do episódio 94 salvos em: 4.7.9\\model_episode_94.pth e 4.7.9\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: -3069.50, Win Rate: 0.52, Wins: 782, Losses: 730, Epsilon: 0.1924, Steps: 36754, Time: 161.58s\n",
      "Ações: Manter=9061, Comprar=13641, Vender=14052\n",
      "Ganhos Totais: 31048.25, Perdas Totais: -34117.75\n",
      "Episode 96/100, Total Reward: -861.75, Win Rate: 0.55, Wins: 863, Losses: 701, Epsilon: 0.1905, Steps: 36754, Time: 159.34s\n",
      "Ações: Manter=9680, Comprar=14499, Vender=12575\n",
      "Ganhos Totais: 32493.25, Perdas Totais: -33355.00\n",
      "Episode 97/100, Total Reward: 1320.00, Win Rate: 0.55, Wins: 882, Losses: 736, Epsilon: 0.1886, Steps: 36754, Time: 160.54s\n",
      "Ações: Manter=8406, Comprar=13848, Vender=14500\n",
      "Ganhos Totais: 33703.25, Perdas Totais: -32383.25\n",
      "Episode 98/100, Total Reward: 113.25, Win Rate: 0.54, Wins: 825, Losses: 691, Epsilon: 0.1867, Steps: 36754, Time: 157.50s\n",
      "Ações: Manter=9009, Comprar=14662, Vender=13083\n",
      "Ganhos Totais: 32914.75, Perdas Totais: -32801.50\n",
      "Episode 99/100, Total Reward: 701.75, Win Rate: 0.55, Wins: 821, Losses: 685, Epsilon: 0.1849, Steps: 36754, Time: 159.78s\n",
      "Ações: Manter=8421, Comprar=14836, Vender=13497\n",
      "Ganhos Totais: 33050.50, Perdas Totais: -32348.75\n",
      "Episode 100/100, Total Reward: -5293.75, Win Rate: 0.52, Wins: 754, Losses: 703, Epsilon: 0.1830, Steps: 36754, Time: 158.67s\n",
      "Ações: Manter=10114, Comprar=12181, Vender=14459\n",
      "Ganhos Totais: 29290.25, Perdas Totais: -34584.00\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 94, Total Reward: 8047.00, Win Rate: 0.57, Wins: 922, Losses: 690, Ações: {0: 9222, 1: 17012, 2: 10520}, Steps: 36754, Time: 157.90s\n",
      "Rank 2: Episode 76, Total Reward: 4988.50, Win Rate: 0.55, Wins: 1026, Losses: 824, Ações: {0: 12233, 1: 12856, 2: 11665}, Steps: 36754, Time: 147.30s\n",
      "Rank 3: Episode 25, Total Reward: 4910.75, Win Rate: 0.55, Wins: 1445, Losses: 1170, Ações: {0: 12631, 1: 13075, 2: 11048}, Steps: 36754, Time: 168.20s\n",
      "Rank 4: Episode 12, Total Reward: 3915.25, Win Rate: 0.55, Wins: 1593, Losses: 1280, Ações: {0: 11593, 1: 12487, 2: 12674}, Steps: 36754, Time: 168.42s\n",
      "Rank 5: Episode 61, Total Reward: 3833.75, Win Rate: 0.54, Wins: 1074, Losses: 906, Ações: {0: 11632, 1: 13297, 2: 11825}, Steps: 36754, Time: 146.59s\n",
      "Rank 6: Episode 22, Total Reward: 3810.25, Win Rate: 0.55, Wins: 1559, Losses: 1278, Ações: {0: 12210, 1: 11441, 2: 13103}, Steps: 36754, Time: 167.82s\n",
      "Rank 7: Episode 62, Total Reward: 3433.50, Win Rate: 0.54, Wins: 989, Losses: 830, Ações: {0: 13882, 1: 10363, 2: 12509}, Steps: 36754, Time: 146.54s\n",
      "Rank 8: Episode 67, Total Reward: 2970.75, Win Rate: 0.53, Wins: 914, Losses: 819, Ações: {0: 12866, 1: 11933, 2: 11955}, Steps: 36754, Time: 146.96s\n",
      "Rank 9: Episode 57, Total Reward: 2960.50, Win Rate: 0.55, Wins: 1067, Losses: 885, Ações: {0: 12993, 1: 12924, 2: 10837}, Steps: 36754, Time: 146.61s\n",
      "Rank 10: Episode 90, Total Reward: 2763.00, Win Rate: 0.56, Wins: 899, Losses: 715, Ações: {0: 11698, 1: 10949, 2: 14107}, Steps: 36754, Time: 143.68s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 512\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.9\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
