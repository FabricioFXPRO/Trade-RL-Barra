{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -38086.25, Total Real Reward: -1495.50, Total Real Profit: -852.50, Win Rate: 0.49, Wins: 1265, Losses: 1299, Epsilon: 0.4950, Steps: 36754, Time: 156.13s\n",
      "Ações: Manter=12088, Comprar=12976, Vender=11690\n",
      "Ganhos Totais: 35738.25, Perdas Totais: -36590.75\n",
      "Modelo e log do episódio 1 salvos em: 4.13\\model_episode_1.pth e 4.13\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -42322.75, Total Real Reward: -4243.25, Total Real Profit: -3641.75, Win Rate: 0.49, Wins: 1184, Losses: 1210, Epsilon: 0.4900, Steps: 36754, Time: 186.10s\n",
      "Ações: Manter=9709, Comprar=14126, Vender=12919\n",
      "Ganhos Totais: 34437.75, Perdas Totais: -38079.50\n",
      "Modelo e log do episódio 2 salvos em: 4.13\\model_episode_2.pth e 4.13\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -36754.25, Total Real Reward: -733.00, Total Real Profit: -147.25, Win Rate: 0.50, Wins: 1178, Losses: 1155, Epsilon: 0.4851, Steps: 36754, Time: 189.89s\n",
      "Ações: Manter=10447, Comprar=13326, Vender=12981\n",
      "Ganhos Totais: 35874.00, Perdas Totais: -36021.25\n",
      "Modelo e log do episódio 3 salvos em: 4.13\\model_episode_3.pth e 4.13\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -34368.25, Total Real Reward: 1297.25, Total Real Profit: 1899.25, Win Rate: 0.51, Wins: 1223, Losses: 1181, Epsilon: 0.4803, Steps: 36754, Time: 190.19s\n",
      "Ações: Manter=10901, Comprar=13898, Vender=11955\n",
      "Ganhos Totais: 37564.75, Perdas Totais: -35665.50\n",
      "Modelo e log do episódio 4 salvos em: 4.13\\model_episode_4.pth e 4.13\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -36846.00, Total Real Reward: -889.50, Total Real Profit: -302.75, Win Rate: 0.51, Wins: 1193, Losses: 1140, Epsilon: 0.4755, Steps: 36754, Time: 188.11s\n",
      "Ações: Manter=10974, Comprar=13838, Vender=11942\n",
      "Ganhos Totais: 35653.75, Perdas Totais: -35956.50\n",
      "Modelo e log do episódio 5 salvos em: 4.13\\model_episode_5.pth e 4.13\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -31191.00, Total Real Reward: 2161.00, Total Real Profit: 2719.50, Win Rate: 0.52, Wins: 1166, Losses: 1064, Epsilon: 0.4707, Steps: 36754, Time: 190.07s\n",
      "Ações: Manter=12619, Comprar=13298, Vender=10837\n",
      "Ganhos Totais: 36071.50, Perdas Totais: -33352.00\n",
      "Modelo e log do episódio 6 salvos em: 4.13\\model_episode_6.pth e 4.13\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -33796.75, Total Real Reward: 879.50, Total Real Profit: 1448.50, Win Rate: 0.52, Wins: 1186, Losses: 1086, Epsilon: 0.4660, Steps: 36754, Time: 189.54s\n",
      "Ações: Manter=12237, Comprar=13525, Vender=10992\n",
      "Ganhos Totais: 36124.75, Perdas Totais: -34676.25\n",
      "Modelo e log do episódio 7 salvos em: 4.13\\model_episode_7.pth e 4.13\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -36859.50, Total Real Reward: -867.25, Total Real Profit: -281.50, Win Rate: 0.53, Wins: 1235, Losses: 1096, Epsilon: 0.4614, Steps: 36754, Time: 190.43s\n",
      "Ações: Manter=12589, Comprar=12923, Vender=11242\n",
      "Ganhos Totais: 35710.75, Perdas Totais: -35992.25\n",
      "Modelo e log do episódio 8 salvos em: 4.13\\model_episode_8.pth e 4.13\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -39732.75, Total Real Reward: -3251.75, Total Real Profit: -2669.50, Win Rate: 0.50, Wins: 1161, Losses: 1153, Epsilon: 0.4568, Steps: 36754, Time: 190.28s\n",
      "Ações: Manter=12540, Comprar=13203, Vender=11011\n",
      "Ganhos Totais: 33811.50, Perdas Totais: -36481.00\n",
      "Modelo e log do episódio 9 salvos em: 4.13\\model_episode_9.pth e 4.13\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -34560.75, Total Real Reward: 1183.75, Total Real Profit: 1777.50, Win Rate: 0.53, Wins: 1251, Losses: 1115, Epsilon: 0.4522, Steps: 36754, Time: 189.34s\n",
      "Ações: Manter=12179, Comprar=13502, Vender=11073\n",
      "Ganhos Totais: 37522.00, Perdas Totais: -35744.50\n",
      "Modelo e log do episódio 10 salvos em: 4.13\\model_episode_10.pth e 4.13\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -35902.25, Total Real Reward: 927.25, Total Real Profit: 1526.50, Win Rate: 0.52, Wins: 1245, Losses: 1148, Epsilon: 0.4477, Steps: 36754, Time: 190.28s\n",
      "Ações: Manter=12645, Comprar=12388, Vender=11721\n",
      "Ganhos Totais: 38356.00, Perdas Totais: -36829.50\n",
      "Modelo e log do episódio 11 salvos em: 4.13\\model_episode_11.pth e 4.13\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -38228.00, Total Real Reward: -2010.25, Total Real Profit: -1425.75, Win Rate: 0.52, Wins: 1215, Losses: 1114, Epsilon: 0.4432, Steps: 36754, Time: 185.36s\n",
      "Ações: Manter=13267, Comprar=13097, Vender=10390\n",
      "Ganhos Totais: 34792.00, Perdas Totais: -36217.75\n",
      "Modelo e log do episódio 12 salvos em: 4.13\\model_episode_12.pth e 4.13\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -34057.25, Total Real Reward: 1133.75, Total Real Profit: 1721.75, Win Rate: 0.53, Wins: 1238, Losses: 1106, Epsilon: 0.4388, Steps: 36754, Time: 184.33s\n",
      "Ações: Manter=13633, Comprar=11605, Vender=11516\n",
      "Ganhos Totais: 36912.75, Perdas Totais: -35191.00\n",
      "Modelo e log do episódio 13 salvos em: 4.13\\model_episode_13.pth e 4.13\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -30525.75, Total Real Reward: 2769.00, Total Real Profit: 3344.25, Win Rate: 0.53, Wins: 1213, Losses: 1081, Epsilon: 0.4344, Steps: 36754, Time: 186.07s\n",
      "Ações: Manter=14220, Comprar=11031, Vender=11503\n",
      "Ganhos Totais: 36639.00, Perdas Totais: -33294.75\n",
      "Modelo e log do episódio 14 salvos em: 4.13\\model_episode_14.pth e 4.13\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -37477.25, Total Real Reward: -1017.25, Total Real Profit: -440.75, Win Rate: 0.52, Wins: 1193, Losses: 1105, Epsilon: 0.4300, Steps: 36754, Time: 187.21s\n",
      "Ações: Manter=12401, Comprar=12997, Vender=11356\n",
      "Ganhos Totais: 36019.25, Perdas Totais: -36460.00\n",
      "Episode 16/100, Total Reward: -33928.50, Total Real Reward: 336.00, Total Real Profit: 880.75, Win Rate: 0.52, Wins: 1123, Losses: 1050, Epsilon: 0.4257, Steps: 36754, Time: 185.14s\n",
      "Ações: Manter=11961, Comprar=13178, Vender=11615\n",
      "Ganhos Totais: 35145.25, Perdas Totais: -34264.50\n",
      "Modelo e log do episódio 16 salvos em: 4.13\\model_episode_16.pth e 4.13\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -35953.25, Total Real Reward: -466.25, Total Real Profit: 103.00, Win Rate: 0.52, Wins: 1176, Losses: 1090, Epsilon: 0.4215, Steps: 36754, Time: 187.03s\n",
      "Ações: Manter=13202, Comprar=11015, Vender=12537\n",
      "Ganhos Totais: 35590.00, Perdas Totais: -35487.00\n",
      "Modelo e log do episódio 17 salvos em: 4.13\\model_episode_17.pth e 4.13\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -39024.00, Total Real Reward: -2501.00, Total Real Profit: -1921.25, Win Rate: 0.51, Wins: 1183, Losses: 1125, Epsilon: 0.4173, Steps: 36754, Time: 185.83s\n",
      "Ações: Manter=14592, Comprar=11508, Vender=10654\n",
      "Ganhos Totais: 34601.75, Perdas Totais: -36523.00\n",
      "Episode 19/100, Total Reward: -37884.25, Total Real Reward: -2167.75, Total Real Profit: -1607.75, Win Rate: 0.52, Wins: 1155, Losses: 1068, Epsilon: 0.4131, Steps: 36754, Time: 186.02s\n",
      "Ações: Manter=12379, Comprar=12132, Vender=12243\n",
      "Ganhos Totais: 34108.75, Perdas Totais: -35716.50\n",
      "Episode 20/100, Total Reward: -41851.25, Total Real Reward: -4874.25, Total Real Profit: -4357.00, Win Rate: 0.51, Wins: 1051, Losses: 1006, Epsilon: 0.4090, Steps: 36754, Time: 185.09s\n",
      "Ações: Manter=13706, Comprar=11448, Vender=11600\n",
      "Ganhos Totais: 32620.00, Perdas Totais: -36977.00\n",
      "Episode 21/100, Total Reward: -40787.75, Total Real Reward: -3738.25, Total Real Profit: -3195.25, Win Rate: 0.51, Wins: 1101, Losses: 1062, Epsilon: 0.4049, Steps: 36754, Time: 186.32s\n",
      "Ações: Manter=13774, Comprar=11518, Vender=11462\n",
      "Ganhos Totais: 33854.25, Perdas Totais: -37049.50\n",
      "Episode 22/100, Total Reward: -38074.50, Total Real Reward: -1847.75, Total Real Profit: -1309.25, Win Rate: 0.51, Wins: 1093, Losses: 1050, Epsilon: 0.4008, Steps: 36754, Time: 181.28s\n",
      "Ações: Manter=12334, Comprar=10829, Vender=13591\n",
      "Ganhos Totais: 34917.50, Perdas Totais: -36226.75\n",
      "Episode 23/100, Total Reward: -29944.75, Total Real Reward: 2841.00, Total Real Profit: 3394.00, Win Rate: 0.54, Wins: 1188, Losses: 1011, Epsilon: 0.3968, Steps: 36754, Time: 183.59s\n",
      "Ações: Manter=12794, Comprar=12512, Vender=11448\n",
      "Ganhos Totais: 36179.75, Perdas Totais: -32785.75\n",
      "Modelo e log do episódio 23 salvos em: 4.13\\model_episode_23.pth e 4.13\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: -39516.25, Total Real Reward: -2671.50, Total Real Profit: -2127.00, Win Rate: 0.51, Wins: 1097, Losses: 1073, Epsilon: 0.3928, Steps: 36754, Time: 178.12s\n",
      "Ações: Manter=13187, Comprar=11767, Vender=11800\n",
      "Ganhos Totais: 34717.75, Perdas Totais: -36844.75\n",
      "Episode 25/100, Total Reward: -38246.75, Total Real Reward: -2522.00, Total Real Profit: -1978.75, Win Rate: 0.51, Wins: 1111, Losses: 1054, Epsilon: 0.3889, Steps: 36754, Time: 178.84s\n",
      "Ações: Manter=12537, Comprar=12939, Vender=11278\n",
      "Ganhos Totais: 33746.00, Perdas Totais: -35724.75\n",
      "Episode 26/100, Total Reward: -37438.50, Total Real Reward: -1307.50, Total Real Profit: -761.00, Win Rate: 0.52, Wins: 1134, Losses: 1044, Epsilon: 0.3850, Steps: 36754, Time: 179.28s\n",
      "Ações: Manter=11698, Comprar=13372, Vender=11684\n",
      "Ganhos Totais: 35370.00, Perdas Totais: -36131.00\n",
      "Episode 27/100, Total Reward: -36280.00, Total Real Reward: -1598.25, Total Real Profit: -1068.25, Win Rate: 0.52, Wins: 1107, Losses: 1004, Epsilon: 0.3812, Steps: 36754, Time: 181.56s\n",
      "Ações: Manter=13225, Comprar=11387, Vender=12142\n",
      "Ganhos Totais: 33613.50, Perdas Totais: -34681.75\n",
      "Episode 28/100, Total Reward: -37551.00, Total Real Reward: -1474.50, Total Real Profit: -927.75, Win Rate: 0.52, Wins: 1132, Losses: 1043, Epsilon: 0.3774, Steps: 36754, Time: 182.42s\n",
      "Ações: Manter=11603, Comprar=11396, Vender=13755\n",
      "Ganhos Totais: 35148.75, Perdas Totais: -36076.50\n",
      "Episode 29/100, Total Reward: -37983.50, Total Real Reward: -2020.75, Total Real Profit: -1477.25, Win Rate: 0.51, Wins: 1114, Losses: 1052, Epsilon: 0.3736, Steps: 36754, Time: 179.92s\n",
      "Ações: Manter=12348, Comprar=12507, Vender=11899\n",
      "Ganhos Totais: 34485.50, Perdas Totais: -35962.75\n",
      "Episode 30/100, Total Reward: -36699.50, Total Real Reward: -1310.75, Total Real Profit: -792.25, Win Rate: 0.52, Wins: 1064, Losses: 1000, Epsilon: 0.3699, Steps: 36754, Time: 178.10s\n",
      "Ações: Manter=13789, Comprar=11643, Vender=11322\n",
      "Ganhos Totais: 34596.50, Perdas Totais: -35388.75\n",
      "Episode 31/100, Total Reward: -38632.00, Total Real Reward: -3400.75, Total Real Profit: -2876.25, Win Rate: 0.52, Wins: 1093, Losses: 997, Epsilon: 0.3662, Steps: 36754, Time: 130.40s\n",
      "Ações: Manter=14011, Comprar=10961, Vender=11782\n",
      "Ganhos Totais: 32355.00, Perdas Totais: -35231.25\n",
      "Episode 32/100, Total Reward: -36996.00, Total Real Reward: -1505.00, Total Real Profit: -962.00, Win Rate: 0.54, Wins: 1162, Losses: 999, Epsilon: 0.3625, Steps: 36754, Time: 151.93s\n",
      "Ações: Manter=13116, Comprar=12268, Vender=11370\n",
      "Ganhos Totais: 34529.00, Perdas Totais: -35491.00\n",
      "Episode 33/100, Total Reward: -32639.50, Total Real Reward: 374.25, Total Real Profit: 885.75, Win Rate: 0.52, Wins: 1051, Losses: 987, Epsilon: 0.3589, Steps: 36754, Time: 149.03s\n",
      "Ações: Manter=14191, Comprar=11104, Vender=11459\n",
      "Ganhos Totais: 33899.50, Perdas Totais: -33013.75\n",
      "Modelo e log do episódio 33 salvos em: 4.13\\model_episode_33.pth e 4.13\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: -38183.75, Total Real Reward: -2303.25, Total Real Profit: -1789.00, Win Rate: 0.53, Wins: 1088, Losses: 960, Epsilon: 0.3553, Steps: 36754, Time: 147.97s\n",
      "Ações: Manter=13826, Comprar=11121, Vender=11807\n",
      "Ganhos Totais: 34091.50, Perdas Totais: -35880.50\n",
      "Episode 35/100, Total Reward: -39371.75, Total Real Reward: -3233.50, Total Real Profit: -2710.00, Win Rate: 0.51, Wins: 1060, Losses: 1030, Epsilon: 0.3517, Steps: 36754, Time: 148.48s\n",
      "Ações: Manter=13692, Comprar=11098, Vender=11964\n",
      "Ganhos Totais: 33428.25, Perdas Totais: -36138.25\n",
      "Episode 36/100, Total Reward: -31964.50, Total Real Reward: 845.50, Total Real Profit: 1309.50, Win Rate: 0.51, Wins: 945, Losses: 909, Epsilon: 0.3482, Steps: 36754, Time: 148.40s\n",
      "Ações: Manter=12592, Comprar=12865, Vender=11297\n",
      "Ganhos Totais: 34119.50, Perdas Totais: -32810.00\n",
      "Modelo e log do episódio 36 salvos em: 4.13\\model_episode_36.pth e 4.13\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: -35648.00, Total Real Reward: -1764.00, Total Real Profit: -1268.00, Win Rate: 0.50, Wins: 992, Losses: 982, Epsilon: 0.3447, Steps: 36754, Time: 148.04s\n",
      "Ações: Manter=14412, Comprar=10942, Vender=11400\n",
      "Ganhos Totais: 32616.00, Perdas Totais: -33884.00\n",
      "Episode 38/100, Total Reward: -40768.50, Total Real Reward: -4014.50, Total Real Profit: -3519.75, Win Rate: 0.51, Wins: 1006, Losses: 964, Epsilon: 0.3413, Steps: 36754, Time: 148.83s\n",
      "Ações: Manter=12886, Comprar=12643, Vender=11225\n",
      "Ganhos Totais: 33234.25, Perdas Totais: -36754.00\n",
      "Episode 39/100, Total Reward: -39140.75, Total Real Reward: -4105.75, Total Real Profit: -3636.25, Win Rate: 0.52, Wins: 970, Losses: 901, Epsilon: 0.3379, Steps: 36754, Time: 148.54s\n",
      "Ações: Manter=14823, Comprar=11032, Vender=10899\n",
      "Ganhos Totais: 31398.75, Perdas Totais: -35035.00\n",
      "Episode 40/100, Total Reward: -36801.75, Total Real Reward: -2381.75, Total Real Profit: -1894.75, Win Rate: 0.53, Wins: 1020, Losses: 920, Epsilon: 0.3345, Steps: 36754, Time: 148.50s\n",
      "Ações: Manter=12575, Comprar=11549, Vender=12630\n",
      "Ganhos Totais: 32525.25, Perdas Totais: -34420.00\n",
      "Episode 41/100, Total Reward: -36492.25, Total Real Reward: -2214.75, Total Real Profit: -1759.00, Win Rate: 0.52, Wins: 936, Losses: 877, Epsilon: 0.3311, Steps: 36754, Time: 149.38s\n",
      "Ações: Manter=14444, Comprar=10943, Vender=11367\n",
      "Ganhos Totais: 32518.50, Perdas Totais: -34277.50\n",
      "Episode 42/100, Total Reward: -32155.00, Total Real Reward: 1236.50, Total Real Profit: 1715.25, Win Rate: 0.52, Wins: 989, Losses: 920, Epsilon: 0.3278, Steps: 36754, Time: 148.72s\n",
      "Ações: Manter=15205, Comprar=11170, Vender=10379\n",
      "Ganhos Totais: 35106.75, Perdas Totais: -33391.50\n",
      "Modelo e log do episódio 42 salvos em: 4.13\\model_episode_42.pth e 4.13\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: -40850.00, Total Real Reward: -3912.75, Total Real Profit: -3435.00, Win Rate: 0.51, Wins: 961, Losses: 939, Epsilon: 0.3246, Steps: 36754, Time: 148.48s\n",
      "Ações: Manter=13676, Comprar=11190, Vender=11888\n",
      "Ganhos Totais: 33502.25, Perdas Totais: -36937.25\n",
      "Episode 44/100, Total Reward: -36766.25, Total Real Reward: -1727.50, Total Real Profit: -1235.50, Win Rate: 0.53, Wins: 1038, Losses: 922, Epsilon: 0.3213, Steps: 36754, Time: 149.18s\n",
      "Ações: Manter=13823, Comprar=11272, Vender=11659\n",
      "Ganhos Totais: 33803.25, Perdas Totais: -35038.75\n",
      "Episode 45/100, Total Reward: -30792.50, Total Real Reward: 1934.50, Total Real Profit: 2408.00, Win Rate: 0.54, Wins: 1012, Losses: 877, Epsilon: 0.3181, Steps: 36754, Time: 148.58s\n",
      "Ações: Manter=12965, Comprar=11125, Vender=12664\n",
      "Ganhos Totais: 35135.00, Perdas Totais: -32727.00\n",
      "Modelo e log do episódio 45 salvos em: 4.13\\model_episode_45.pth e 4.13\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: -31840.50, Total Real Reward: 1283.75, Total Real Profit: 1766.50, Win Rate: 0.52, Wins: 1008, Losses: 916, Epsilon: 0.3149, Steps: 36754, Time: 148.68s\n",
      "Ações: Manter=14299, Comprar=9935, Vender=12520\n",
      "Ganhos Totais: 34890.75, Perdas Totais: -33124.25\n",
      "Modelo e log do episódio 46 salvos em: 4.13\\model_episode_46.pth e 4.13\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: -42576.50, Total Real Reward: -5204.00, Total Real Profit: -4722.25, Win Rate: 0.52, Wins: 997, Losses: 922, Epsilon: 0.3118, Steps: 36754, Time: 149.29s\n",
      "Ações: Manter=13902, Comprar=10946, Vender=11906\n",
      "Ganhos Totais: 32650.25, Perdas Totais: -37372.50\n",
      "Episode 48/100, Total Reward: -39013.50, Total Real Reward: -4533.50, Total Real Profit: -4079.75, Win Rate: 0.51, Wins: 921, Losses: 886, Epsilon: 0.3086, Steps: 36754, Time: 145.17s\n",
      "Ações: Manter=13935, Comprar=10479, Vender=12340\n",
      "Ganhos Totais: 30400.25, Perdas Totais: -34480.00\n",
      "Episode 49/100, Total Reward: -40040.75, Total Real Reward: -5290.50, Total Real Profit: -4847.25, Win Rate: 0.51, Wins: 907, Losses: 860, Epsilon: 0.3056, Steps: 36754, Time: 148.57s\n",
      "Ações: Manter=14105, Comprar=10337, Vender=12312\n",
      "Ganhos Totais: 29903.00, Perdas Totais: -34750.25\n",
      "Episode 50/100, Total Reward: -32976.00, Total Real Reward: -15.50, Total Real Profit: 435.25, Win Rate: 0.54, Wins: 964, Losses: 829, Epsilon: 0.3025, Steps: 36754, Time: 149.86s\n",
      "Ações: Manter=13158, Comprar=10671, Vender=12925\n",
      "Ganhos Totais: 33395.75, Perdas Totais: -32960.50\n",
      "Modelo e log do episódio 50 salvos em: 4.13\\model_episode_50.pth e 4.13\\log_episode_50.csv\n",
      "\n",
      "Episode 51/100, Total Reward: -38777.25, Total Real Reward: -3691.00, Total Real Profit: -3239.25, Win Rate: 0.52, Wins: 929, Losses: 867, Epsilon: 0.2995, Steps: 36754, Time: 149.60s\n",
      "Ações: Manter=12826, Comprar=10812, Vender=13116\n",
      "Ganhos Totais: 31847.00, Perdas Totais: -35086.25\n",
      "Episode 52/100, Total Reward: -35402.50, Total Real Reward: -1791.00, Total Real Profit: -1365.25, Win Rate: 0.51, Wins: 863, Losses: 832, Epsilon: 0.2965, Steps: 36754, Time: 149.42s\n",
      "Ações: Manter=13647, Comprar=9745, Vender=13362\n",
      "Ganhos Totais: 32246.25, Perdas Totais: -33611.50\n",
      "Episode 53/100, Total Reward: -36360.50, Total Real Reward: -2504.25, Total Real Profit: -2050.00, Win Rate: 0.52, Wins: 944, Losses: 864, Epsilon: 0.2935, Steps: 36754, Time: 149.56s\n",
      "Ações: Manter=13148, Comprar=8733, Vender=14873\n",
      "Ganhos Totais: 31806.25, Perdas Totais: -33856.25\n",
      "Episode 54/100, Total Reward: -36469.75, Total Real Reward: -2535.25, Total Real Profit: -2114.50, Win Rate: 0.50, Wins: 844, Losses: 830, Epsilon: 0.2906, Steps: 36754, Time: 149.76s\n",
      "Ações: Manter=12981, Comprar=9808, Vender=13965\n",
      "Ganhos Totais: 31820.00, Perdas Totais: -33934.50\n",
      "Episode 55/100, Total Reward: -40815.25, Total Real Reward: -5278.50, Total Real Profit: -4844.50, Win Rate: 0.49, Wins: 845, Losses: 885, Epsilon: 0.2877, Steps: 36754, Time: 150.41s\n",
      "Ações: Manter=13134, Comprar=10673, Vender=12947\n",
      "Ganhos Totais: 30692.25, Perdas Totais: -35536.75\n",
      "Episode 56/100, Total Reward: -31842.50, Total Real Reward: 622.25, Total Real Profit: 1063.50, Win Rate: 0.53, Wins: 934, Losses: 824, Epsilon: 0.2848, Steps: 36754, Time: 150.45s\n",
      "Ações: Manter=14021, Comprar=10091, Vender=12642\n",
      "Ganhos Totais: 33528.25, Perdas Totais: -32464.75\n",
      "Modelo e log do episódio 56 salvos em: 4.13\\model_episode_56.pth e 4.13\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: -41923.50, Total Real Reward: -7626.50, Total Real Profit: -7240.75, Win Rate: 0.49, Wins: 748, Losses: 794, Epsilon: 0.2820, Steps: 36754, Time: 150.87s\n",
      "Ações: Manter=17207, Comprar=8569, Vender=10978\n",
      "Ganhos Totais: 27056.25, Perdas Totais: -34297.00\n",
      "Episode 58/100, Total Reward: -42433.00, Total Real Reward: -6590.50, Total Real Profit: -6163.75, Win Rate: 0.50, Wins: 848, Losses: 855, Epsilon: 0.2791, Steps: 36754, Time: 149.98s\n",
      "Ações: Manter=12268, Comprar=11257, Vender=13229\n",
      "Ganhos Totais: 29678.75, Perdas Totais: -35842.50\n",
      "Episode 59/100, Total Reward: -37543.00, Total Real Reward: -3238.00, Total Real Profit: -2805.25, Win Rate: 0.53, Wins: 914, Losses: 814, Epsilon: 0.2763, Steps: 36754, Time: 150.56s\n",
      "Ações: Manter=12537, Comprar=10623, Vender=13594\n",
      "Ganhos Totais: 31499.75, Perdas Totais: -34305.00\n",
      "Episode 60/100, Total Reward: -38366.00, Total Real Reward: -5213.25, Total Real Profit: -4780.75, Win Rate: 0.52, Wins: 902, Losses: 821, Epsilon: 0.2736, Steps: 36754, Time: 150.29s\n",
      "Ações: Manter=15985, Comprar=9779, Vender=10990\n",
      "Ganhos Totais: 28372.00, Perdas Totais: -33152.75\n",
      "Episode 61/100, Total Reward: -37348.75, Total Real Reward: -3130.50, Total Real Profit: -2680.75, Win Rate: 0.51, Wins: 916, Losses: 873, Epsilon: 0.2708, Steps: 36754, Time: 150.54s\n",
      "Ações: Manter=13206, Comprar=11031, Vender=12517\n",
      "Ganhos Totais: 31537.50, Perdas Totais: -34218.25\n",
      "Episode 62/100, Total Reward: -28998.50, Total Real Reward: 1644.00, Total Real Profit: 2091.50, Win Rate: 0.52, Wins: 935, Losses: 847, Epsilon: 0.2681, Steps: 36754, Time: 150.94s\n",
      "Ações: Manter=15391, Comprar=8948, Vender=12415\n",
      "Ganhos Totais: 32734.00, Perdas Totais: -30642.50\n",
      "Modelo e log do episódio 62 salvos em: 4.13\\model_episode_62.pth e 4.13\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: -34647.00, Total Real Reward: -1617.00, Total Real Profit: -1197.75, Win Rate: 0.52, Wins: 864, Losses: 805, Epsilon: 0.2655, Steps: 36754, Time: 150.78s\n",
      "Ações: Manter=15976, Comprar=9328, Vender=11450\n",
      "Ganhos Totais: 31832.25, Perdas Totais: -33030.00\n",
      "Episode 64/100, Total Reward: -33369.25, Total Real Reward: -943.00, Total Real Profit: -496.25, Win Rate: 0.53, Wins: 936, Losses: 842, Epsilon: 0.2628, Steps: 36754, Time: 150.44s\n",
      "Ações: Manter=13630, Comprar=11754, Vender=11370\n",
      "Ganhos Totais: 31930.00, Perdas Totais: -32426.25\n",
      "Episode 65/100, Total Reward: -37798.75, Total Real Reward: -3626.75, Total Real Profit: -3209.75, Win Rate: 0.53, Wins: 876, Losses: 788, Epsilon: 0.2602, Steps: 36754, Time: 151.07s\n",
      "Ações: Manter=12788, Comprar=12781, Vender=11185\n",
      "Ganhos Totais: 30962.25, Perdas Totais: -34172.00\n",
      "Episode 66/100, Total Reward: -41506.50, Total Real Reward: -6745.75, Total Real Profit: -6325.00, Win Rate: 0.49, Wins: 818, Losses: 859, Epsilon: 0.2576, Steps: 36754, Time: 151.00s\n",
      "Ações: Manter=12561, Comprar=13098, Vender=11095\n",
      "Ganhos Totais: 28435.75, Perdas Totais: -34760.75\n",
      "Episode 67/100, Total Reward: -29804.00, Total Real Reward: 1434.00, Total Real Profit: 1846.25, Win Rate: 0.53, Wins: 871, Losses: 772, Epsilon: 0.2550, Steps: 36754, Time: 150.87s\n",
      "Ações: Manter=13895, Comprar=10831, Vender=12028\n",
      "Ganhos Totais: 33084.25, Perdas Totais: -31238.00\n",
      "Modelo e log do episódio 67 salvos em: 4.13\\model_episode_67.pth e 4.13\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: -36214.75, Total Real Reward: -2987.00, Total Real Profit: -2539.75, Win Rate: 0.50, Wins: 899, Losses: 885, Epsilon: 0.2524, Steps: 36754, Time: 151.39s\n",
      "Ações: Manter=10481, Comprar=10819, Vender=15454\n",
      "Ganhos Totais: 30688.00, Perdas Totais: -33227.75\n",
      "Episode 69/100, Total Reward: -39360.25, Total Real Reward: -4959.25, Total Real Profit: -4534.25, Win Rate: 0.53, Wins: 889, Losses: 801, Epsilon: 0.2499, Steps: 36754, Time: 151.31s\n",
      "Ações: Manter=12596, Comprar=11938, Vender=12220\n",
      "Ganhos Totais: 29866.75, Perdas Totais: -34401.00\n",
      "Episode 70/100, Total Reward: -33074.75, Total Real Reward: -1367.50, Total Real Profit: -972.75, Win Rate: 0.52, Wins: 815, Losses: 755, Epsilon: 0.2474, Steps: 36754, Time: 151.78s\n",
      "Ações: Manter=14724, Comprar=10268, Vender=11762\n",
      "Ganhos Totais: 30734.50, Perdas Totais: -31707.25\n",
      "Episode 71/100, Total Reward: -29083.50, Total Real Reward: 1546.25, Total Real Profit: 1942.75, Win Rate: 0.52, Wins: 817, Losses: 766, Epsilon: 0.2449, Steps: 36754, Time: 151.42s\n",
      "Ações: Manter=13958, Comprar=8591, Vender=14205\n",
      "Ganhos Totais: 32572.50, Perdas Totais: -30629.75\n",
      "Modelo e log do episódio 71 salvos em: 4.13\\model_episode_71.pth e 4.13\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: -34061.25, Total Real Reward: -1516.00, Total Real Profit: -1090.00, Win Rate: 0.51, Wins: 872, Losses: 823, Epsilon: 0.2425, Steps: 36754, Time: 152.30s\n",
      "Ações: Manter=13819, Comprar=10150, Vender=12785\n",
      "Ganhos Totais: 31455.25, Perdas Totais: -32545.25\n",
      "Episode 73/100, Total Reward: -36140.50, Total Real Reward: -3022.75, Total Real Profit: -2586.25, Win Rate: 0.53, Wins: 924, Losses: 818, Epsilon: 0.2401, Steps: 36754, Time: 151.63s\n",
      "Ações: Manter=14463, Comprar=9470, Vender=12821\n",
      "Ganhos Totais: 30531.50, Perdas Totais: -33117.75\n",
      "Episode 74/100, Total Reward: -33424.00, Total Real Reward: -1740.25, Total Real Profit: -1339.00, Win Rate: 0.52, Wins: 833, Losses: 768, Epsilon: 0.2377, Steps: 36754, Time: 151.61s\n",
      "Ações: Manter=15538, Comprar=9087, Vender=12129\n",
      "Ganhos Totais: 30344.75, Perdas Totais: -31683.75\n",
      "Episode 75/100, Total Reward: -32758.50, Total Real Reward: -2236.75, Total Real Profit: -1841.25, Win Rate: 0.54, Wins: 852, Losses: 723, Epsilon: 0.2353, Steps: 36754, Time: 151.30s\n",
      "Ações: Manter=15756, Comprar=9496, Vender=11502\n",
      "Ganhos Totais: 28680.50, Perdas Totais: -30521.75\n",
      "Episode 76/100, Total Reward: -31673.00, Total Real Reward: -602.00, Total Real Profit: -187.50, Win Rate: 0.54, Wins: 896, Losses: 756, Epsilon: 0.2329, Steps: 36754, Time: 151.11s\n",
      "Ações: Manter=15332, Comprar=9797, Vender=11625\n",
      "Ganhos Totais: 30883.50, Perdas Totais: -31071.00\n",
      "Modelo e log do episódio 76 salvos em: 4.13\\model_episode_76.pth e 4.13\\log_episode_76.csv\n",
      "\n",
      "Episode 77/100, Total Reward: -32538.25, Total Real Reward: -1640.75, Total Real Profit: -1240.25, Win Rate: 0.53, Wins: 839, Losses: 758, Epsilon: 0.2306, Steps: 36754, Time: 144.52s\n",
      "Ações: Manter=13705, Comprar=11852, Vender=11197\n",
      "Ganhos Totais: 29657.25, Perdas Totais: -30897.50\n",
      "Episode 78/100, Total Reward: -30585.25, Total Real Reward: -1131.00, Total Real Profit: -760.00, Win Rate: 0.51, Wins: 761, Losses: 718, Epsilon: 0.2283, Steps: 36754, Time: 151.71s\n",
      "Ações: Manter=18297, Comprar=7573, Vender=10884\n",
      "Ganhos Totais: 28694.25, Perdas Totais: -29454.25\n",
      "Modelo e log do episódio 78 salvos em: 4.13\\model_episode_78.pth e 4.13\\log_episode_78.csv\n",
      "\n",
      "Episode 79/100, Total Reward: -36597.75, Total Real Reward: -2699.50, Total Real Profit: -2256.25, Win Rate: 0.52, Wins: 922, Losses: 836, Epsilon: 0.2260, Steps: 36754, Time: 153.87s\n",
      "Ações: Manter=11630, Comprar=10122, Vender=15002\n",
      "Ganhos Totais: 31642.00, Perdas Totais: -33898.25\n",
      "Episode 80/100, Total Reward: -33695.75, Total Real Reward: -2177.75, Total Real Profit: -1766.25, Win Rate: 0.52, Wins: 860, Losses: 781, Epsilon: 0.2238, Steps: 36754, Time: 160.39s\n",
      "Ações: Manter=14792, Comprar=10292, Vender=11670\n",
      "Ganhos Totais: 29751.75, Perdas Totais: -31518.00\n",
      "Episode 81/100, Total Reward: -38924.50, Total Real Reward: -4710.25, Total Real Profit: -4294.50, Win Rate: 0.52, Wins: 855, Losses: 800, Epsilon: 0.2215, Steps: 36754, Time: 160.66s\n",
      "Ações: Manter=14080, Comprar=9994, Vender=12680\n",
      "Ganhos Totais: 29919.75, Perdas Totais: -34214.25\n",
      "Episode 82/100, Total Reward: -34668.00, Total Real Reward: -3910.50, Total Real Profit: -3558.25, Win Rate: 0.52, Wins: 725, Losses: 682, Epsilon: 0.2193, Steps: 36754, Time: 158.84s\n",
      "Ações: Manter=16918, Comprar=8377, Vender=11459\n",
      "Ganhos Totais: 27199.25, Perdas Totais: -30757.50\n",
      "Episode 83/100, Total Reward: -34594.00, Total Real Reward: -1633.50, Total Real Profit: -1235.25, Win Rate: 0.53, Wins: 840, Losses: 749, Epsilon: 0.2171, Steps: 36754, Time: 160.34s\n",
      "Ações: Manter=12834, Comprar=8714, Vender=15206\n",
      "Ganhos Totais: 31725.25, Perdas Totais: -32960.50\n",
      "Episode 84/100, Total Reward: -35356.50, Total Real Reward: -4448.75, Total Real Profit: -4091.00, Win Rate: 0.50, Wins: 712, Losses: 713, Epsilon: 0.2149, Steps: 36754, Time: 157.61s\n",
      "Ações: Manter=16662, Comprar=8250, Vender=11842\n",
      "Ganhos Totais: 26816.75, Perdas Totais: -30907.75\n",
      "Episode 85/100, Total Reward: -27645.75, Total Real Reward: 653.50, Total Real Profit: 1017.25, Win Rate: 0.53, Wins: 764, Losses: 687, Epsilon: 0.2128, Steps: 36754, Time: 155.97s\n",
      "Ações: Manter=17784, Comprar=8667, Vender=10303\n",
      "Ganhos Totais: 29316.50, Perdas Totais: -28299.25\n",
      "Modelo e log do episódio 85 salvos em: 4.13\\model_episode_85.pth e 4.13\\log_episode_85.csv\n",
      "\n",
      "Episode 86/100, Total Reward: -30732.50, Total Real Reward: -458.25, Total Real Profit: -50.75, Win Rate: 0.53, Wins: 862, Losses: 764, Epsilon: 0.2107, Steps: 36754, Time: 156.53s\n",
      "Ações: Manter=15473, Comprar=9730, Vender=11551\n",
      "Ganhos Totais: 30223.50, Perdas Totais: -30274.25\n",
      "Modelo e log do episódio 86 salvos em: 4.13\\model_episode_86.pth e 4.13\\log_episode_86.csv\n",
      "\n",
      "Episode 87/100, Total Reward: -38374.00, Total Real Reward: -5749.25, Total Real Profit: -5396.25, Win Rate: 0.52, Wins: 725, Losses: 682, Epsilon: 0.2086, Steps: 36754, Time: 160.09s\n",
      "Ações: Manter=17302, Comprar=6866, Vender=12586\n",
      "Ganhos Totais: 27228.50, Perdas Totais: -32624.75\n",
      "Episode 88/100, Total Reward: -31893.75, Total Real Reward: -2651.00, Total Real Profit: -2309.25, Win Rate: 0.50, Wins: 685, Losses: 675, Epsilon: 0.2065, Steps: 36754, Time: 161.76s\n",
      "Ações: Manter=18034, Comprar=8758, Vender=9962\n",
      "Ganhos Totais: 26933.50, Perdas Totais: -29242.75\n",
      "Episode 89/100, Total Reward: -30777.25, Total Real Reward: -902.25, Total Real Profit: -528.75, Win Rate: 0.53, Wins: 787, Losses: 696, Epsilon: 0.2044, Steps: 36754, Time: 160.87s\n",
      "Ações: Manter=16707, Comprar=9425, Vender=10622\n",
      "Ganhos Totais: 29346.25, Perdas Totais: -29875.00\n",
      "Modelo e log do episódio 89 salvos em: 4.13\\model_episode_89.pth e 4.13\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: -30220.75, Total Real Reward: -995.25, Total Real Profit: -645.00, Win Rate: 0.53, Wins: 742, Losses: 653, Epsilon: 0.2024, Steps: 36754, Time: 159.00s\n",
      "Ações: Manter=17837, Comprar=9956, Vender=8961\n",
      "Ganhos Totais: 28580.50, Perdas Totais: -29225.50\n",
      "Modelo e log do episódio 90 salvos em: 4.13\\model_episode_90.pth e 4.13\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: -27898.00, Total Real Reward: 794.00, Total Real Profit: 1156.75, Win Rate: 0.54, Wins: 774, Losses: 670, Epsilon: 0.2003, Steps: 36754, Time: 162.06s\n",
      "Ações: Manter=17370, Comprar=8728, Vender=10656\n",
      "Ganhos Totais: 29848.75, Perdas Totais: -28692.00\n",
      "Modelo e log do episódio 91 salvos em: 4.13\\model_episode_91.pth e 4.13\\log_episode_91.csv\n",
      "\n",
      "Episode 92/100, Total Reward: -28005.50, Total Real Reward: 459.00, Total Real Profit: 862.75, Win Rate: 0.53, Wins: 848, Losses: 752, Epsilon: 0.1983, Steps: 36754, Time: 158.81s\n",
      "Ações: Manter=18433, Comprar=8752, Vender=9569\n",
      "Ganhos Totais: 29327.25, Perdas Totais: -28464.50\n",
      "Modelo e log do episódio 92 salvos em: 4.13\\model_episode_92.pth e 4.13\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: -29130.50, Total Real Reward: 409.75, Total Real Profit: 753.75, Win Rate: 0.53, Wins: 722, Losses: 651, Epsilon: 0.1964, Steps: 36754, Time: 163.52s\n",
      "Ações: Manter=17938, Comprar=9486, Vender=9330\n",
      "Ganhos Totais: 30294.00, Perdas Totais: -29540.25\n",
      "Modelo e log do episódio 93 salvos em: 4.13\\model_episode_93.pth e 4.13\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: -28168.00, Total Real Reward: 446.25, Total Real Profit: 796.00, Win Rate: 0.52, Wins: 729, Losses: 668, Epsilon: 0.1944, Steps: 36754, Time: 162.44s\n",
      "Ações: Manter=18077, Comprar=10117, Vender=8560\n",
      "Ganhos Totais: 29410.25, Perdas Totais: -28614.25\n",
      "Modelo e log do episódio 94 salvos em: 4.13\\model_episode_94.pth e 4.13\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: -28767.50, Total Real Reward: -211.50, Total Real Profit: 163.00, Win Rate: 0.54, Wins: 797, Losses: 692, Epsilon: 0.1924, Steps: 36754, Time: 163.21s\n",
      "Ações: Manter=18427, Comprar=8278, Vender=10049\n",
      "Ganhos Totais: 28719.00, Perdas Totais: -28556.00\n",
      "Modelo e log do episódio 95 salvos em: 4.13\\model_episode_95.pth e 4.13\\log_episode_95.csv\n",
      "\n",
      "Episode 96/100, Total Reward: -31104.25, Total Real Reward: -510.25, Total Real Profit: -136.00, Win Rate: 0.53, Wins: 784, Losses: 709, Epsilon: 0.1905, Steps: 36754, Time: 160.35s\n",
      "Ações: Manter=16598, Comprar=9541, Vender=10615\n",
      "Ganhos Totais: 30458.00, Perdas Totais: -30594.00\n",
      "Episode 97/100, Total Reward: -32193.50, Total Real Reward: -3395.50, Total Real Profit: -3058.25, Win Rate: 0.52, Wins: 702, Losses: 644, Epsilon: 0.1886, Steps: 36754, Time: 157.40s\n",
      "Ações: Manter=19110, Comprar=7727, Vender=9917\n",
      "Ganhos Totais: 25739.75, Perdas Totais: -28798.00\n",
      "Episode 98/100, Total Reward: -25236.00, Total Real Reward: 2621.00, Total Real Profit: 2983.25, Win Rate: 0.55, Wins: 799, Losses: 647, Epsilon: 0.1867, Steps: 36754, Time: 151.15s\n",
      "Ações: Manter=15993, Comprar=9754, Vender=11007\n",
      "Ganhos Totais: 30840.25, Perdas Totais: -27857.00\n",
      "Modelo e log do episódio 98 salvos em: 4.13\\model_episode_98.pth e 4.13\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: -31966.25, Total Real Reward: -793.50, Total Real Profit: -400.75, Win Rate: 0.51, Wins: 799, Losses: 767, Epsilon: 0.1849, Steps: 36754, Time: 134.31s\n",
      "Ações: Manter=16147, Comprar=11063, Vender=9544\n",
      "Ganhos Totais: 30772.00, Perdas Totais: -31172.75\n",
      "Episode 100/100, Total Reward: -29844.25, Total Real Reward: -1025.50, Total Real Profit: -691.25, Win Rate: 0.54, Wins: 716, Losses: 614, Epsilon: 0.1830, Steps: 36754, Time: 133.61s\n",
      "Ações: Manter=17748, Comprar=9542, Vender=9464\n",
      "Ganhos Totais: 28127.50, Perdas Totais: -28818.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 98, Total Reward: -25236.00, Total Real Reward: 2621.00, Total Real Profit: 2983.25, Win Rate: 0.55, Wins: 799, Losses: 647, Ações: {0: 15993, 1: 9754, 2: 11007}, Steps: 36754, Time: 151.15s\n",
      "Rank 2: Episode 85, Total Reward: -27645.75, Total Real Reward: 653.50, Total Real Profit: 1017.25, Win Rate: 0.53, Wins: 764, Losses: 687, Ações: {0: 17784, 1: 8667, 2: 10303}, Steps: 36754, Time: 155.97s\n",
      "Rank 3: Episode 91, Total Reward: -27898.00, Total Real Reward: 794.00, Total Real Profit: 1156.75, Win Rate: 0.54, Wins: 774, Losses: 670, Ações: {0: 17370, 1: 8728, 2: 10656}, Steps: 36754, Time: 162.06s\n",
      "Rank 4: Episode 92, Total Reward: -28005.50, Total Real Reward: 459.00, Total Real Profit: 862.75, Win Rate: 0.53, Wins: 848, Losses: 752, Ações: {0: 18433, 1: 8752, 2: 9569}, Steps: 36754, Time: 158.81s\n",
      "Rank 5: Episode 94, Total Reward: -28168.00, Total Real Reward: 446.25, Total Real Profit: 796.00, Win Rate: 0.52, Wins: 729, Losses: 668, Ações: {0: 18077, 1: 10117, 2: 8560}, Steps: 36754, Time: 162.44s\n",
      "Rank 6: Episode 95, Total Reward: -28767.50, Total Real Reward: -211.50, Total Real Profit: 163.00, Win Rate: 0.54, Wins: 797, Losses: 692, Ações: {0: 18427, 1: 8278, 2: 10049}, Steps: 36754, Time: 163.21s\n",
      "Rank 7: Episode 62, Total Reward: -28998.50, Total Real Reward: 1644.00, Total Real Profit: 2091.50, Win Rate: 0.52, Wins: 935, Losses: 847, Ações: {0: 15391, 1: 8948, 2: 12415}, Steps: 36754, Time: 150.94s\n",
      "Rank 8: Episode 71, Total Reward: -29083.50, Total Real Reward: 1546.25, Total Real Profit: 1942.75, Win Rate: 0.52, Wins: 817, Losses: 766, Ações: {0: 13958, 1: 8591, 2: 14205}, Steps: 36754, Time: 151.42s\n",
      "Rank 9: Episode 93, Total Reward: -29130.50, Total Real Reward: 409.75, Total Real Profit: 753.75, Win Rate: 0.53, Wins: 722, Losses: 651, Ações: {0: 17938, 1: 9486, 2: 9330}, Steps: 36754, Time: 163.52s\n",
      "Rank 10: Episode 67, Total Reward: -29804.00, Total Real Reward: 1434.00, Total Real Profit: 1846.25, Win Rate: 0.53, Wins: 871, Losses: 772, Ações: {0: 13895, 1: 10831, 2: 12028}, Steps: 36754, Time: 150.87s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "        # Alteração 1: Variáveis para Recompensa e Lucro Reais\n",
    "        self.reward_real = 0.0\n",
    "        self.profit_real = 0.0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        self.reward_real = 0.0  # Resetar recompensa real\n",
    "        self.profit_real = 0.0  # Resetar lucro real\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Alteração 2: Inicializar variáveis para recompensa e lucro reais\n",
    "        profit_real = 0  # Lucro real da operação\n",
    "        reward_real = 0  # Recompensa real da operação\n",
    "        reward = 0       # Recompensa modificada para o agente\n",
    "        penalty_factor = 2.0  # Fator de penalidade (pode ajustar conforme necessário)\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    reward_real -= 0.25  # Custo real\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit_real = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.profit_real += profit_real\n",
    "                    reward_real += profit_real\n",
    "                    # Aplicar punição mais severa se houve perda\n",
    "                    if profit_real < 0:\n",
    "                        reward += profit_real * penalty_factor\n",
    "                    else:\n",
    "                        reward += profit_real\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real,\n",
    "                        'reward_real': reward_real\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    reward_real -= 0.25  # Custo real\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit_real = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.profit_real += profit_real\n",
    "                    reward_real += profit_real\n",
    "                    # Aplicar punição mais severa se houve perda\n",
    "                    if profit_real < 0:\n",
    "                        reward += profit_real * penalty_factor\n",
    "                    else:\n",
    "                        reward += profit_real\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real,\n",
    "                        'reward_real': reward_real\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit_real = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.profit_real += profit_real\n",
    "                reward_real += profit_real\n",
    "                # Aplicar punição mais severa se houve perda\n",
    "                if profit_real < 0:\n",
    "                    reward += profit_real * penalty_factor\n",
    "                else:\n",
    "                    reward += profit_real\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real,\n",
    "                    'reward_real': reward_real\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit_real = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.profit_real += profit_real\n",
    "                reward_real += profit_real\n",
    "                # Aplicar punição mais severa se houve perda\n",
    "                if profit_real < 0:\n",
    "                    reward += profit_real * penalty_factor\n",
    "                else:\n",
    "                    reward += profit_real\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real,\n",
    "                    'reward_real': reward_real\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar as recompensas reais\n",
    "        self.reward_real += reward_real\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.13\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_reward_real = 0  # Total da recompensa real\n",
    "    total_profit_real = 0  # Total do lucro real\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Atualizar recompensas e lucros reais\n",
    "        total_reward_real = env.reward_real\n",
    "        total_profit_real = env.profit_real\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, \"\n",
    "          f\"Total Real Reward: {total_reward_real:.2f}, Total Real Profit: {total_profit_real:.2f}, \"\n",
    "          f\"Win Rate: {win_rate:.2f}, Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'total_reward_real': total_reward_real,\n",
    "        'total_profit_real': total_profit_real,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Total Real Reward: {ep['total_reward_real']:.2f}, Total Real Profit: {ep['total_profit_real']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
