{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -5042.00, Win Rate: 0.50, Wins: 1438, Losses: 1431, Epsilon: 0.4950, Steps: 36754, Time: 142.16s\n",
      "Ações: Manter=11591, Comprar=12412, Vender=12751\n",
      "Ganhos Totais: 35547.25, Perdas Totais: -39868.00\n",
      "Modelo e log do episódio 1 salvos em: 4.15\\model_episode_1.pth e 4.15\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -2773.00, Win Rate: 0.51, Wins: 1406, Losses: 1346, Epsilon: 0.4900, Steps: 36754, Time: 147.88s\n",
      "Ações: Manter=10315, Comprar=11260, Vender=15179\n",
      "Ganhos Totais: 37108.25, Perdas Totais: -39189.00\n",
      "Modelo e log do episódio 2 salvos em: 4.15\\model_episode_2.pth e 4.15\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1560.25, Win Rate: 0.51, Wins: 1444, Losses: 1373, Epsilon: 0.4851, Steps: 36754, Time: 175.63s\n",
      "Ações: Manter=11520, Comprar=11734, Vender=13500\n",
      "Ganhos Totais: 36917.00, Perdas Totais: -37771.00\n",
      "Modelo e log do episódio 3 salvos em: 4.15\\model_episode_3.pth e 4.15\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -4003.75, Win Rate: 0.50, Wins: 1429, Losses: 1423, Epsilon: 0.4803, Steps: 36754, Time: 165.63s\n",
      "Ações: Manter=11068, Comprar=11422, Vender=14264\n",
      "Ganhos Totais: 35846.25, Perdas Totais: -39133.00\n",
      "Modelo e log do episódio 4 salvos em: 4.15\\model_episode_4.pth e 4.15\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -1945.00, Win Rate: 0.52, Wins: 1421, Losses: 1322, Epsilon: 0.4755, Steps: 36754, Time: 157.89s\n",
      "Ações: Manter=11782, Comprar=10824, Vender=14148\n",
      "Ganhos Totais: 36028.75, Perdas Totais: -37284.50\n",
      "Modelo e log do episódio 5 salvos em: 4.15\\model_episode_5.pth e 4.15\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -2849.00, Win Rate: 0.50, Wins: 1417, Losses: 1400, Epsilon: 0.4707, Steps: 36754, Time: 139.00s\n",
      "Ações: Manter=11656, Comprar=10806, Vender=14292\n",
      "Ganhos Totais: 36698.25, Perdas Totais: -38841.00\n",
      "Modelo e log do episódio 6 salvos em: 4.15\\model_episode_6.pth e 4.15\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -1661.00, Win Rate: 0.53, Wins: 1462, Losses: 1287, Epsilon: 0.4660, Steps: 36754, Time: 134.78s\n",
      "Ações: Manter=12584, Comprar=11746, Vender=12424\n",
      "Ganhos Totais: 36508.50, Perdas Totais: -37478.00\n",
      "Modelo e log do episódio 7 salvos em: 4.15\\model_episode_7.pth e 4.15\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -4032.00, Win Rate: 0.52, Wins: 1445, Losses: 1336, Epsilon: 0.4614, Steps: 36754, Time: 134.56s\n",
      "Ações: Manter=11775, Comprar=11476, Vender=13503\n",
      "Ganhos Totais: 36588.25, Perdas Totais: -39920.75\n",
      "Modelo e log do episódio 8 salvos em: 4.15\\model_episode_8.pth e 4.15\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -4471.50, Win Rate: 0.50, Wins: 1271, Losses: 1258, Epsilon: 0.4568, Steps: 36754, Time: 134.77s\n",
      "Ações: Manter=13584, Comprar=11891, Vender=11279\n",
      "Ganhos Totais: 34549.75, Perdas Totais: -38386.75\n",
      "Modelo e log do episódio 9 salvos em: 4.15\\model_episode_9.pth e 4.15\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -920.75, Win Rate: 0.52, Wins: 1343, Losses: 1256, Epsilon: 0.4522, Steps: 36754, Time: 141.13s\n",
      "Ações: Manter=12984, Comprar=10582, Vender=13188\n",
      "Ganhos Totais: 36630.00, Perdas Totais: -36896.50\n",
      "Modelo e log do episódio 10 salvos em: 4.15\\model_episode_10.pth e 4.15\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -7267.75, Win Rate: 0.50, Wins: 1301, Losses: 1281, Epsilon: 0.4477, Steps: 36754, Time: 166.08s\n",
      "Ações: Manter=12500, Comprar=10708, Vender=13546\n",
      "Ganhos Totais: 33337.25, Perdas Totais: -39956.50\n",
      "Episode 12/100, Total Reward: -3252.25, Win Rate: 0.51, Wins: 1444, Losses: 1371, Epsilon: 0.4432, Steps: 36754, Time: 160.94s\n",
      "Ações: Manter=10461, Comprar=12320, Vender=13973\n",
      "Ganhos Totais: 38050.00, Perdas Totais: -40593.00\n",
      "Modelo e log do episódio 12 salvos em: 4.15\\model_episode_12.pth e 4.15\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -1093.75, Win Rate: 0.53, Wins: 1435, Losses: 1290, Epsilon: 0.4388, Steps: 36754, Time: 152.70s\n",
      "Ações: Manter=10489, Comprar=13561, Vender=12704\n",
      "Ganhos Totais: 38504.00, Perdas Totais: -38914.75\n",
      "Modelo e log do episódio 13 salvos em: 4.15\\model_episode_13.pth e 4.15\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -93.25, Win Rate: 0.53, Wins: 1395, Losses: 1262, Epsilon: 0.4344, Steps: 36754, Time: 151.41s\n",
      "Ações: Manter=13179, Comprar=11401, Vender=12174\n",
      "Ganhos Totais: 37084.50, Perdas Totais: -36510.75\n",
      "Modelo e log do episódio 14 salvos em: 4.15\\model_episode_14.pth e 4.15\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -1942.25, Win Rate: 0.52, Wins: 1471, Losses: 1372, Epsilon: 0.4300, Steps: 36754, Time: 151.36s\n",
      "Ações: Manter=12027, Comprar=10938, Vender=13789\n",
      "Ganhos Totais: 37526.25, Perdas Totais: -38754.50\n",
      "Modelo e log do episódio 15 salvos em: 4.15\\model_episode_15.pth e 4.15\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -2695.25, Win Rate: 0.53, Wins: 1358, Losses: 1227, Epsilon: 0.4257, Steps: 36754, Time: 151.17s\n",
      "Ações: Manter=12457, Comprar=13173, Vender=11124\n",
      "Ganhos Totais: 35700.00, Perdas Totais: -37745.75\n",
      "Modelo e log do episódio 16 salvos em: 4.15\\model_episode_16.pth e 4.15\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: 2136.50, Win Rate: 0.54, Wins: 1409, Losses: 1216, Epsilon: 0.4215, Steps: 36754, Time: 151.79s\n",
      "Ações: Manter=12528, Comprar=11917, Vender=12309\n",
      "Ganhos Totais: 39079.50, Perdas Totais: -36282.75\n",
      "Modelo e log do episódio 17 salvos em: 4.15\\model_episode_17.pth e 4.15\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -2237.00, Win Rate: 0.53, Wins: 1415, Losses: 1269, Epsilon: 0.4173, Steps: 36754, Time: 152.09s\n",
      "Ações: Manter=12317, Comprar=10670, Vender=13767\n",
      "Ganhos Totais: 36743.25, Perdas Totais: -38305.25\n",
      "Modelo e log do episódio 18 salvos em: 4.15\\model_episode_18.pth e 4.15\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: 1476.00, Win Rate: 0.54, Wins: 1385, Losses: 1161, Epsilon: 0.4131, Steps: 36754, Time: 151.80s\n",
      "Ações: Manter=13655, Comprar=12194, Vender=10905\n",
      "Ganhos Totais: 37863.00, Perdas Totais: -35748.00\n",
      "Modelo e log do episódio 19 salvos em: 4.15\\model_episode_19.pth e 4.15\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: -4213.75, Win Rate: 0.52, Wins: 1304, Losses: 1220, Epsilon: 0.4090, Steps: 36754, Time: 152.46s\n",
      "Ações: Manter=12758, Comprar=12726, Vender=11270\n",
      "Ganhos Totais: 34559.00, Perdas Totais: -38138.00\n",
      "Episode 21/100, Total Reward: -4181.50, Win Rate: 0.51, Wins: 1353, Losses: 1301, Epsilon: 0.4049, Steps: 36754, Time: 152.17s\n",
      "Ações: Manter=12965, Comprar=12043, Vender=11746\n",
      "Ganhos Totais: 35043.75, Perdas Totais: -38558.50\n",
      "Episode 22/100, Total Reward: -2283.50, Win Rate: 0.53, Wins: 1433, Losses: 1253, Epsilon: 0.4008, Steps: 36754, Time: 152.24s\n",
      "Ações: Manter=11748, Comprar=13130, Vender=11876\n",
      "Ganhos Totais: 36766.75, Perdas Totais: -38375.75\n",
      "Episode 23/100, Total Reward: -3979.00, Win Rate: 0.52, Wins: 1354, Losses: 1256, Epsilon: 0.3968, Steps: 36754, Time: 153.21s\n",
      "Ações: Manter=12943, Comprar=12418, Vender=11393\n",
      "Ganhos Totais: 36491.00, Perdas Totais: -39813.75\n",
      "Episode 24/100, Total Reward: -1952.75, Win Rate: 0.53, Wins: 1426, Losses: 1272, Epsilon: 0.3928, Steps: 36754, Time: 153.18s\n",
      "Ações: Manter=11232, Comprar=14412, Vender=11110\n",
      "Ganhos Totais: 37184.50, Perdas Totais: -38460.25\n",
      "Modelo e log do episódio 24 salvos em: 4.15\\model_episode_24.pth e 4.15\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 63.00, Win Rate: 0.54, Wins: 1484, Losses: 1274, Epsilon: 0.3889, Steps: 36754, Time: 153.02s\n",
      "Ações: Manter=13036, Comprar=13065, Vender=10653\n",
      "Ganhos Totais: 37904.25, Perdas Totais: -37148.50\n",
      "Modelo e log do episódio 25 salvos em: 4.15\\model_episode_25.pth e 4.15\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -481.25, Win Rate: 0.54, Wins: 1488, Losses: 1286, Epsilon: 0.3850, Steps: 36754, Time: 153.25s\n",
      "Ações: Manter=13395, Comprar=11740, Vender=11619\n",
      "Ganhos Totais: 36815.75, Perdas Totais: -36600.75\n",
      "Modelo e log do episódio 26 salvos em: 4.15\\model_episode_26.pth e 4.15\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -1124.75, Win Rate: 0.53, Wins: 1422, Losses: 1257, Epsilon: 0.3812, Steps: 36754, Time: 152.78s\n",
      "Ações: Manter=13212, Comprar=10696, Vender=12846\n",
      "Ganhos Totais: 37182.50, Perdas Totais: -37633.25\n",
      "Modelo e log do episódio 27 salvos em: 4.15\\model_episode_27.pth e 4.15\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: -808.25, Win Rate: 0.53, Wins: 1487, Losses: 1299, Epsilon: 0.3774, Steps: 36754, Time: 152.39s\n",
      "Ações: Manter=11291, Comprar=13255, Vender=12208\n",
      "Ganhos Totais: 37599.25, Perdas Totais: -37708.00\n",
      "Modelo e log do episódio 28 salvos em: 4.15\\model_episode_28.pth e 4.15\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 1919.75, Win Rate: 0.53, Wins: 1340, Losses: 1199, Epsilon: 0.3736, Steps: 36754, Time: 152.67s\n",
      "Ações: Manter=13877, Comprar=10899, Vender=11978\n",
      "Ganhos Totais: 37967.00, Perdas Totais: -35409.25\n",
      "Modelo e log do episódio 29 salvos em: 4.15\\model_episode_29.pth e 4.15\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: -1199.25, Win Rate: 0.54, Wins: 1383, Losses: 1198, Epsilon: 0.3699, Steps: 36754, Time: 153.09s\n",
      "Ações: Manter=13624, Comprar=11321, Vender=11809\n",
      "Ganhos Totais: 35925.00, Perdas Totais: -36476.50\n",
      "Episode 31/100, Total Reward: -3210.75, Win Rate: 0.53, Wins: 1293, Losses: 1145, Epsilon: 0.3662, Steps: 36754, Time: 152.50s\n",
      "Ações: Manter=14067, Comprar=11362, Vender=11325\n",
      "Ganhos Totais: 35537.00, Perdas Totais: -38136.00\n",
      "Episode 32/100, Total Reward: -4653.25, Win Rate: 0.52, Wins: 1373, Losses: 1258, Epsilon: 0.3625, Steps: 36754, Time: 152.80s\n",
      "Ações: Manter=11976, Comprar=11449, Vender=13329\n",
      "Ganhos Totais: 35900.50, Perdas Totais: -39893.50\n",
      "Episode 33/100, Total Reward: -1417.00, Win Rate: 0.53, Wins: 1332, Losses: 1158, Epsilon: 0.3589, Steps: 36754, Time: 153.20s\n",
      "Ações: Manter=13473, Comprar=11280, Vender=12001\n",
      "Ganhos Totais: 35991.00, Perdas Totais: -36783.50\n",
      "Episode 34/100, Total Reward: -1925.75, Win Rate: 0.53, Wins: 1378, Losses: 1202, Epsilon: 0.3553, Steps: 36754, Time: 153.74s\n",
      "Ações: Manter=13501, Comprar=11343, Vender=11910\n",
      "Ganhos Totais: 36144.75, Perdas Totais: -37423.50\n",
      "Episode 35/100, Total Reward: -4548.00, Win Rate: 0.54, Wins: 1311, Losses: 1138, Epsilon: 0.3517, Steps: 36754, Time: 153.13s\n",
      "Ações: Manter=14296, Comprar=9436, Vender=13022\n",
      "Ganhos Totais: 34311.50, Perdas Totais: -38243.75\n",
      "Episode 36/100, Total Reward: 1786.25, Win Rate: 0.54, Wins: 1331, Losses: 1137, Epsilon: 0.3482, Steps: 36754, Time: 153.52s\n",
      "Ações: Manter=14081, Comprar=11138, Vender=11535\n",
      "Ganhos Totais: 36981.50, Perdas Totais: -34574.50\n",
      "Modelo e log do episódio 36 salvos em: 4.15\\model_episode_36.pth e 4.15\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: 461.00, Win Rate: 0.53, Wins: 1509, Losses: 1313, Epsilon: 0.3447, Steps: 36754, Time: 153.73s\n",
      "Ações: Manter=10442, Comprar=12113, Vender=14199\n",
      "Ganhos Totais: 39414.50, Perdas Totais: -38245.00\n",
      "Modelo e log do episódio 37 salvos em: 4.15\\model_episode_37.pth e 4.15\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: -2047.00, Win Rate: 0.53, Wins: 1520, Losses: 1344, Epsilon: 0.3413, Steps: 36754, Time: 153.34s\n",
      "Ações: Manter=11378, Comprar=11006, Vender=14370\n",
      "Ganhos Totais: 38245.50, Perdas Totais: -39572.25\n",
      "Episode 39/100, Total Reward: -1482.50, Win Rate: 0.55, Wins: 1312, Losses: 1089, Epsilon: 0.3379, Steps: 36754, Time: 153.65s\n",
      "Ações: Manter=13655, Comprar=9670, Vender=13429\n",
      "Ganhos Totais: 35201.25, Perdas Totais: -36081.00\n",
      "Episode 40/100, Total Reward: -3121.00, Win Rate: 0.54, Wins: 1324, Losses: 1146, Epsilon: 0.3345, Steps: 36754, Time: 154.30s\n",
      "Ações: Manter=13331, Comprar=10650, Vender=12773\n",
      "Ganhos Totais: 35494.75, Perdas Totais: -37994.75\n",
      "Episode 41/100, Total Reward: -1940.75, Win Rate: 0.53, Wins: 1371, Losses: 1236, Epsilon: 0.3311, Steps: 36754, Time: 154.03s\n",
      "Ações: Manter=12401, Comprar=10813, Vender=13540\n",
      "Ganhos Totais: 35117.50, Perdas Totais: -36403.00\n",
      "Episode 42/100, Total Reward: -3561.00, Win Rate: 0.54, Wins: 1364, Losses: 1162, Epsilon: 0.3278, Steps: 36754, Time: 154.85s\n",
      "Ações: Manter=13260, Comprar=10433, Vender=13061\n",
      "Ganhos Totais: 35183.50, Perdas Totais: -38109.75\n",
      "Episode 43/100, Total Reward: -50.75, Win Rate: 0.54, Wins: 1356, Losses: 1150, Epsilon: 0.3246, Steps: 36754, Time: 154.35s\n",
      "Ações: Manter=12937, Comprar=10238, Vender=13579\n",
      "Ganhos Totais: 37790.25, Perdas Totais: -37211.00\n",
      "Modelo e log do episódio 43 salvos em: 4.15\\model_episode_43.pth e 4.15\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -1464.25, Win Rate: 0.54, Wins: 1410, Losses: 1183, Epsilon: 0.3213, Steps: 36754, Time: 154.45s\n",
      "Ações: Manter=12918, Comprar=10766, Vender=13070\n",
      "Ganhos Totais: 36176.75, Perdas Totais: -36989.25\n",
      "Episode 45/100, Total Reward: -4598.75, Win Rate: 0.55, Wins: 1301, Losses: 1077, Epsilon: 0.3181, Steps: 36754, Time: 154.34s\n",
      "Ações: Manter=14801, Comprar=10556, Vender=11397\n",
      "Ganhos Totais: 33389.00, Perdas Totais: -37389.25\n",
      "Episode 46/100, Total Reward: -1604.25, Win Rate: 0.54, Wins: 1204, Losses: 1029, Epsilon: 0.3149, Steps: 36754, Time: 155.04s\n",
      "Ações: Manter=15120, Comprar=10060, Vender=11574\n",
      "Ganhos Totais: 33393.25, Perdas Totais: -34435.50\n",
      "Episode 47/100, Total Reward: -1864.00, Win Rate: 0.54, Wins: 1361, Losses: 1155, Epsilon: 0.3118, Steps: 36754, Time: 154.56s\n",
      "Ações: Manter=12196, Comprar=12377, Vender=12181\n",
      "Ganhos Totais: 36624.75, Perdas Totais: -37856.25\n",
      "Episode 48/100, Total Reward: -4194.50, Win Rate: 0.51, Wins: 1296, Losses: 1235, Epsilon: 0.3086, Steps: 36754, Time: 154.81s\n",
      "Ações: Manter=13637, Comprar=11186, Vender=11931\n",
      "Ganhos Totais: 34278.50, Perdas Totais: -37837.00\n",
      "Episode 49/100, Total Reward: -6046.50, Win Rate: 0.52, Wins: 1158, Losses: 1084, Epsilon: 0.3056, Steps: 36754, Time: 176.08s\n",
      "Ações: Manter=15058, Comprar=9864, Vender=11832\n",
      "Ganhos Totais: 32705.25, Perdas Totais: -38188.75\n",
      "Episode 50/100, Total Reward: -6697.75, Win Rate: 0.54, Wins: 1205, Losses: 1030, Epsilon: 0.3025, Steps: 36754, Time: 180.44s\n",
      "Ações: Manter=14082, Comprar=10484, Vender=12188\n",
      "Ganhos Totais: 33008.50, Perdas Totais: -39144.75\n",
      "Episode 51/100, Total Reward: -2034.25, Win Rate: 0.55, Wins: 1317, Losses: 1088, Epsilon: 0.2995, Steps: 36754, Time: 177.13s\n",
      "Ações: Manter=12288, Comprar=11442, Vender=13024\n",
      "Ganhos Totais: 35700.00, Perdas Totais: -37130.00\n",
      "Episode 52/100, Total Reward: -4738.00, Win Rate: 0.52, Wins: 1147, Losses: 1042, Epsilon: 0.2965, Steps: 36754, Time: 176.94s\n",
      "Ações: Manter=13288, Comprar=12735, Vender=10731\n",
      "Ganhos Totais: 32949.75, Perdas Totais: -37139.00\n",
      "Episode 53/100, Total Reward: -3522.75, Win Rate: 0.53, Wins: 1132, Losses: 1008, Epsilon: 0.2935, Steps: 36754, Time: 178.09s\n",
      "Ações: Manter=13358, Comprar=8350, Vender=15046\n",
      "Ganhos Totais: 32618.75, Perdas Totais: -35604.75\n",
      "Episode 54/100, Total Reward: 2161.00, Win Rate: 0.55, Wins: 1279, Losses: 1053, Epsilon: 0.2906, Steps: 36754, Time: 177.13s\n",
      "Ações: Manter=15087, Comprar=10680, Vender=10987\n",
      "Ganhos Totais: 36229.25, Perdas Totais: -33483.25\n",
      "Modelo e log do episódio 54 salvos em: 4.15\\model_episode_54.pth e 4.15\\log_episode_54.csv\n",
      "\n",
      "Episode 55/100, Total Reward: -3722.75, Win Rate: 0.54, Wins: 1340, Losses: 1161, Epsilon: 0.2877, Steps: 36754, Time: 176.96s\n",
      "Ações: Manter=11145, Comprar=12899, Vender=12710\n",
      "Ganhos Totais: 35513.75, Perdas Totais: -38609.25\n",
      "Episode 56/100, Total Reward: -3963.75, Win Rate: 0.56, Wins: 1319, Losses: 1042, Epsilon: 0.2848, Steps: 36754, Time: 177.62s\n",
      "Ações: Manter=13346, Comprar=12449, Vender=10959\n",
      "Ganhos Totais: 35460.25, Perdas Totais: -38830.50\n",
      "Episode 57/100, Total Reward: -3193.00, Win Rate: 0.53, Wins: 1168, Losses: 1017, Epsilon: 0.2820, Steps: 36754, Time: 178.82s\n",
      "Ações: Manter=14087, Comprar=11022, Vender=11645\n",
      "Ganhos Totais: 34121.50, Perdas Totais: -36766.00\n",
      "Episode 58/100, Total Reward: -5879.25, Win Rate: 0.55, Wins: 1296, Losses: 1076, Epsilon: 0.2791, Steps: 36754, Time: 178.70s\n",
      "Ações: Manter=13477, Comprar=11174, Vender=12103\n",
      "Ganhos Totais: 33132.50, Perdas Totais: -38416.00\n",
      "Episode 59/100, Total Reward: -4819.75, Win Rate: 0.53, Wins: 1360, Losses: 1201, Epsilon: 0.2763, Steps: 36754, Time: 178.57s\n",
      "Ações: Manter=11851, Comprar=12198, Vender=12705\n",
      "Ganhos Totais: 34617.75, Perdas Totais: -38793.50\n",
      "Episode 60/100, Total Reward: -28.50, Win Rate: 0.54, Wins: 1280, Losses: 1099, Epsilon: 0.2736, Steps: 36754, Time: 178.69s\n",
      "Ações: Manter=12863, Comprar=11375, Vender=12516\n",
      "Ganhos Totais: 36497.00, Perdas Totais: -35929.25\n",
      "Modelo e log do episódio 60 salvos em: 4.15\\model_episode_60.pth e 4.15\\log_episode_60.csv\n",
      "\n",
      "Episode 61/100, Total Reward: -4818.25, Win Rate: 0.55, Wins: 1333, Losses: 1109, Epsilon: 0.2708, Steps: 36754, Time: 180.37s\n",
      "Ações: Manter=12155, Comprar=11514, Vender=13085\n",
      "Ganhos Totais: 34872.00, Perdas Totais: -39077.00\n",
      "Episode 62/100, Total Reward: -4144.25, Win Rate: 0.54, Wins: 1335, Losses: 1143, Epsilon: 0.2681, Steps: 36754, Time: 170.45s\n",
      "Ações: Manter=14223, Comprar=11585, Vender=10946\n",
      "Ganhos Totais: 34770.00, Perdas Totais: -38293.25\n",
      "Episode 63/100, Total Reward: -3508.25, Win Rate: 0.53, Wins: 1271, Losses: 1142, Epsilon: 0.2655, Steps: 36754, Time: 157.05s\n",
      "Ações: Manter=13798, Comprar=11080, Vender=11876\n",
      "Ganhos Totais: 35996.00, Perdas Totais: -38897.00\n",
      "Episode 64/100, Total Reward: -1695.25, Win Rate: 0.56, Wins: 1453, Losses: 1152, Epsilon: 0.2628, Steps: 36754, Time: 160.13s\n",
      "Ações: Manter=12552, Comprar=11461, Vender=12741\n",
      "Ganhos Totais: 36961.75, Perdas Totais: -38003.50\n",
      "Episode 65/100, Total Reward: -3615.25, Win Rate: 0.53, Wins: 1244, Losses: 1100, Epsilon: 0.2602, Steps: 36754, Time: 153.60s\n",
      "Ações: Manter=13692, Comprar=11506, Vender=11556\n",
      "Ganhos Totais: 34216.00, Perdas Totais: -37240.75\n",
      "Episode 66/100, Total Reward: -3123.00, Win Rate: 0.53, Wins: 1406, Losses: 1234, Epsilon: 0.2576, Steps: 36754, Time: 158.08s\n",
      "Ações: Manter=12981, Comprar=10613, Vender=13160\n",
      "Ganhos Totais: 34646.50, Perdas Totais: -37107.75\n",
      "Episode 67/100, Total Reward: 28.25, Win Rate: 0.53, Wins: 1390, Losses: 1212, Epsilon: 0.2550, Steps: 36754, Time: 162.62s\n",
      "Ações: Manter=12167, Comprar=14887, Vender=9700\n",
      "Ganhos Totais: 35470.50, Perdas Totais: -34789.25\n",
      "Modelo e log do episódio 67 salvos em: 4.15\\model_episode_67.pth e 4.15\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: -994.50, Win Rate: 0.54, Wins: 1295, Losses: 1090, Epsilon: 0.2524, Steps: 36754, Time: 172.00s\n",
      "Ações: Manter=14431, Comprar=10671, Vender=11652\n",
      "Ganhos Totais: 36011.50, Perdas Totais: -36407.75\n",
      "Episode 69/100, Total Reward: -1805.50, Win Rate: 0.53, Wins: 1340, Losses: 1200, Epsilon: 0.2499, Steps: 36754, Time: 144.42s\n",
      "Ações: Manter=13921, Comprar=12455, Vender=10378\n",
      "Ganhos Totais: 36020.00, Perdas Totais: -37188.50\n",
      "Episode 70/100, Total Reward: -4513.50, Win Rate: 0.54, Wins: 1215, Losses: 1023, Epsilon: 0.2474, Steps: 36754, Time: 159.33s\n",
      "Ações: Manter=15399, Comprar=11648, Vender=9707\n",
      "Ganhos Totais: 32417.50, Perdas Totais: -36369.75\n",
      "Episode 71/100, Total Reward: -7479.50, Win Rate: 0.52, Wins: 1118, Losses: 1041, Epsilon: 0.2449, Steps: 36754, Time: 154.25s\n",
      "Ações: Manter=14480, Comprar=11875, Vender=10399\n",
      "Ganhos Totais: 31036.25, Perdas Totais: -37975.25\n",
      "Episode 72/100, Total Reward: -2061.75, Win Rate: 0.54, Wins: 1298, Losses: 1087, Epsilon: 0.2425, Steps: 36754, Time: 150.00s\n",
      "Ações: Manter=13781, Comprar=12348, Vender=10625\n",
      "Ganhos Totais: 35578.00, Perdas Totais: -37040.50\n",
      "Episode 73/100, Total Reward: -1327.00, Win Rate: 0.55, Wins: 1406, Losses: 1158, Epsilon: 0.2401, Steps: 36754, Time: 154.40s\n",
      "Ações: Manter=10684, Comprar=14626, Vender=11444\n",
      "Ganhos Totais: 36503.25, Perdas Totais: -37185.25\n",
      "Episode 74/100, Total Reward: -942.75, Win Rate: 0.55, Wins: 1427, Losses: 1157, Epsilon: 0.2377, Steps: 36754, Time: 155.44s\n",
      "Ações: Manter=12310, Comprar=14261, Vender=10183\n",
      "Ganhos Totais: 37723.25, Perdas Totais: -38016.50\n",
      "Episode 75/100, Total Reward: 1281.50, Win Rate: 0.55, Wins: 1559, Losses: 1286, Epsilon: 0.2353, Steps: 36754, Time: 151.32s\n",
      "Ações: Manter=10039, Comprar=16008, Vender=10707\n",
      "Ganhos Totais: 38503.00, Perdas Totais: -36505.25\n",
      "Modelo e log do episódio 75 salvos em: 4.15\\model_episode_75.pth e 4.15\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: -4853.75, Win Rate: 0.54, Wins: 1366, Losses: 1163, Epsilon: 0.2329, Steps: 36754, Time: 140.79s\n",
      "Ações: Manter=12157, Comprar=15034, Vender=9563\n",
      "Ganhos Totais: 34577.50, Perdas Totais: -38796.75\n",
      "Episode 77/100, Total Reward: -4406.25, Win Rate: 0.54, Wins: 1370, Losses: 1165, Epsilon: 0.2306, Steps: 36754, Time: 138.45s\n",
      "Ações: Manter=13428, Comprar=13937, Vender=9389\n",
      "Ganhos Totais: 32907.25, Perdas Totais: -36675.50\n",
      "Episode 78/100, Total Reward: -3631.00, Win Rate: 0.53, Wins: 1294, Losses: 1147, Epsilon: 0.2283, Steps: 36754, Time: 152.45s\n",
      "Ações: Manter=13978, Comprar=12663, Vender=10113\n",
      "Ganhos Totais: 33799.50, Perdas Totais: -36817.00\n",
      "Episode 79/100, Total Reward: -903.75, Win Rate: 0.55, Wins: 1462, Losses: 1218, Epsilon: 0.2260, Steps: 36754, Time: 142.30s\n",
      "Ações: Manter=11017, Comprar=14756, Vender=10981\n",
      "Ganhos Totais: 36870.25, Perdas Totais: -37101.75\n",
      "Episode 80/100, Total Reward: -1762.00, Win Rate: 0.54, Wins: 1304, Losses: 1131, Epsilon: 0.2238, Steps: 36754, Time: 152.32s\n",
      "Ações: Manter=12100, Comprar=14477, Vender=10177\n",
      "Ganhos Totais: 34889.25, Perdas Totais: -36038.75\n",
      "Episode 81/100, Total Reward: -996.25, Win Rate: 0.55, Wins: 1173, Losses: 979, Epsilon: 0.2215, Steps: 36754, Time: 151.41s\n",
      "Ações: Manter=13826, Comprar=14409, Vender=8519\n",
      "Ganhos Totais: 31939.50, Perdas Totais: -32395.75\n",
      "Episode 82/100, Total Reward: 1001.50, Win Rate: 0.55, Wins: 1356, Losses: 1125, Epsilon: 0.2193, Steps: 36754, Time: 144.85s\n",
      "Ações: Manter=12078, Comprar=15210, Vender=9466\n",
      "Ganhos Totais: 35911.75, Perdas Totais: -34287.00\n",
      "Modelo e log do episódio 82 salvos em: 4.15\\model_episode_82.pth e 4.15\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: -3665.75, Win Rate: 0.55, Wins: 1371, Losses: 1143, Epsilon: 0.2171, Steps: 36754, Time: 134.57s\n",
      "Ações: Manter=13511, Comprar=12612, Vender=10631\n",
      "Ganhos Totais: 32826.50, Perdas Totais: -35859.75\n",
      "Episode 84/100, Total Reward: -3169.50, Win Rate: 0.54, Wins: 1236, Losses: 1061, Epsilon: 0.2149, Steps: 36754, Time: 139.57s\n",
      "Ações: Manter=14311, Comprar=12066, Vender=10377\n",
      "Ganhos Totais: 34086.75, Perdas Totais: -36679.50\n",
      "Episode 85/100, Total Reward: -48.00, Win Rate: 0.55, Wins: 1312, Losses: 1063, Epsilon: 0.2128, Steps: 36754, Time: 150.40s\n",
      "Ações: Manter=15625, Comprar=11673, Vender=9456\n",
      "Ganhos Totais: 35798.00, Perdas Totais: -35249.50\n",
      "Episode 86/100, Total Reward: -1504.50, Win Rate: 0.55, Wins: 1380, Losses: 1125, Epsilon: 0.2107, Steps: 36754, Time: 140.70s\n",
      "Ações: Manter=13147, Comprar=13497, Vender=10110\n",
      "Ganhos Totais: 36393.25, Perdas Totais: -37267.25\n",
      "Episode 87/100, Total Reward: -1080.75, Win Rate: 0.56, Wins: 1394, Losses: 1108, Epsilon: 0.2086, Steps: 36754, Time: 138.28s\n",
      "Ações: Manter=12495, Comprar=13829, Vender=10430\n",
      "Ganhos Totais: 35693.50, Perdas Totais: -36145.00\n",
      "Episode 88/100, Total Reward: -1451.50, Win Rate: 0.56, Wins: 1369, Losses: 1085, Epsilon: 0.2065, Steps: 36754, Time: 145.83s\n",
      "Ações: Manter=13831, Comprar=13356, Vender=9567\n",
      "Ganhos Totais: 35277.00, Perdas Totais: -36112.50\n",
      "Episode 89/100, Total Reward: -3003.00, Win Rate: 0.56, Wins: 1300, Losses: 1038, Epsilon: 0.2044, Steps: 36754, Time: 141.72s\n",
      "Ações: Manter=14498, Comprar=11782, Vender=10474\n",
      "Ganhos Totais: 33580.50, Perdas Totais: -35997.25\n",
      "Episode 90/100, Total Reward: -5351.50, Win Rate: 0.53, Wins: 1279, Losses: 1114, Epsilon: 0.2024, Steps: 36754, Time: 138.03s\n",
      "Ações: Manter=13886, Comprar=12288, Vender=10580\n",
      "Ganhos Totais: 33654.25, Perdas Totais: -38406.50\n",
      "Episode 91/100, Total Reward: -1943.50, Win Rate: 0.56, Wins: 1240, Losses: 977, Epsilon: 0.2003, Steps: 36754, Time: 138.07s\n",
      "Ações: Manter=15417, Comprar=11494, Vender=9843\n",
      "Ganhos Totais: 34137.75, Perdas Totais: -35524.00\n",
      "Episode 92/100, Total Reward: -2066.00, Win Rate: 0.56, Wins: 1306, Losses: 1028, Epsilon: 0.1983, Steps: 36754, Time: 138.46s\n",
      "Ações: Manter=13513, Comprar=13103, Vender=10138\n",
      "Ganhos Totais: 33910.75, Perdas Totais: -35390.75\n",
      "Episode 93/100, Total Reward: -3303.50, Win Rate: 0.54, Wins: 1165, Losses: 982, Epsilon: 0.1964, Steps: 36754, Time: 138.03s\n",
      "Ações: Manter=15156, Comprar=10112, Vender=11486\n",
      "Ganhos Totais: 33214.75, Perdas Totais: -35978.50\n",
      "Episode 94/100, Total Reward: -1115.75, Win Rate: 0.56, Wins: 1247, Losses: 998, Epsilon: 0.1944, Steps: 36754, Time: 137.57s\n",
      "Ações: Manter=16121, Comprar=10994, Vender=9639\n",
      "Ganhos Totais: 33214.50, Perdas Totais: -33766.25\n",
      "Episode 95/100, Total Reward: -3485.75, Win Rate: 0.55, Wins: 1330, Losses: 1102, Epsilon: 0.1924, Steps: 36754, Time: 137.68s\n",
      "Ações: Manter=12501, Comprar=12826, Vender=11427\n",
      "Ganhos Totais: 34858.00, Perdas Totais: -37731.25\n",
      "Episode 96/100, Total Reward: -2945.00, Win Rate: 0.55, Wins: 1320, Losses: 1083, Epsilon: 0.1905, Steps: 36754, Time: 138.62s\n",
      "Ações: Manter=12961, Comprar=14219, Vender=9574\n",
      "Ganhos Totais: 33958.25, Perdas Totais: -36300.25\n",
      "Episode 97/100, Total Reward: 1921.75, Win Rate: 0.56, Wins: 1248, Losses: 996, Epsilon: 0.1886, Steps: 36754, Time: 142.15s\n",
      "Ações: Manter=12781, Comprar=12568, Vender=11405\n",
      "Ganhos Totais: 35137.50, Perdas Totais: -32653.25\n",
      "Modelo e log do episódio 97 salvos em: 4.15\\model_episode_97.pth e 4.15\\log_episode_97.csv\n",
      "\n",
      "Episode 98/100, Total Reward: 259.00, Win Rate: 0.55, Wins: 1294, Losses: 1063, Epsilon: 0.1867, Steps: 36754, Time: 139.31s\n",
      "Ações: Manter=14079, Comprar=11212, Vender=11463\n",
      "Ganhos Totais: 35048.00, Perdas Totais: -34195.25\n",
      "Modelo e log do episódio 98 salvos em: 4.15\\model_episode_98.pth e 4.15\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: -1326.25, Win Rate: 0.54, Wins: 1383, Losses: 1186, Epsilon: 0.1849, Steps: 36754, Time: 138.23s\n",
      "Ações: Manter=12429, Comprar=13545, Vender=10780\n",
      "Ganhos Totais: 35448.00, Perdas Totais: -36130.50\n",
      "Episode 100/100, Total Reward: -3770.25, Win Rate: 0.54, Wins: 1309, Losses: 1123, Epsilon: 0.1830, Steps: 36754, Time: 138.17s\n",
      "Ações: Manter=14304, Comprar=13063, Vender=9387\n",
      "Ganhos Totais: 34332.25, Perdas Totais: -37492.00\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 54, Total Reward: 2161.00, Win Rate: 0.55, Wins: 1279, Losses: 1053, Ações: {0: 15087, 1: 10680, 2: 10987}, Steps: 36754, Time: 177.13s\n",
      "Rank 2: Episode 17, Total Reward: 2136.50, Win Rate: 0.54, Wins: 1409, Losses: 1216, Ações: {0: 12528, 1: 11917, 2: 12309}, Steps: 36754, Time: 151.79s\n",
      "Rank 3: Episode 97, Total Reward: 1921.75, Win Rate: 0.56, Wins: 1248, Losses: 996, Ações: {0: 12781, 1: 12568, 2: 11405}, Steps: 36754, Time: 142.15s\n",
      "Rank 4: Episode 29, Total Reward: 1919.75, Win Rate: 0.53, Wins: 1340, Losses: 1199, Ações: {0: 13877, 1: 10899, 2: 11978}, Steps: 36754, Time: 152.67s\n",
      "Rank 5: Episode 36, Total Reward: 1786.25, Win Rate: 0.54, Wins: 1331, Losses: 1137, Ações: {0: 14081, 1: 11138, 2: 11535}, Steps: 36754, Time: 153.52s\n",
      "Rank 6: Episode 19, Total Reward: 1476.00, Win Rate: 0.54, Wins: 1385, Losses: 1161, Ações: {0: 13655, 1: 12194, 2: 10905}, Steps: 36754, Time: 151.80s\n",
      "Rank 7: Episode 75, Total Reward: 1281.50, Win Rate: 0.55, Wins: 1559, Losses: 1286, Ações: {0: 10039, 1: 16008, 2: 10707}, Steps: 36754, Time: 151.32s\n",
      "Rank 8: Episode 82, Total Reward: 1001.50, Win Rate: 0.55, Wins: 1356, Losses: 1125, Ações: {0: 12078, 1: 15210, 2: 9466}, Steps: 36754, Time: 144.85s\n",
      "Rank 9: Episode 37, Total Reward: 461.00, Win Rate: 0.53, Wins: 1509, Losses: 1313, Ações: {0: 10442, 1: 12113, 2: 14199}, Steps: 36754, Time: 153.73s\n",
      "Rank 10: Episode 98, Total Reward: 259.00, Win Rate: 0.55, Wins: 1294, Losses: 1063, Ações: {0: 14079, 1: 11212, 2: 11463}, Steps: 36754, Time: 139.31s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Definir o limite de stop loss\n",
    "        stop_loss = -200  # Limite de perda em pontos\n",
    "\n",
    "        # Verificar stop loss\n",
    "        stop_loss_triggered = False\n",
    "        if self.position != 0:\n",
    "            if self.position == 1:  # Posição comprada\n",
    "                # Calcular lucro não realizado\n",
    "                unrealized_profit = current_price - self.entry_price - 0.25\n",
    "            elif self.position == -1:  # Posição vendida\n",
    "                # Calcular lucro não realizado\n",
    "                unrealized_profit = self.entry_price - current_price - 0.25\n",
    "\n",
    "            # Se o lucro não realizado <= stop_loss, fechar a posição\n",
    "            if unrealized_profit <= stop_loss:\n",
    "                stop_loss_triggered = True\n",
    "                # Fechar a posição\n",
    "                self.exit_price = current_price\n",
    "                profit = unrealized_profit  # Lucro realizado é o lucro não realizado atual\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "\n",
    "                if self.position == 1:\n",
    "                    trade_type = 'close_long_stop_loss'\n",
    "                else:\n",
    "                    trade_type = 'close_short_stop_loss'\n",
    "\n",
    "                info['trade'] = {\n",
    "                    'type': trade_type,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy' if self.position == 1 else 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Processar a ação se a posição não foi fechada pelo stop loss\n",
    "        if not stop_loss_triggered:\n",
    "            # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "            if gatilho == 1:\n",
    "                if action == 1:  # Comprar\n",
    "                    if self.position == 0:\n",
    "                        self.position = 1  # Abrir posição comprada\n",
    "                        self.entry_price = current_price\n",
    "                        self.entry_step = self.current_step\n",
    "                        self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward -= 0.25  # Custo de operação\n",
    "                        info['trade'] = {\n",
    "                            'type': 'buy',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime\n",
    "                        }\n",
    "                    elif self.position == -1:\n",
    "                        # Fechar posição vendida\n",
    "                        self.exit_price = current_price\n",
    "                        profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                        self.exit_step = self.current_step\n",
    "                        self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward += profit\n",
    "                        info['trade'] = {\n",
    "                            'type': 'close_short',\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        }\n",
    "                        # Registrar a operação\n",
    "                        self.trades.append({\n",
    "                            'type': 'sell',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime,\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        })\n",
    "                        # Resetar posição\n",
    "                        self.position = 0\n",
    "                        self.entry_step = None\n",
    "                        self.entry_datetime = None\n",
    "                elif action == 2:  # Vender\n",
    "                    if self.position == 0:\n",
    "                        self.position = -1  # Abrir posição vendida\n",
    "                        self.entry_price = current_price\n",
    "                        self.entry_step = self.current_step\n",
    "                        self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward -= 0.25  # Custo de operação\n",
    "                        info['trade'] = {\n",
    "                            'type': 'sell',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime\n",
    "                        }\n",
    "                    elif self.position == 1:\n",
    "                        # Fechar posição comprada\n",
    "                        self.exit_price = current_price\n",
    "                        profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                        self.exit_step = self.current_step\n",
    "                        self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward += profit\n",
    "                        info['trade'] = {\n",
    "                            'type': 'close_long',\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        }\n",
    "                        # Registrar a operação\n",
    "                        self.trades.append({\n",
    "                            'type': 'buy',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime,\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        })\n",
    "                        # Resetar posição\n",
    "                        self.position = 0\n",
    "                        self.entry_step = None\n",
    "                        self.entry_datetime = None\n",
    "                else:  # Manter\n",
    "                    pass  # Nenhuma ação necessária\n",
    "            else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "                # Se há uma posição aberta, fechá-la\n",
    "                if self.position == 1:  # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "                elif self.position == -1:  # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.15\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short', 'close_long_stop_loss', 'close_short_stop_loss']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
