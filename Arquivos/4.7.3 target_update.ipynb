{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 191.00, Win Rate: 0.52, Wins: 1482, Losses: 1393, Epsilon: 0.4950, Steps: 36754, Time: 114.45s\n",
      "Ações: Manter=10499, Comprar=12527, Vender=13728\n",
      "Ganhos Totais: 38611.50, Perdas Totais: -38420.50\n",
      "Modelo e log do episódio 1 salvos em: 4.7.3\\model_episode_1.pth e 4.7.3\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -2375.50, Win Rate: 0.51, Wins: 1448, Losses: 1405, Epsilon: 0.4900, Steps: 36754, Time: 124.09s\n",
      "Ações: Manter=10107, Comprar=11633, Vender=15014\n",
      "Ganhos Totais: 37149.75, Perdas Totais: -39525.25\n",
      "Modelo e log do episódio 2 salvos em: 4.7.3\\model_episode_2.pth e 4.7.3\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -3416.25, Win Rate: 0.51, Wins: 1342, Losses: 1307, Epsilon: 0.4851, Steps: 36754, Time: 126.92s\n",
      "Ações: Manter=11026, Comprar=11849, Vender=13879\n",
      "Ganhos Totais: 35899.75, Perdas Totais: -39316.00\n",
      "Modelo e log do episódio 3 salvos em: 4.7.3\\model_episode_3.pth e 4.7.3\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -82.00, Win Rate: 0.52, Wins: 1339, Losses: 1260, Epsilon: 0.4803, Steps: 36754, Time: 123.10s\n",
      "Ações: Manter=11096, Comprar=11962, Vender=13696\n",
      "Ganhos Totais: 38097.25, Perdas Totais: -38179.25\n",
      "Modelo e log do episódio 4 salvos em: 4.7.3\\model_episode_4.pth e 4.7.3\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: 1906.00, Win Rate: 0.54, Wins: 1441, Losses: 1245, Epsilon: 0.4755, Steps: 36754, Time: 123.89s\n",
      "Ações: Manter=10708, Comprar=13206, Vender=12840\n",
      "Ganhos Totais: 39179.25, Perdas Totais: -37273.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.3\\model_episode_5.pth e 4.7.3\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -1126.25, Win Rate: 0.51, Wins: 1396, Losses: 1329, Epsilon: 0.4707, Steps: 36754, Time: 124.92s\n",
      "Ações: Manter=11628, Comprar=12025, Vender=13101\n",
      "Ganhos Totais: 37870.25, Perdas Totais: -38996.50\n",
      "Modelo e log do episódio 6 salvos em: 4.7.3\\model_episode_6.pth e 4.7.3\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -4827.50, Win Rate: 0.52, Wins: 1378, Losses: 1267, Epsilon: 0.4660, Steps: 36754, Time: 124.70s\n",
      "Ações: Manter=11925, Comprar=11775, Vender=13054\n",
      "Ganhos Totais: 35570.50, Perdas Totais: -40398.00\n",
      "Modelo e log do episódio 7 salvos em: 4.7.3\\model_episode_7.pth e 4.7.3\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -4795.00, Win Rate: 0.50, Wins: 1274, Losses: 1276, Epsilon: 0.4614, Steps: 36754, Time: 123.62s\n",
      "Ações: Manter=12375, Comprar=11734, Vender=12645\n",
      "Ganhos Totais: 34281.50, Perdas Totais: -39076.50\n",
      "Modelo e log do episódio 8 salvos em: 4.7.3\\model_episode_8.pth e 4.7.3\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: 484.25, Win Rate: 0.53, Wins: 1358, Losses: 1216, Epsilon: 0.4568, Steps: 36754, Time: 123.58s\n",
      "Ações: Manter=12184, Comprar=11737, Vender=12833\n",
      "Ganhos Totais: 37634.75, Perdas Totais: -37150.50\n",
      "Modelo e log do episódio 9 salvos em: 4.7.3\\model_episode_9.pth e 4.7.3\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -759.75, Win Rate: 0.53, Wins: 1388, Losses: 1237, Epsilon: 0.4522, Steps: 36754, Time: 124.05s\n",
      "Ações: Manter=13158, Comprar=11930, Vender=11666\n",
      "Ganhos Totais: 37753.00, Perdas Totais: -38512.75\n",
      "Modelo e log do episódio 10 salvos em: 4.7.3\\model_episode_10.pth e 4.7.3\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -3307.75, Win Rate: 0.50, Wins: 1278, Losses: 1268, Epsilon: 0.4477, Steps: 36754, Time: 123.91s\n",
      "Ações: Manter=12115, Comprar=13326, Vender=11313\n",
      "Ganhos Totais: 35154.75, Perdas Totais: -38462.50\n",
      "Modelo e log do episódio 11 salvos em: 4.7.3\\model_episode_11.pth e 4.7.3\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -6101.25, Win Rate: 0.51, Wins: 1279, Losses: 1213, Epsilon: 0.4432, Steps: 36754, Time: 124.45s\n",
      "Ações: Manter=13324, Comprar=11315, Vender=12115\n",
      "Ganhos Totais: 32976.00, Perdas Totais: -39077.25\n",
      "Episode 13/100, Total Reward: 339.50, Win Rate: 0.54, Wins: 1408, Losses: 1194, Epsilon: 0.4388, Steps: 36754, Time: 124.47s\n",
      "Ações: Manter=12188, Comprar=11549, Vender=13017\n",
      "Ganhos Totais: 37850.00, Perdas Totais: -37510.50\n",
      "Modelo e log do episódio 13 salvos em: 4.7.3\\model_episode_13.pth e 4.7.3\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 2302.50, Win Rate: 0.53, Wins: 1336, Losses: 1195, Epsilon: 0.4344, Steps: 36754, Time: 124.46s\n",
      "Ações: Manter=13018, Comprar=12086, Vender=11650\n",
      "Ganhos Totais: 38781.50, Perdas Totais: -36479.00\n",
      "Modelo e log do episódio 14 salvos em: 4.7.3\\model_episode_14.pth e 4.7.3\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -5182.25, Win Rate: 0.52, Wins: 1284, Losses: 1194, Epsilon: 0.4300, Steps: 36754, Time: 124.18s\n",
      "Ações: Manter=12891, Comprar=12234, Vender=11629\n",
      "Ganhos Totais: 35664.50, Perdas Totais: -40846.75\n",
      "Episode 16/100, Total Reward: -4907.50, Win Rate: 0.54, Wins: 1323, Losses: 1138, Epsilon: 0.4257, Steps: 36754, Time: 125.21s\n",
      "Ações: Manter=13042, Comprar=12344, Vender=11368\n",
      "Ganhos Totais: 33088.00, Perdas Totais: -37995.50\n",
      "Episode 17/100, Total Reward: -2816.75, Win Rate: 0.53, Wins: 1346, Losses: 1210, Epsilon: 0.4215, Steps: 36754, Time: 124.44s\n",
      "Ações: Manter=11771, Comprar=12611, Vender=12372\n",
      "Ganhos Totais: 35022.75, Perdas Totais: -37839.50\n",
      "Modelo e log do episódio 17 salvos em: 4.7.3\\model_episode_17.pth e 4.7.3\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -4199.50, Win Rate: 0.52, Wins: 1256, Losses: 1146, Epsilon: 0.4173, Steps: 36754, Time: 125.27s\n",
      "Ações: Manter=13471, Comprar=13124, Vender=10159\n",
      "Ganhos Totais: 33188.50, Perdas Totais: -37388.00\n",
      "Episode 19/100, Total Reward: -3660.50, Win Rate: 0.52, Wins: 1233, Losses: 1118, Epsilon: 0.4131, Steps: 36754, Time: 125.58s\n",
      "Ações: Manter=13661, Comprar=11282, Vender=11811\n",
      "Ganhos Totais: 34724.25, Perdas Totais: -38384.75\n",
      "Episode 20/100, Total Reward: -388.75, Win Rate: 0.53, Wins: 1254, Losses: 1131, Epsilon: 0.4090, Steps: 36754, Time: 125.52s\n",
      "Ações: Manter=14199, Comprar=11918, Vender=10637\n",
      "Ganhos Totais: 35302.75, Perdas Totais: -35691.50\n",
      "Modelo e log do episódio 20 salvos em: 4.7.3\\model_episode_20.pth e 4.7.3\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -810.25, Win Rate: 0.53, Wins: 1325, Losses: 1171, Epsilon: 0.4049, Steps: 36754, Time: 125.22s\n",
      "Ações: Manter=12701, Comprar=11900, Vender=12153\n",
      "Ganhos Totais: 36402.25, Perdas Totais: -37212.50\n",
      "Modelo e log do episódio 21 salvos em: 4.7.3\\model_episode_21.pth e 4.7.3\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -1044.50, Win Rate: 0.53, Wins: 1274, Losses: 1151, Epsilon: 0.4008, Steps: 36754, Time: 125.62s\n",
      "Ações: Manter=12957, Comprar=10545, Vender=13252\n",
      "Ganhos Totais: 35186.25, Perdas Totais: -36230.75\n",
      "Modelo e log do episódio 22 salvos em: 4.7.3\\model_episode_22.pth e 4.7.3\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: 1855.25, Win Rate: 0.55, Wins: 1376, Losses: 1133, Epsilon: 0.3968, Steps: 36754, Time: 120.97s\n",
      "Ações: Manter=11843, Comprar=13210, Vender=11701\n",
      "Ganhos Totais: 38102.25, Perdas Totais: -36247.00\n",
      "Modelo e log do episódio 23 salvos em: 4.7.3\\model_episode_23.pth e 4.7.3\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: -3975.25, Win Rate: 0.53, Wins: 1354, Losses: 1202, Epsilon: 0.3928, Steps: 36754, Time: 111.54s\n",
      "Ações: Manter=12428, Comprar=11630, Vender=12696\n",
      "Ganhos Totais: 34918.75, Perdas Totais: -38894.00\n",
      "Episode 25/100, Total Reward: -1155.75, Win Rate: 0.53, Wins: 1307, Losses: 1137, Epsilon: 0.3889, Steps: 36754, Time: 109.83s\n",
      "Ações: Manter=13065, Comprar=12815, Vender=10874\n",
      "Ganhos Totais: 35242.00, Perdas Totais: -36397.75\n",
      "Episode 26/100, Total Reward: -448.75, Win Rate: 0.55, Wins: 1470, Losses: 1196, Epsilon: 0.3850, Steps: 36754, Time: 111.36s\n",
      "Ações: Manter=13173, Comprar=12156, Vender=11425\n",
      "Ganhos Totais: 37077.00, Perdas Totais: -37525.75\n",
      "Modelo e log do episódio 26 salvos em: 4.7.3\\model_episode_26.pth e 4.7.3\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -2349.25, Win Rate: 0.54, Wins: 1406, Losses: 1192, Epsilon: 0.3812, Steps: 36754, Time: 109.84s\n",
      "Ações: Manter=11792, Comprar=13304, Vender=11658\n",
      "Ganhos Totais: 36264.00, Perdas Totais: -38613.25\n",
      "Episode 28/100, Total Reward: -7628.50, Win Rate: 0.52, Wins: 1303, Losses: 1210, Epsilon: 0.3774, Steps: 36754, Time: 110.16s\n",
      "Ações: Manter=12745, Comprar=13551, Vender=10458\n",
      "Ganhos Totais: 32338.00, Perdas Totais: -39966.50\n",
      "Episode 29/100, Total Reward: -1274.25, Win Rate: 0.53, Wins: 1350, Losses: 1177, Epsilon: 0.3736, Steps: 36754, Time: 110.11s\n",
      "Ações: Manter=11756, Comprar=13598, Vender=11400\n",
      "Ganhos Totais: 36307.75, Perdas Totais: -37582.00\n",
      "Episode 30/100, Total Reward: -4285.00, Win Rate: 0.54, Wins: 1383, Losses: 1162, Epsilon: 0.3699, Steps: 36754, Time: 109.94s\n",
      "Ações: Manter=13352, Comprar=12598, Vender=10804\n",
      "Ganhos Totais: 33985.75, Perdas Totais: -38270.75\n",
      "Episode 31/100, Total Reward: -4692.25, Win Rate: 0.54, Wins: 1443, Losses: 1213, Epsilon: 0.3662, Steps: 36754, Time: 110.03s\n",
      "Ações: Manter=12617, Comprar=12858, Vender=11279\n",
      "Ganhos Totais: 34561.50, Perdas Totais: -39253.75\n",
      "Episode 32/100, Total Reward: -792.75, Win Rate: 0.55, Wins: 1328, Losses: 1082, Epsilon: 0.3625, Steps: 36754, Time: 109.80s\n",
      "Ações: Manter=12827, Comprar=13730, Vender=10197\n",
      "Ganhos Totais: 36342.00, Perdas Totais: -37134.75\n",
      "Episode 33/100, Total Reward: -1450.00, Win Rate: 0.54, Wins: 1287, Losses: 1077, Epsilon: 0.3589, Steps: 36754, Time: 112.07s\n",
      "Ações: Manter=12585, Comprar=13966, Vender=10203\n",
      "Ganhos Totais: 35124.00, Perdas Totais: -36574.00\n",
      "Episode 34/100, Total Reward: -508.50, Win Rate: 0.57, Wins: 1432, Losses: 1089, Epsilon: 0.3553, Steps: 36754, Time: 109.91s\n",
      "Ações: Manter=12359, Comprar=15365, Vender=9030\n",
      "Ganhos Totais: 36633.75, Perdas Totais: -37142.25\n",
      "Modelo e log do episódio 34 salvos em: 4.7.3\\model_episode_34.pth e 4.7.3\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: -3747.50, Win Rate: 0.53, Wins: 1292, Losses: 1140, Epsilon: 0.3517, Steps: 36754, Time: 110.16s\n",
      "Ações: Manter=12737, Comprar=14053, Vender=9964\n",
      "Ganhos Totais: 34388.50, Perdas Totais: -38136.00\n",
      "Episode 36/100, Total Reward: 45.00, Win Rate: 0.57, Wins: 1371, Losses: 1055, Epsilon: 0.3482, Steps: 36754, Time: 110.41s\n",
      "Ações: Manter=13240, Comprar=11645, Vender=11869\n",
      "Ganhos Totais: 36269.00, Perdas Totais: -36224.00\n",
      "Modelo e log do episódio 36 salvos em: 4.7.3\\model_episode_36.pth e 4.7.3\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: -2651.75, Win Rate: 0.54, Wins: 1271, Losses: 1063, Epsilon: 0.3447, Steps: 36754, Time: 110.06s\n",
      "Ações: Manter=11627, Comprar=14976, Vender=10151\n",
      "Ganhos Totais: 33702.50, Perdas Totais: -36354.25\n",
      "Episode 38/100, Total Reward: 1605.75, Win Rate: 0.56, Wins: 1200, Losses: 958, Epsilon: 0.3413, Steps: 36754, Time: 110.64s\n",
      "Ações: Manter=14304, Comprar=12376, Vender=10074\n",
      "Ganhos Totais: 35821.50, Perdas Totais: -34215.75\n",
      "Modelo e log do episódio 38 salvos em: 4.7.3\\model_episode_38.pth e 4.7.3\\log_episode_38.csv\n",
      "\n",
      "Episode 39/100, Total Reward: -901.25, Win Rate: 0.54, Wins: 1157, Losses: 1000, Epsilon: 0.3379, Steps: 36754, Time: 110.17s\n",
      "Ações: Manter=15018, Comprar=11772, Vender=9964\n",
      "Ganhos Totais: 33967.75, Perdas Totais: -34869.00\n",
      "Episode 40/100, Total Reward: -6208.00, Win Rate: 0.51, Wins: 1072, Losses: 1026, Epsilon: 0.3345, Steps: 36754, Time: 110.10s\n",
      "Ações: Manter=12385, Comprar=11957, Vender=12412\n",
      "Ganhos Totais: 31026.75, Perdas Totais: -37234.75\n",
      "Episode 41/100, Total Reward: -2158.25, Win Rate: 0.53, Wins: 1064, Losses: 940, Epsilon: 0.3311, Steps: 36754, Time: 110.15s\n",
      "Ações: Manter=12014, Comprar=13377, Vender=11363\n",
      "Ganhos Totais: 33023.25, Perdas Totais: -35181.50\n",
      "Episode 42/100, Total Reward: 722.50, Win Rate: 0.56, Wins: 1324, Losses: 1061, Epsilon: 0.3278, Steps: 36754, Time: 110.43s\n",
      "Ações: Manter=11310, Comprar=14128, Vender=11316\n",
      "Ganhos Totais: 35591.25, Perdas Totais: -34868.75\n",
      "Modelo e log do episódio 42 salvos em: 4.7.3\\model_episode_42.pth e 4.7.3\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: 840.50, Win Rate: 0.55, Wins: 1247, Losses: 1000, Epsilon: 0.3246, Steps: 36754, Time: 110.52s\n",
      "Ações: Manter=12215, Comprar=13894, Vender=10645\n",
      "Ganhos Totais: 36383.00, Perdas Totais: -35542.50\n",
      "Modelo e log do episódio 43 salvos em: 4.7.3\\model_episode_43.pth e 4.7.3\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -1427.50, Win Rate: 0.54, Wins: 1361, Losses: 1137, Epsilon: 0.3213, Steps: 36754, Time: 110.34s\n",
      "Ações: Manter=12055, Comprar=14327, Vender=10372\n",
      "Ganhos Totais: 34148.00, Perdas Totais: -35575.50\n",
      "Episode 45/100, Total Reward: 1108.75, Win Rate: 0.56, Wins: 1251, Losses: 993, Epsilon: 0.3181, Steps: 36754, Time: 110.61s\n",
      "Ações: Manter=13452, Comprar=12406, Vender=10896\n",
      "Ganhos Totais: 35334.00, Perdas Totais: -34225.25\n",
      "Modelo e log do episódio 45 salvos em: 4.7.3\\model_episode_45.pth e 4.7.3\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: -484.25, Win Rate: 0.54, Wins: 1321, Losses: 1131, Epsilon: 0.3149, Steps: 36754, Time: 110.45s\n",
      "Ações: Manter=13510, Comprar=11982, Vender=11262\n",
      "Ganhos Totais: 34965.75, Perdas Totais: -35450.00\n",
      "Episode 47/100, Total Reward: -2261.00, Win Rate: 0.54, Wins: 1204, Losses: 1016, Epsilon: 0.3118, Steps: 36754, Time: 110.61s\n",
      "Ações: Manter=14391, Comprar=11785, Vender=10578\n",
      "Ganhos Totais: 33036.50, Perdas Totais: -35297.50\n",
      "Episode 48/100, Total Reward: -2585.50, Win Rate: 0.54, Wins: 1189, Losses: 1021, Epsilon: 0.3086, Steps: 36754, Time: 110.28s\n",
      "Ações: Manter=14654, Comprar=10436, Vender=11664\n",
      "Ganhos Totais: 32797.50, Perdas Totais: -35383.00\n",
      "Episode 49/100, Total Reward: 1105.50, Win Rate: 0.56, Wins: 1218, Losses: 967, Epsilon: 0.3056, Steps: 36754, Time: 110.65s\n",
      "Ações: Manter=15889, Comprar=11156, Vender=9709\n",
      "Ganhos Totais: 33455.50, Perdas Totais: -32350.00\n",
      "Modelo e log do episódio 49 salvos em: 4.7.3\\model_episode_49.pth e 4.7.3\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: -5950.25, Win Rate: 0.53, Wins: 1099, Losses: 960, Epsilon: 0.3025, Steps: 36754, Time: 110.40s\n",
      "Ações: Manter=15072, Comprar=10644, Vender=11038\n",
      "Ganhos Totais: 30227.50, Perdas Totais: -36177.75\n",
      "Episode 51/100, Total Reward: -1896.00, Win Rate: 0.56, Wins: 1308, Losses: 1034, Epsilon: 0.2995, Steps: 36754, Time: 110.72s\n",
      "Ações: Manter=11268, Comprar=12085, Vender=13401\n",
      "Ganhos Totais: 34398.50, Perdas Totais: -36294.50\n",
      "Episode 52/100, Total Reward: 1832.50, Win Rate: 0.56, Wins: 1296, Losses: 1030, Epsilon: 0.2965, Steps: 36754, Time: 110.73s\n",
      "Ações: Manter=13884, Comprar=10696, Vender=12174\n",
      "Ganhos Totais: 35997.50, Perdas Totais: -34165.00\n",
      "Modelo e log do episódio 52 salvos em: 4.7.3\\model_episode_52.pth e 4.7.3\\log_episode_52.csv\n",
      "\n",
      "Episode 53/100, Total Reward: -3875.25, Win Rate: 0.53, Wins: 1115, Losses: 976, Epsilon: 0.2935, Steps: 36754, Time: 110.55s\n",
      "Ações: Manter=15077, Comprar=10631, Vender=11046\n",
      "Ganhos Totais: 31793.00, Perdas Totais: -35668.25\n",
      "Episode 54/100, Total Reward: -635.00, Win Rate: 0.54, Wins: 1299, Losses: 1124, Epsilon: 0.2906, Steps: 36754, Time: 110.81s\n",
      "Ações: Manter=12111, Comprar=12087, Vender=12556\n",
      "Ganhos Totais: 35740.25, Perdas Totais: -36375.25\n",
      "Episode 55/100, Total Reward: -3987.00, Win Rate: 0.55, Wins: 1289, Losses: 1067, Epsilon: 0.2877, Steps: 36754, Time: 110.70s\n",
      "Ações: Manter=12731, Comprar=12737, Vender=11286\n",
      "Ganhos Totais: 32617.50, Perdas Totais: -36604.50\n",
      "Episode 56/100, Total Reward: 344.75, Win Rate: 0.56, Wins: 1351, Losses: 1069, Epsilon: 0.2848, Steps: 36754, Time: 110.91s\n",
      "Ações: Manter=13041, Comprar=13529, Vender=10184\n",
      "Ganhos Totais: 36130.00, Perdas Totais: -35785.25\n",
      "Episode 57/100, Total Reward: -2944.75, Win Rate: 0.55, Wins: 1331, Losses: 1098, Epsilon: 0.2820, Steps: 36754, Time: 110.92s\n",
      "Ações: Manter=13116, Comprar=13548, Vender=10090\n",
      "Ganhos Totais: 32902.00, Perdas Totais: -35846.75\n",
      "Episode 58/100, Total Reward: -1271.00, Win Rate: 0.56, Wins: 1412, Losses: 1091, Epsilon: 0.2791, Steps: 36754, Time: 110.93s\n",
      "Ações: Manter=11705, Comprar=13421, Vender=11628\n",
      "Ganhos Totais: 34726.00, Perdas Totais: -35997.00\n",
      "Episode 59/100, Total Reward: -1214.25, Win Rate: 0.56, Wins: 1351, Losses: 1073, Epsilon: 0.2763, Steps: 36754, Time: 111.23s\n",
      "Ações: Manter=13215, Comprar=14171, Vender=9368\n",
      "Ganhos Totais: 33790.50, Perdas Totais: -35004.75\n",
      "Episode 60/100, Total Reward: 472.25, Win Rate: 0.56, Wins: 1416, Losses: 1095, Epsilon: 0.2736, Steps: 36754, Time: 111.23s\n",
      "Ações: Manter=11709, Comprar=14727, Vender=10318\n",
      "Ganhos Totais: 35110.25, Perdas Totais: -34638.00\n",
      "Episode 61/100, Total Reward: -2143.50, Win Rate: 0.55, Wins: 1338, Losses: 1093, Epsilon: 0.2708, Steps: 36754, Time: 110.98s\n",
      "Ações: Manter=9914, Comprar=15374, Vender=11466\n",
      "Ganhos Totais: 33829.00, Perdas Totais: -35972.50\n",
      "Episode 62/100, Total Reward: 287.50, Win Rate: 0.56, Wins: 1250, Losses: 998, Epsilon: 0.2681, Steps: 36754, Time: 111.12s\n",
      "Ações: Manter=8512, Comprar=15016, Vender=13226\n",
      "Ganhos Totais: 36540.75, Perdas Totais: -36253.25\n",
      "Episode 63/100, Total Reward: 41.75, Win Rate: 0.54, Wins: 1207, Losses: 1021, Epsilon: 0.2655, Steps: 36754, Time: 110.93s\n",
      "Ações: Manter=9897, Comprar=13835, Vender=13022\n",
      "Ganhos Totais: 34627.50, Perdas Totais: -34585.75\n",
      "Episode 64/100, Total Reward: 2017.00, Win Rate: 0.55, Wins: 1408, Losses: 1139, Epsilon: 0.2628, Steps: 36754, Time: 111.00s\n",
      "Ações: Manter=9431, Comprar=16419, Vender=10904\n",
      "Ganhos Totais: 37692.00, Perdas Totais: -35675.00\n",
      "Modelo e log do episódio 64 salvos em: 4.7.3\\model_episode_64.pth e 4.7.3\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: -2797.50, Win Rate: 0.55, Wins: 1289, Losses: 1073, Epsilon: 0.2602, Steps: 36754, Time: 111.20s\n",
      "Ações: Manter=10308, Comprar=14246, Vender=12200\n",
      "Ganhos Totais: 33750.00, Perdas Totais: -36547.50\n",
      "Episode 66/100, Total Reward: 1461.00, Win Rate: 0.53, Wins: 1371, Losses: 1202, Epsilon: 0.2576, Steps: 36754, Time: 111.01s\n",
      "Ações: Manter=7652, Comprar=16855, Vender=12247\n",
      "Ganhos Totais: 38318.75, Perdas Totais: -36857.75\n",
      "Modelo e log do episódio 66 salvos em: 4.7.3\\model_episode_66.pth e 4.7.3\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: -2166.50, Win Rate: 0.53, Wins: 1396, Losses: 1223, Epsilon: 0.2550, Steps: 36754, Time: 111.37s\n",
      "Ações: Manter=8080, Comprar=18152, Vender=10522\n",
      "Ganhos Totais: 36663.00, Perdas Totais: -38829.50\n",
      "Episode 68/100, Total Reward: -1821.50, Win Rate: 0.54, Wins: 1297, Losses: 1096, Epsilon: 0.2524, Steps: 36754, Time: 111.30s\n",
      "Ações: Manter=8157, Comprar=15409, Vender=13188\n",
      "Ganhos Totais: 34175.00, Perdas Totais: -35996.50\n",
      "Episode 69/100, Total Reward: -2470.00, Win Rate: 0.52, Wins: 1431, Losses: 1304, Epsilon: 0.2499, Steps: 36754, Time: 111.21s\n",
      "Ações: Manter=8262, Comprar=16750, Vender=11742\n",
      "Ganhos Totais: 36937.75, Perdas Totais: -39407.75\n",
      "Episode 70/100, Total Reward: -914.50, Win Rate: 0.55, Wins: 1456, Losses: 1179, Epsilon: 0.2474, Steps: 36754, Time: 111.44s\n",
      "Ações: Manter=8078, Comprar=16929, Vender=11747\n",
      "Ganhos Totais: 37419.75, Perdas Totais: -38334.25\n",
      "Episode 71/100, Total Reward: -2130.00, Win Rate: 0.55, Wins: 1303, Losses: 1082, Epsilon: 0.2449, Steps: 36754, Time: 111.66s\n",
      "Ações: Manter=8479, Comprar=15792, Vender=12483\n",
      "Ganhos Totais: 35045.00, Perdas Totais: -37175.00\n",
      "Episode 72/100, Total Reward: -934.25, Win Rate: 0.54, Wins: 1353, Losses: 1151, Epsilon: 0.2425, Steps: 36754, Time: 111.39s\n",
      "Ações: Manter=8060, Comprar=16806, Vender=11888\n",
      "Ganhos Totais: 36977.75, Perdas Totais: -37912.00\n",
      "Episode 73/100, Total Reward: -1791.75, Win Rate: 0.54, Wins: 1473, Losses: 1231, Epsilon: 0.2401, Steps: 36754, Time: 111.65s\n",
      "Ações: Manter=6025, Comprar=19036, Vender=11693\n",
      "Ganhos Totais: 37788.50, Perdas Totais: -39580.25\n",
      "Episode 74/100, Total Reward: -613.50, Win Rate: 0.54, Wins: 1372, Losses: 1166, Epsilon: 0.2377, Steps: 36754, Time: 111.55s\n",
      "Ações: Manter=9218, Comprar=15633, Vender=11903\n",
      "Ganhos Totais: 35037.50, Perdas Totais: -35651.00\n",
      "Episode 75/100, Total Reward: 320.00, Win Rate: 0.55, Wins: 1368, Losses: 1121, Epsilon: 0.2353, Steps: 36754, Time: 111.58s\n",
      "Ações: Manter=7785, Comprar=15907, Vender=13062\n",
      "Ganhos Totais: 37342.50, Perdas Totais: -37022.50\n",
      "Episode 76/100, Total Reward: 992.75, Win Rate: 0.56, Wins: 1324, Losses: 1030, Epsilon: 0.2329, Steps: 36754, Time: 111.69s\n",
      "Ações: Manter=10841, Comprar=14329, Vender=11584\n",
      "Ganhos Totais: 36395.50, Perdas Totais: -35402.75\n",
      "Modelo e log do episódio 76 salvos em: 4.7.3\\model_episode_76.pth e 4.7.3\\log_episode_76.csv\n",
      "\n",
      "Episode 77/100, Total Reward: -1872.00, Win Rate: 0.55, Wins: 1332, Losses: 1077, Epsilon: 0.2306, Steps: 36754, Time: 111.47s\n",
      "Ações: Manter=9336, Comprar=15332, Vender=12086\n",
      "Ganhos Totais: 35136.25, Perdas Totais: -37008.25\n",
      "Episode 78/100, Total Reward: -4208.50, Win Rate: 0.53, Wins: 1029, Losses: 924, Epsilon: 0.2283, Steps: 36754, Time: 111.46s\n",
      "Ações: Manter=14700, Comprar=10753, Vender=11301\n",
      "Ganhos Totais: 31175.00, Perdas Totais: -35383.50\n",
      "Episode 79/100, Total Reward: 2288.50, Win Rate: 0.56, Wins: 1262, Losses: 975, Epsilon: 0.2260, Steps: 36754, Time: 111.41s\n",
      "Ações: Manter=12350, Comprar=11214, Vender=13190\n",
      "Ganhos Totais: 37446.50, Perdas Totais: -35158.00\n",
      "Modelo e log do episódio 79 salvos em: 4.7.3\\model_episode_79.pth e 4.7.3\\log_episode_79.csv\n",
      "\n",
      "Episode 80/100, Total Reward: -2065.25, Win Rate: 0.54, Wins: 1184, Losses: 1018, Epsilon: 0.2238, Steps: 36754, Time: 112.00s\n",
      "Ações: Manter=14259, Comprar=10936, Vender=11559\n",
      "Ganhos Totais: 34234.75, Perdas Totais: -36300.00\n",
      "Episode 81/100, Total Reward: -3268.00, Win Rate: 0.54, Wins: 1055, Losses: 899, Epsilon: 0.2215, Steps: 36754, Time: 111.61s\n",
      "Ações: Manter=16150, Comprar=7984, Vender=12620\n",
      "Ganhos Totais: 31536.50, Perdas Totais: -34804.50\n",
      "Episode 82/100, Total Reward: 2474.50, Win Rate: 0.56, Wins: 1204, Losses: 950, Epsilon: 0.2193, Steps: 36754, Time: 111.47s\n",
      "Ações: Manter=14432, Comprar=10831, Vender=11491\n",
      "Ganhos Totais: 35533.75, Perdas Totais: -33059.25\n",
      "Modelo e log do episódio 82 salvos em: 4.7.3\\model_episode_82.pth e 4.7.3\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: 1188.50, Win Rate: 0.55, Wins: 1152, Losses: 936, Epsilon: 0.2171, Steps: 36754, Time: 111.56s\n",
      "Ações: Manter=12416, Comprar=9195, Vender=15143\n",
      "Ganhos Totais: 36212.00, Perdas Totais: -35023.50\n",
      "Modelo e log do episódio 83 salvos em: 4.7.3\\model_episode_83.pth e 4.7.3\\log_episode_83.csv\n",
      "\n",
      "Episode 84/100, Total Reward: -5668.50, Win Rate: 0.54, Wins: 972, Losses: 843, Epsilon: 0.2149, Steps: 36754, Time: 112.07s\n",
      "Ações: Manter=15685, Comprar=8182, Vender=12887\n",
      "Ganhos Totais: 29463.75, Perdas Totais: -35132.25\n",
      "Episode 85/100, Total Reward: -1768.00, Win Rate: 0.55, Wins: 1052, Losses: 862, Epsilon: 0.2128, Steps: 36754, Time: 111.68s\n",
      "Ações: Manter=17032, Comprar=6576, Vender=13146\n",
      "Ganhos Totais: 31959.25, Perdas Totais: -33727.25\n",
      "Episode 86/100, Total Reward: -918.75, Win Rate: 0.54, Wins: 925, Losses: 794, Epsilon: 0.2107, Steps: 36754, Time: 111.77s\n",
      "Ações: Manter=19288, Comprar=5012, Vender=12454\n",
      "Ganhos Totais: 30524.00, Perdas Totais: -31442.75\n",
      "Episode 87/100, Total Reward: -711.25, Win Rate: 0.53, Wins: 1005, Losses: 898, Epsilon: 0.2086, Steps: 36754, Time: 111.84s\n",
      "Ações: Manter=18071, Comprar=6584, Vender=12099\n",
      "Ganhos Totais: 31966.75, Perdas Totais: -32678.00\n",
      "Episode 88/100, Total Reward: -4047.25, Win Rate: 0.53, Wins: 971, Losses: 873, Epsilon: 0.2065, Steps: 36754, Time: 111.85s\n",
      "Ações: Manter=16434, Comprar=7232, Vender=13088\n",
      "Ganhos Totais: 30555.25, Perdas Totais: -34602.50\n",
      "Episode 89/100, Total Reward: 2643.25, Win Rate: 0.56, Wins: 956, Losses: 744, Epsilon: 0.2044, Steps: 36754, Time: 111.71s\n",
      "Ações: Manter=18094, Comprar=5551, Vender=13109\n",
      "Ganhos Totais: 33449.00, Perdas Totais: -30805.75\n",
      "Modelo e log do episódio 89 salvos em: 4.7.3\\model_episode_89.pth e 4.7.3\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: -2126.75, Win Rate: 0.54, Wins: 959, Losses: 803, Epsilon: 0.2024, Steps: 36754, Time: 111.87s\n",
      "Ações: Manter=16539, Comprar=7644, Vender=12571\n",
      "Ganhos Totais: 31225.75, Perdas Totais: -33352.50\n",
      "Episode 91/100, Total Reward: -1349.75, Win Rate: 0.52, Wins: 886, Losses: 812, Epsilon: 0.2003, Steps: 36754, Time: 112.63s\n",
      "Ações: Manter=17482, Comprar=7559, Vender=11713\n",
      "Ganhos Totais: 31313.50, Perdas Totais: -32663.25\n",
      "Episode 92/100, Total Reward: -1808.75, Win Rate: 0.53, Wins: 924, Losses: 804, Epsilon: 0.1983, Steps: 36754, Time: 112.37s\n",
      "Ações: Manter=16869, Comprar=7558, Vender=12327\n",
      "Ganhos Totais: 31962.25, Perdas Totais: -33771.00\n",
      "Episode 93/100, Total Reward: 1598.25, Win Rate: 0.54, Wins: 899, Losses: 759, Epsilon: 0.1964, Steps: 36754, Time: 112.64s\n",
      "Ações: Manter=17892, Comprar=8658, Vender=10204\n",
      "Ganhos Totais: 33443.25, Perdas Totais: -31845.00\n",
      "Modelo e log do episódio 93 salvos em: 4.7.3\\model_episode_93.pth e 4.7.3\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: -806.50, Win Rate: 0.53, Wins: 904, Losses: 790, Epsilon: 0.1944, Steps: 36754, Time: 112.45s\n",
      "Ações: Manter=18060, Comprar=6490, Vender=12204\n",
      "Ganhos Totais: 32323.75, Perdas Totais: -33130.25\n",
      "Episode 95/100, Total Reward: -2124.25, Win Rate: 0.53, Wins: 882, Losses: 787, Epsilon: 0.1924, Steps: 36754, Time: 112.22s\n",
      "Ações: Manter=18055, Comprar=5721, Vender=12978\n",
      "Ganhos Totais: 30933.50, Perdas Totais: -33057.75\n",
      "Episode 96/100, Total Reward: 1259.25, Win Rate: 0.54, Wins: 859, Losses: 740, Epsilon: 0.1905, Steps: 36754, Time: 112.23s\n",
      "Ações: Manter=19201, Comprar=5485, Vender=12068\n",
      "Ganhos Totais: 32244.75, Perdas Totais: -30985.50\n",
      "Episode 97/100, Total Reward: 1813.00, Win Rate: 0.54, Wins: 936, Losses: 801, Epsilon: 0.1886, Steps: 36754, Time: 112.30s\n",
      "Ações: Manter=18085, Comprar=6071, Vender=12598\n",
      "Ganhos Totais: 33187.50, Perdas Totais: -31374.50\n",
      "Modelo e log do episódio 97 salvos em: 4.7.3\\model_episode_97.pth e 4.7.3\\log_episode_97.csv\n",
      "\n",
      "Episode 98/100, Total Reward: -2660.25, Win Rate: 0.53, Wins: 873, Losses: 762, Epsilon: 0.1867, Steps: 36754, Time: 111.80s\n",
      "Ações: Manter=18136, Comprar=6347, Vender=12271\n",
      "Ganhos Totais: 30121.00, Perdas Totais: -32781.25\n",
      "Episode 99/100, Total Reward: -2831.00, Win Rate: 0.52, Wins: 821, Losses: 748, Epsilon: 0.1849, Steps: 36754, Time: 112.04s\n",
      "Ações: Manter=18765, Comprar=5078, Vender=12911\n",
      "Ganhos Totais: 29812.00, Perdas Totais: -32643.00\n",
      "Episode 100/100, Total Reward: 4714.50, Win Rate: 0.56, Wins: 942, Losses: 732, Epsilon: 0.1830, Steps: 36754, Time: 111.90s\n",
      "Ações: Manter=18919, Comprar=7712, Vender=10123\n",
      "Ganhos Totais: 34706.25, Perdas Totais: -29991.75\n",
      "Modelo e log do episódio 100 salvos em: 4.7.3\\model_episode_100.pth e 4.7.3\\log_episode_100.csv\n",
      "\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 100, Total Reward: 4714.50, Win Rate: 0.56, Wins: 942, Losses: 732, Ações: {0: 18919, 1: 7712, 2: 10123}, Steps: 36754, Time: 111.90s\n",
      "Rank 2: Episode 89, Total Reward: 2643.25, Win Rate: 0.56, Wins: 956, Losses: 744, Ações: {0: 18094, 1: 5551, 2: 13109}, Steps: 36754, Time: 111.71s\n",
      "Rank 3: Episode 82, Total Reward: 2474.50, Win Rate: 0.56, Wins: 1204, Losses: 950, Ações: {0: 14432, 1: 10831, 2: 11491}, Steps: 36754, Time: 111.47s\n",
      "Rank 4: Episode 14, Total Reward: 2302.50, Win Rate: 0.53, Wins: 1336, Losses: 1195, Ações: {0: 13018, 1: 12086, 2: 11650}, Steps: 36754, Time: 124.46s\n",
      "Rank 5: Episode 79, Total Reward: 2288.50, Win Rate: 0.56, Wins: 1262, Losses: 975, Ações: {0: 12350, 1: 11214, 2: 13190}, Steps: 36754, Time: 111.41s\n",
      "Rank 6: Episode 64, Total Reward: 2017.00, Win Rate: 0.55, Wins: 1408, Losses: 1139, Ações: {0: 9431, 1: 16419, 2: 10904}, Steps: 36754, Time: 111.00s\n",
      "Rank 7: Episode 5, Total Reward: 1906.00, Win Rate: 0.54, Wins: 1441, Losses: 1245, Ações: {0: 10708, 1: 13206, 2: 12840}, Steps: 36754, Time: 123.89s\n",
      "Rank 8: Episode 23, Total Reward: 1855.25, Win Rate: 0.55, Wins: 1376, Losses: 1133, Ações: {0: 11843, 1: 13210, 2: 11701}, Steps: 36754, Time: 120.97s\n",
      "Rank 9: Episode 52, Total Reward: 1832.50, Win Rate: 0.56, Wins: 1296, Losses: 1030, Ações: {0: 13884, 1: 10696, 2: 12174}, Steps: 36754, Time: 110.73s\n",
      "Rank 10: Episode 97, Total Reward: 1813.00, Win Rate: 0.54, Wins: 936, Losses: 801, Ações: {0: 18085, 1: 6071, 2: 12598}, Steps: 36754, Time: 112.30s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 30  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.3\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
