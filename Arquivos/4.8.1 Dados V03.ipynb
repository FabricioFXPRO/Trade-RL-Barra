{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 620.75, Win Rate: 0.49, Wins: 1398, Losses: 1449, Epsilon: 0.4950, Steps: 36754, Time: 121.53s\n",
      "Ações: Manter=11289, Comprar=12418, Vender=13047\n",
      "Ganhos Totais: 39068.00, Perdas Totais: -38447.25\n",
      "Modelo e log do episódio 1 salvos em: 4.8.1\\model_episode_1.pth e 4.8.1\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -1577.75, Win Rate: 0.51, Wins: 1394, Losses: 1354, Epsilon: 0.4900, Steps: 36754, Time: 125.85s\n",
      "Ações: Manter=11637, Comprar=11376, Vender=13741\n",
      "Ganhos Totais: 37837.00, Perdas Totais: -39414.75\n",
      "Modelo e log do episódio 2 salvos em: 4.8.1\\model_episode_2.pth e 4.8.1\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: 862.75, Win Rate: 0.53, Wins: 1448, Losses: 1307, Epsilon: 0.4851, Steps: 36754, Time: 131.04s\n",
      "Ações: Manter=11639, Comprar=12038, Vender=13077\n",
      "Ganhos Totais: 38505.75, Perdas Totais: -37643.00\n",
      "Modelo e log do episódio 3 salvos em: 4.8.1\\model_episode_3.pth e 4.8.1\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 3732.75, Win Rate: 0.53, Wins: 1545, Losses: 1377, Epsilon: 0.4803, Steps: 36754, Time: 126.29s\n",
      "Ações: Manter=11905, Comprar=11701, Vender=13148\n",
      "Ganhos Totais: 39815.00, Perdas Totais: -36082.25\n",
      "Modelo e log do episódio 4 salvos em: 4.8.1\\model_episode_4.pth e 4.8.1\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -1078.50, Win Rate: 0.50, Wins: 1402, Losses: 1378, Epsilon: 0.4755, Steps: 36754, Time: 108.69s\n",
      "Ações: Manter=12258, Comprar=12148, Vender=12348\n",
      "Ganhos Totais: 36368.25, Perdas Totais: -37446.75\n",
      "Modelo e log do episódio 5 salvos em: 4.8.1\\model_episode_5.pth e 4.8.1\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -1389.00, Win Rate: 0.51, Wins: 1454, Losses: 1387, Epsilon: 0.4707, Steps: 36754, Time: 108.04s\n",
      "Ações: Manter=12683, Comprar=12110, Vender=11961\n",
      "Ganhos Totais: 36893.25, Perdas Totais: -38282.25\n",
      "Modelo e log do episódio 6 salvos em: 4.8.1\\model_episode_6.pth e 4.8.1\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -5098.00, Win Rate: 0.52, Wins: 1387, Losses: 1304, Epsilon: 0.4660, Steps: 36754, Time: 107.96s\n",
      "Ações: Manter=12609, Comprar=12732, Vender=11413\n",
      "Ganhos Totais: 35359.25, Perdas Totais: -40457.25\n",
      "Modelo e log do episódio 7 salvos em: 4.8.1\\model_episode_7.pth e 4.8.1\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 3516.75, Win Rate: 0.54, Wins: 1551, Losses: 1304, Epsilon: 0.4614, Steps: 36754, Time: 106.21s\n",
      "Ações: Manter=11899, Comprar=13451, Vender=11404\n",
      "Ganhos Totais: 39396.00, Perdas Totais: -35879.25\n",
      "Modelo e log do episódio 8 salvos em: 4.8.1\\model_episode_8.pth e 4.8.1\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -1618.50, Win Rate: 0.53, Wins: 1491, Losses: 1316, Epsilon: 0.4568, Steps: 36754, Time: 107.38s\n",
      "Ações: Manter=12491, Comprar=12645, Vender=11618\n",
      "Ganhos Totais: 36447.50, Perdas Totais: -38066.00\n",
      "Modelo e log do episódio 9 salvos em: 4.8.1\\model_episode_9.pth e 4.8.1\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -4565.25, Win Rate: 0.52, Wins: 1378, Losses: 1268, Epsilon: 0.4522, Steps: 36754, Time: 107.43s\n",
      "Ações: Manter=13955, Comprar=11525, Vender=11274\n",
      "Ganhos Totais: 35176.75, Perdas Totais: -39742.00\n",
      "Modelo e log do episódio 10 salvos em: 4.8.1\\model_episode_10.pth e 4.8.1\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 1990.00, Win Rate: 0.54, Wins: 1421, Losses: 1224, Epsilon: 0.4477, Steps: 36754, Time: 106.63s\n",
      "Ações: Manter=13154, Comprar=11588, Vender=12012\n",
      "Ganhos Totais: 37565.75, Perdas Totais: -35575.75\n",
      "Modelo e log do episódio 11 salvos em: 4.8.1\\model_episode_11.pth e 4.8.1\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -3436.00, Win Rate: 0.53, Wins: 1447, Losses: 1268, Epsilon: 0.4432, Steps: 36754, Time: 107.08s\n",
      "Ações: Manter=12373, Comprar=12330, Vender=12051\n",
      "Ganhos Totais: 36261.50, Perdas Totais: -39697.50\n",
      "Modelo e log do episódio 12 salvos em: 4.8.1\\model_episode_12.pth e 4.8.1\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 2400.25, Win Rate: 0.55, Wins: 1417, Losses: 1182, Epsilon: 0.4388, Steps: 36754, Time: 107.57s\n",
      "Ações: Manter=12370, Comprar=12348, Vender=12036\n",
      "Ganhos Totais: 37731.00, Perdas Totais: -35330.75\n",
      "Modelo e log do episódio 13 salvos em: 4.8.1\\model_episode_13.pth e 4.8.1\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 360.75, Win Rate: 0.54, Wins: 1498, Losses: 1259, Epsilon: 0.4344, Steps: 36754, Time: 107.80s\n",
      "Ações: Manter=12230, Comprar=12913, Vender=11611\n",
      "Ganhos Totais: 38774.25, Perdas Totais: -38413.50\n",
      "Modelo e log do episódio 14 salvos em: 4.8.1\\model_episode_14.pth e 4.8.1\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -2168.00, Win Rate: 0.52, Wins: 1261, Losses: 1156, Epsilon: 0.4300, Steps: 36754, Time: 106.75s\n",
      "Ações: Manter=15097, Comprar=11228, Vender=10429\n",
      "Ganhos Totais: 35240.75, Perdas Totais: -37408.75\n",
      "Episode 16/100, Total Reward: -3760.75, Win Rate: 0.53, Wins: 1301, Losses: 1150, Epsilon: 0.4257, Steps: 36754, Time: 110.77s\n",
      "Ações: Manter=13886, Comprar=11592, Vender=11276\n",
      "Ganhos Totais: 34270.75, Perdas Totais: -38031.50\n",
      "Episode 17/100, Total Reward: -3936.00, Win Rate: 0.52, Wins: 1339, Losses: 1223, Epsilon: 0.4215, Steps: 36754, Time: 107.47s\n",
      "Ações: Manter=13467, Comprar=11822, Vender=11465\n",
      "Ganhos Totais: 34871.75, Perdas Totais: -38807.75\n",
      "Episode 18/100, Total Reward: 2079.00, Win Rate: 0.54, Wins: 1472, Losses: 1264, Epsilon: 0.4173, Steps: 36754, Time: 107.75s\n",
      "Ações: Manter=12604, Comprar=12788, Vender=11362\n",
      "Ganhos Totais: 38581.25, Perdas Totais: -36502.25\n",
      "Modelo e log do episódio 18 salvos em: 4.8.1\\model_episode_18.pth e 4.8.1\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -244.50, Win Rate: 0.54, Wins: 1382, Losses: 1170, Epsilon: 0.4131, Steps: 36754, Time: 108.30s\n",
      "Ações: Manter=13225, Comprar=13197, Vender=10332\n",
      "Ganhos Totais: 37316.75, Perdas Totais: -37561.25\n",
      "Modelo e log do episódio 19 salvos em: 4.8.1\\model_episode_19.pth e 4.8.1\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 275.00, Win Rate: 0.54, Wins: 1413, Losses: 1199, Epsilon: 0.4090, Steps: 36754, Time: 109.01s\n",
      "Ações: Manter=12595, Comprar=13132, Vender=11027\n",
      "Ganhos Totais: 36743.50, Perdas Totais: -36468.50\n",
      "Modelo e log do episódio 20 salvos em: 4.8.1\\model_episode_20.pth e 4.8.1\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -3150.75, Win Rate: 0.53, Wins: 1451, Losses: 1288, Epsilon: 0.4049, Steps: 36754, Time: 109.21s\n",
      "Ações: Manter=12093, Comprar=13983, Vender=10678\n",
      "Ganhos Totais: 35555.00, Perdas Totais: -38705.75\n",
      "Episode 22/100, Total Reward: -817.00, Win Rate: 0.54, Wins: 1476, Losses: 1282, Epsilon: 0.4008, Steps: 36754, Time: 108.45s\n",
      "Ações: Manter=12300, Comprar=13022, Vender=11432\n",
      "Ganhos Totais: 36638.00, Perdas Totais: -37455.00\n",
      "Episode 23/100, Total Reward: -1366.50, Win Rate: 0.53, Wins: 1408, Losses: 1248, Epsilon: 0.3968, Steps: 36754, Time: 108.97s\n",
      "Ações: Manter=12803, Comprar=12122, Vender=11829\n",
      "Ganhos Totais: 36665.75, Perdas Totais: -38032.25\n",
      "Episode 24/100, Total Reward: -768.00, Win Rate: 0.53, Wins: 1388, Losses: 1216, Epsilon: 0.3928, Steps: 36754, Time: 108.97s\n",
      "Ações: Manter=14588, Comprar=10182, Vender=11984\n",
      "Ganhos Totais: 36639.25, Perdas Totais: -37407.25\n",
      "Episode 25/100, Total Reward: -2534.50, Win Rate: 0.54, Wins: 1500, Losses: 1290, Epsilon: 0.3889, Steps: 36754, Time: 108.80s\n",
      "Ações: Manter=12684, Comprar=13007, Vender=11063\n",
      "Ganhos Totais: 36483.75, Perdas Totais: -39018.25\n",
      "Episode 26/100, Total Reward: 2598.75, Win Rate: 0.54, Wins: 1501, Losses: 1269, Epsilon: 0.3850, Steps: 36754, Time: 109.05s\n",
      "Ações: Manter=12444, Comprar=12862, Vender=11448\n",
      "Ganhos Totais: 37963.75, Perdas Totais: -35365.00\n",
      "Modelo e log do episódio 26 salvos em: 4.8.1\\model_episode_26.pth e 4.8.1\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -2349.75, Win Rate: 0.54, Wins: 1362, Losses: 1182, Epsilon: 0.3812, Steps: 36754, Time: 108.72s\n",
      "Ações: Manter=14602, Comprar=10530, Vender=11622\n",
      "Ganhos Totais: 34817.25, Perdas Totais: -37167.00\n",
      "Episode 28/100, Total Reward: -122.50, Win Rate: 0.55, Wins: 1478, Losses: 1212, Epsilon: 0.3774, Steps: 36754, Time: 109.05s\n",
      "Ações: Manter=13974, Comprar=11197, Vender=11583\n",
      "Ganhos Totais: 36330.50, Perdas Totais: -36453.00\n",
      "Episode 29/100, Total Reward: -483.00, Win Rate: 0.54, Wins: 1409, Losses: 1194, Epsilon: 0.3736, Steps: 36754, Time: 109.01s\n",
      "Ações: Manter=12941, Comprar=13597, Vender=10216\n",
      "Ganhos Totais: 35578.75, Perdas Totais: -36061.75\n",
      "Episode 30/100, Total Reward: -195.25, Win Rate: 0.53, Wins: 1283, Losses: 1155, Epsilon: 0.3699, Steps: 36754, Time: 109.46s\n",
      "Ações: Manter=13263, Comprar=10896, Vender=12595\n",
      "Ganhos Totais: 36061.00, Perdas Totais: -36256.25\n",
      "Episode 31/100, Total Reward: 368.75, Win Rate: 0.55, Wins: 1311, Losses: 1079, Epsilon: 0.3662, Steps: 36754, Time: 109.50s\n",
      "Ações: Manter=14143, Comprar=9956, Vender=12655\n",
      "Ganhos Totais: 35442.25, Perdas Totais: -35073.50\n",
      "Modelo e log do episódio 31 salvos em: 4.8.1\\model_episode_31.pth e 4.8.1\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 269.00, Win Rate: 0.53, Wins: 1250, Losses: 1106, Epsilon: 0.3625, Steps: 36754, Time: 108.49s\n",
      "Ações: Manter=10239, Comprar=14955, Vender=11560\n",
      "Ganhos Totais: 36819.00, Perdas Totais: -36550.00\n",
      "Episode 33/100, Total Reward: -2459.50, Win Rate: 0.53, Wins: 1229, Losses: 1081, Epsilon: 0.3589, Steps: 36754, Time: 109.82s\n",
      "Ações: Manter=12982, Comprar=13180, Vender=10592\n",
      "Ganhos Totais: 34232.25, Perdas Totais: -36691.75\n",
      "Episode 34/100, Total Reward: -1505.75, Win Rate: 0.53, Wins: 1259, Losses: 1104, Epsilon: 0.3553, Steps: 36754, Time: 112.37s\n",
      "Ações: Manter=9947, Comprar=14706, Vender=12101\n",
      "Ganhos Totais: 37129.00, Perdas Totais: -38634.75\n",
      "Episode 35/100, Total Reward: -1998.50, Win Rate: 0.54, Wins: 1153, Losses: 980, Epsilon: 0.3517, Steps: 36754, Time: 112.24s\n",
      "Ações: Manter=14922, Comprar=10972, Vender=10860\n",
      "Ganhos Totais: 33105.25, Perdas Totais: -35103.75\n",
      "Episode 36/100, Total Reward: -1177.50, Win Rate: 0.54, Wins: 1218, Losses: 1051, Epsilon: 0.3482, Steps: 36754, Time: 109.66s\n",
      "Ações: Manter=10433, Comprar=14965, Vender=11356\n",
      "Ganhos Totais: 35334.25, Perdas Totais: -36511.75\n",
      "Episode 37/100, Total Reward: -3332.25, Win Rate: 0.53, Wins: 1239, Losses: 1081, Epsilon: 0.3447, Steps: 36754, Time: 110.29s\n",
      "Ações: Manter=11660, Comprar=13194, Vender=11900\n",
      "Ganhos Totais: 34480.75, Perdas Totais: -37813.00\n",
      "Episode 38/100, Total Reward: -158.25, Win Rate: 0.53, Wins: 1165, Losses: 1021, Epsilon: 0.3413, Steps: 36754, Time: 110.12s\n",
      "Ações: Manter=14702, Comprar=10266, Vender=11786\n",
      "Ganhos Totais: 34946.25, Perdas Totais: -35104.50\n",
      "Episode 39/100, Total Reward: 237.75, Win Rate: 0.53, Wins: 1162, Losses: 1020, Epsilon: 0.3379, Steps: 36754, Time: 109.73s\n",
      "Ações: Manter=11832, Comprar=13864, Vender=11058\n",
      "Ganhos Totais: 36020.25, Perdas Totais: -35782.50\n",
      "Episode 40/100, Total Reward: -161.00, Win Rate: 0.54, Wins: 1236, Losses: 1055, Epsilon: 0.3345, Steps: 36754, Time: 111.17s\n",
      "Ações: Manter=9967, Comprar=15287, Vender=11500\n",
      "Ganhos Totais: 36475.50, Perdas Totais: -36636.50\n",
      "Episode 41/100, Total Reward: 1377.50, Win Rate: 0.55, Wins: 1192, Losses: 995, Epsilon: 0.3311, Steps: 36754, Time: 111.19s\n",
      "Ações: Manter=13167, Comprar=13206, Vender=10381\n",
      "Ganhos Totais: 34778.00, Perdas Totais: -33400.50\n",
      "Modelo e log do episódio 41 salvos em: 4.8.1\\model_episode_41.pth e 4.8.1\\log_episode_41.csv\n",
      "\n",
      "Episode 42/100, Total Reward: 497.75, Win Rate: 0.55, Wins: 1367, Losses: 1098, Epsilon: 0.3278, Steps: 36754, Time: 110.31s\n",
      "Ações: Manter=10394, Comprar=13256, Vender=13104\n",
      "Ganhos Totais: 36995.00, Perdas Totais: -36497.25\n",
      "Modelo e log do episódio 42 salvos em: 4.8.1\\model_episode_42.pth e 4.8.1\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: -1907.00, Win Rate: 0.55, Wins: 1210, Losses: 998, Epsilon: 0.3246, Steps: 36754, Time: 111.73s\n",
      "Ações: Manter=12071, Comprar=12461, Vender=12222\n",
      "Ganhos Totais: 34938.00, Perdas Totais: -36845.00\n",
      "Episode 44/100, Total Reward: -1246.25, Win Rate: 0.55, Wins: 1270, Losses: 1049, Epsilon: 0.3213, Steps: 36754, Time: 110.72s\n",
      "Ações: Manter=10665, Comprar=13348, Vender=12741\n",
      "Ganhos Totais: 35947.50, Perdas Totais: -37193.75\n",
      "Episode 45/100, Total Reward: -521.00, Win Rate: 0.54, Wins: 1196, Losses: 1007, Epsilon: 0.3181, Steps: 36754, Time: 110.18s\n",
      "Ações: Manter=11246, Comprar=13277, Vender=12231\n",
      "Ganhos Totais: 36147.25, Perdas Totais: -36668.25\n",
      "Episode 46/100, Total Reward: 569.00, Win Rate: 0.56, Wins: 1311, Losses: 1046, Epsilon: 0.3149, Steps: 36754, Time: 110.32s\n",
      "Ações: Manter=10138, Comprar=13422, Vender=13194\n",
      "Ganhos Totais: 36476.50, Perdas Totais: -35907.50\n",
      "Modelo e log do episódio 46 salvos em: 4.8.1\\model_episode_46.pth e 4.8.1\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: -143.50, Win Rate: 0.56, Wins: 1320, Losses: 1048, Epsilon: 0.3118, Steps: 36754, Time: 110.79s\n",
      "Ações: Manter=10044, Comprar=14025, Vender=12685\n",
      "Ganhos Totais: 36800.25, Perdas Totais: -36943.75\n",
      "Episode 48/100, Total Reward: -3471.75, Win Rate: 0.55, Wins: 1240, Losses: 998, Epsilon: 0.3086, Steps: 36754, Time: 111.08s\n",
      "Ações: Manter=11307, Comprar=12951, Vender=12496\n",
      "Ganhos Totais: 34105.25, Perdas Totais: -37577.00\n",
      "Episode 49/100, Total Reward: 2706.50, Win Rate: 0.56, Wins: 1381, Losses: 1082, Epsilon: 0.3056, Steps: 36754, Time: 165.98s\n",
      "Ações: Manter=8725, Comprar=15905, Vender=12124\n",
      "Ganhos Totais: 37164.75, Perdas Totais: -34458.25\n",
      "Modelo e log do episódio 49 salvos em: 4.8.1\\model_episode_49.pth e 4.8.1\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: 1247.00, Win Rate: 0.55, Wins: 1315, Losses: 1079, Epsilon: 0.3025, Steps: 36754, Time: 135.36s\n",
      "Ações: Manter=7375, Comprar=17337, Vender=12042\n",
      "Ganhos Totais: 37589.50, Perdas Totais: -36342.50\n",
      "Modelo e log do episódio 50 salvos em: 4.8.1\\model_episode_50.pth e 4.8.1\\log_episode_50.csv\n",
      "\n",
      "Episode 51/100, Total Reward: -2745.50, Win Rate: 0.55, Wins: 1220, Losses: 1018, Epsilon: 0.2995, Steps: 36754, Time: 152.44s\n",
      "Ações: Manter=13501, Comprar=11145, Vender=12108\n",
      "Ganhos Totais: 33997.50, Perdas Totais: -36743.00\n",
      "Episode 52/100, Total Reward: -3326.75, Win Rate: 0.52, Wins: 1097, Losses: 1027, Epsilon: 0.2965, Steps: 36754, Time: 182.41s\n",
      "Ações: Manter=9723, Comprar=13898, Vender=13133\n",
      "Ganhos Totais: 33129.25, Perdas Totais: -36456.00\n",
      "Episode 53/100, Total Reward: 896.75, Win Rate: 0.55, Wins: 1193, Losses: 976, Epsilon: 0.2935, Steps: 36754, Time: 155.45s\n",
      "Ações: Manter=10206, Comprar=13093, Vender=13455\n",
      "Ganhos Totais: 35712.50, Perdas Totais: -34815.75\n",
      "Modelo e log do episódio 53 salvos em: 4.8.1\\model_episode_53.pth e 4.8.1\\log_episode_53.csv\n",
      "\n",
      "Episode 54/100, Total Reward: -2492.50, Win Rate: 0.53, Wins: 1130, Losses: 1017, Epsilon: 0.2906, Steps: 36754, Time: 165.24s\n",
      "Ações: Manter=9264, Comprar=14827, Vender=12663\n",
      "Ganhos Totais: 34062.25, Perdas Totais: -36554.75\n",
      "Episode 55/100, Total Reward: -1338.50, Win Rate: 0.54, Wins: 1141, Losses: 958, Epsilon: 0.2877, Steps: 36754, Time: 157.77s\n",
      "Ações: Manter=9904, Comprar=14493, Vender=12357\n",
      "Ganhos Totais: 33998.75, Perdas Totais: -35337.25\n",
      "Episode 56/100, Total Reward: -4122.25, Win Rate: 0.54, Wins: 1092, Losses: 947, Epsilon: 0.2848, Steps: 36754, Time: 164.29s\n",
      "Ações: Manter=11137, Comprar=12411, Vender=13206\n",
      "Ganhos Totais: 31722.50, Perdas Totais: -35844.75\n",
      "Episode 57/100, Total Reward: -1224.50, Win Rate: 0.55, Wins: 1144, Losses: 955, Epsilon: 0.2820, Steps: 36754, Time: 151.64s\n",
      "Ações: Manter=11504, Comprar=13332, Vender=11918\n",
      "Ganhos Totais: 34567.75, Perdas Totais: -35792.25\n",
      "Episode 58/100, Total Reward: 1845.75, Win Rate: 0.56, Wins: 1140, Losses: 913, Epsilon: 0.2791, Steps: 36754, Time: 156.77s\n",
      "Ações: Manter=11602, Comprar=14286, Vender=10866\n",
      "Ganhos Totais: 36048.25, Perdas Totais: -34202.50\n",
      "Modelo e log do episódio 58 salvos em: 4.8.1\\model_episode_58.pth e 4.8.1\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: -2309.25, Win Rate: 0.55, Wins: 1187, Losses: 988, Epsilon: 0.2763, Steps: 36754, Time: 173.64s\n",
      "Ações: Manter=11317, Comprar=13922, Vender=11515\n",
      "Ganhos Totais: 34485.75, Perdas Totais: -36795.00\n",
      "Episode 60/100, Total Reward: -2412.00, Win Rate: 0.56, Wins: 1219, Losses: 953, Epsilon: 0.2736, Steps: 36754, Time: 153.13s\n",
      "Ações: Manter=9772, Comprar=14802, Vender=12180\n",
      "Ganhos Totais: 33843.50, Perdas Totais: -36255.50\n",
      "Episode 61/100, Total Reward: -353.25, Win Rate: 0.55, Wins: 1096, Losses: 901, Epsilon: 0.2708, Steps: 36754, Time: 152.96s\n",
      "Ações: Manter=12464, Comprar=11610, Vender=12680\n",
      "Ganhos Totais: 34082.50, Perdas Totais: -34435.75\n",
      "Episode 62/100, Total Reward: -2327.00, Win Rate: 0.55, Wins: 1212, Losses: 980, Epsilon: 0.2681, Steps: 36754, Time: 154.86s\n",
      "Ações: Manter=11256, Comprar=12602, Vender=12896\n",
      "Ganhos Totais: 34073.50, Perdas Totais: -36400.50\n",
      "Episode 63/100, Total Reward: -2743.50, Win Rate: 0.53, Wins: 1113, Losses: 1004, Epsilon: 0.2655, Steps: 36754, Time: 152.76s\n",
      "Ações: Manter=11269, Comprar=13412, Vender=12073\n",
      "Ganhos Totais: 33300.00, Perdas Totais: -36043.50\n",
      "Episode 64/100, Total Reward: 2954.50, Win Rate: 0.56, Wins: 1241, Losses: 990, Epsilon: 0.2628, Steps: 36754, Time: 152.42s\n",
      "Ações: Manter=10800, Comprar=12706, Vender=13248\n",
      "Ganhos Totais: 37432.75, Perdas Totais: -34478.25\n",
      "Modelo e log do episódio 64 salvos em: 4.8.1\\model_episode_64.pth e 4.8.1\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: -486.00, Win Rate: 0.54, Wins: 1163, Losses: 999, Epsilon: 0.2602, Steps: 36754, Time: 151.01s\n",
      "Ações: Manter=12482, Comprar=12104, Vender=12168\n",
      "Ganhos Totais: 34145.00, Perdas Totais: -34631.00\n",
      "Episode 66/100, Total Reward: -1523.50, Win Rate: 0.56, Wins: 1238, Losses: 981, Epsilon: 0.2576, Steps: 36754, Time: 152.15s\n",
      "Ações: Manter=8759, Comprar=16030, Vender=11965\n",
      "Ganhos Totais: 35491.00, Perdas Totais: -37014.50\n",
      "Episode 67/100, Total Reward: 911.25, Win Rate: 0.57, Wins: 1266, Losses: 958, Epsilon: 0.2550, Steps: 36754, Time: 152.20s\n",
      "Ações: Manter=7630, Comprar=16571, Vender=12553\n",
      "Ganhos Totais: 37067.50, Perdas Totais: -36156.25\n",
      "Episode 68/100, Total Reward: 949.25, Win Rate: 0.56, Wins: 1188, Losses: 946, Epsilon: 0.2524, Steps: 36754, Time: 154.65s\n",
      "Ações: Manter=9120, Comprar=15978, Vender=11656\n",
      "Ganhos Totais: 35538.50, Perdas Totais: -34589.25\n",
      "Episode 69/100, Total Reward: -3592.25, Win Rate: 0.54, Wins: 1168, Losses: 994, Epsilon: 0.2499, Steps: 36754, Time: 158.91s\n",
      "Ações: Manter=10589, Comprar=14295, Vender=11870\n",
      "Ganhos Totais: 33380.00, Perdas Totais: -36972.25\n",
      "Episode 70/100, Total Reward: 2663.50, Win Rate: 0.57, Wins: 1182, Losses: 899, Epsilon: 0.2474, Steps: 36754, Time: 149.49s\n",
      "Ações: Manter=14214, Comprar=9985, Vender=12555\n",
      "Ganhos Totais: 35369.25, Perdas Totais: -32705.75\n",
      "Modelo e log do episódio 70 salvos em: 4.8.1\\model_episode_70.pth e 4.8.1\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: 1016.50, Win Rate: 0.55, Wins: 1248, Losses: 1029, Epsilon: 0.2449, Steps: 36754, Time: 136.36s\n",
      "Ações: Manter=10076, Comprar=14726, Vender=11952\n",
      "Ganhos Totais: 35557.25, Perdas Totais: -34540.75\n",
      "Episode 72/100, Total Reward: 634.50, Win Rate: 0.56, Wins: 1220, Losses: 964, Epsilon: 0.2425, Steps: 36754, Time: 139.68s\n",
      "Ações: Manter=8862, Comprar=17926, Vender=9966\n",
      "Ganhos Totais: 36274.25, Perdas Totais: -35639.75\n",
      "Episode 73/100, Total Reward: -782.75, Win Rate: 0.55, Wins: 1110, Losses: 898, Epsilon: 0.2401, Steps: 36754, Time: 141.05s\n",
      "Ações: Manter=7518, Comprar=18776, Vender=10460\n",
      "Ganhos Totais: 34356.00, Perdas Totais: -35138.75\n",
      "Episode 74/100, Total Reward: 6502.50, Win Rate: 0.59, Wins: 1233, Losses: 853, Epsilon: 0.2377, Steps: 36754, Time: 143.49s\n",
      "Ações: Manter=9350, Comprar=16633, Vender=10771\n",
      "Ganhos Totais: 38586.25, Perdas Totais: -32083.75\n",
      "Modelo e log do episódio 74 salvos em: 4.8.1\\model_episode_74.pth e 4.8.1\\log_episode_74.csv\n",
      "\n",
      "Episode 75/100, Total Reward: -2536.50, Win Rate: 0.55, Wins: 1076, Losses: 880, Epsilon: 0.2353, Steps: 36754, Time: 139.22s\n",
      "Ações: Manter=7676, Comprar=16771, Vender=12307\n",
      "Ganhos Totais: 32309.25, Perdas Totais: -34845.75\n",
      "Episode 76/100, Total Reward: 351.50, Win Rate: 0.56, Wins: 1132, Losses: 888, Epsilon: 0.2329, Steps: 36754, Time: 142.29s\n",
      "Ações: Manter=10623, Comprar=13278, Vender=12853\n",
      "Ganhos Totais: 35706.75, Perdas Totais: -35355.25\n",
      "Episode 77/100, Total Reward: 640.50, Win Rate: 0.57, Wins: 1192, Losses: 898, Epsilon: 0.2306, Steps: 36754, Time: 137.65s\n",
      "Ações: Manter=9860, Comprar=14401, Vender=12493\n",
      "Ganhos Totais: 35599.00, Perdas Totais: -34958.50\n",
      "Episode 78/100, Total Reward: -578.75, Win Rate: 0.56, Wins: 1140, Losses: 904, Epsilon: 0.2283, Steps: 36754, Time: 142.74s\n",
      "Ações: Manter=11763, Comprar=10686, Vender=14305\n",
      "Ganhos Totais: 33899.00, Perdas Totais: -34477.75\n",
      "Episode 79/100, Total Reward: -812.50, Win Rate: 0.54, Wins: 1158, Losses: 976, Epsilon: 0.2260, Steps: 36754, Time: 138.32s\n",
      "Ações: Manter=9109, Comprar=16167, Vender=11478\n",
      "Ganhos Totais: 34713.00, Perdas Totais: -35525.50\n",
      "Episode 80/100, Total Reward: -1770.25, Win Rate: 0.55, Wins: 1108, Losses: 894, Epsilon: 0.2238, Steps: 36754, Time: 125.87s\n",
      "Ações: Manter=11855, Comprar=12163, Vender=12736\n",
      "Ganhos Totais: 33343.25, Perdas Totais: -35113.50\n",
      "Episode 81/100, Total Reward: 1927.75, Win Rate: 0.57, Wins: 1129, Losses: 853, Epsilon: 0.2215, Steps: 36754, Time: 134.32s\n",
      "Ações: Manter=11758, Comprar=12836, Vender=12160\n",
      "Ganhos Totais: 36475.00, Perdas Totais: -34547.25\n",
      "Episode 82/100, Total Reward: -700.75, Win Rate: 0.54, Wins: 988, Losses: 839, Epsilon: 0.2193, Steps: 36754, Time: 127.16s\n",
      "Ações: Manter=14490, Comprar=9138, Vender=13126\n",
      "Ganhos Totais: 33788.25, Perdas Totais: -34489.00\n",
      "Episode 83/100, Total Reward: -3984.50, Win Rate: 0.54, Wins: 917, Losses: 797, Epsilon: 0.2171, Steps: 36754, Time: 126.72s\n",
      "Ações: Manter=16244, Comprar=7608, Vender=12902\n",
      "Ganhos Totais: 31061.00, Perdas Totais: -35045.50\n",
      "Episode 84/100, Total Reward: 318.25, Win Rate: 0.54, Wins: 939, Losses: 797, Epsilon: 0.2149, Steps: 36754, Time: 126.37s\n",
      "Ações: Manter=12834, Comprar=11808, Vender=12112\n",
      "Ganhos Totais: 33104.25, Perdas Totais: -32786.00\n",
      "Episode 85/100, Total Reward: -1900.50, Win Rate: 0.54, Wins: 955, Losses: 824, Epsilon: 0.2128, Steps: 36754, Time: 126.03s\n",
      "Ações: Manter=12024, Comprar=12747, Vender=11983\n",
      "Ganhos Totais: 32298.50, Perdas Totais: -34199.00\n",
      "Episode 86/100, Total Reward: -1121.75, Win Rate: 0.56, Wins: 1023, Losses: 815, Epsilon: 0.2107, Steps: 36754, Time: 126.94s\n",
      "Ações: Manter=8920, Comprar=16419, Vender=11415\n",
      "Ganhos Totais: 33574.25, Perdas Totais: -34696.00\n",
      "Episode 87/100, Total Reward: -402.75, Win Rate: 0.54, Wins: 984, Losses: 836, Epsilon: 0.2086, Steps: 36754, Time: 125.13s\n",
      "Ações: Manter=12376, Comprar=12683, Vender=11695\n",
      "Ganhos Totais: 34091.25, Perdas Totais: -34494.00\n",
      "Episode 88/100, Total Reward: 1207.75, Win Rate: 0.54, Wins: 1009, Losses: 844, Epsilon: 0.2065, Steps: 36754, Time: 126.07s\n",
      "Ações: Manter=8974, Comprar=14174, Vender=13606\n",
      "Ganhos Totais: 34889.00, Perdas Totais: -33681.25\n",
      "Episode 89/100, Total Reward: -969.75, Win Rate: 0.54, Wins: 979, Losses: 834, Epsilon: 0.2044, Steps: 36754, Time: 125.68s\n",
      "Ações: Manter=11202, Comprar=12801, Vender=12751\n",
      "Ganhos Totais: 32138.00, Perdas Totais: -33107.75\n",
      "Episode 90/100, Total Reward: 2265.25, Win Rate: 0.57, Wins: 1037, Losses: 795, Epsilon: 0.2024, Steps: 36754, Time: 125.49s\n",
      "Ações: Manter=8969, Comprar=16668, Vender=11117\n",
      "Ganhos Totais: 35675.00, Perdas Totais: -33409.75\n",
      "Modelo e log do episódio 90 salvos em: 4.8.1\\model_episode_90.pth e 4.8.1\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: 1293.50, Win Rate: 0.54, Wins: 990, Losses: 856, Epsilon: 0.2003, Steps: 36754, Time: 126.00s\n",
      "Ações: Manter=9379, Comprar=15734, Vender=11641\n",
      "Ganhos Totais: 35044.75, Perdas Totais: -33751.25\n",
      "Episode 92/100, Total Reward: 4380.50, Win Rate: 0.58, Wins: 1033, Losses: 755, Epsilon: 0.1983, Steps: 36754, Time: 125.77s\n",
      "Ações: Manter=8103, Comprar=15497, Vender=13154\n",
      "Ganhos Totais: 36585.50, Perdas Totais: -32205.00\n",
      "Modelo e log do episódio 92 salvos em: 4.8.1\\model_episode_92.pth e 4.8.1\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: 2743.75, Win Rate: 0.57, Wins: 1011, Losses: 764, Epsilon: 0.1964, Steps: 36754, Time: 125.44s\n",
      "Ações: Manter=8667, Comprar=15339, Vender=12748\n",
      "Ganhos Totais: 34990.75, Perdas Totais: -32247.00\n",
      "Modelo e log do episódio 93 salvos em: 4.8.1\\model_episode_93.pth e 4.8.1\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: -1804.25, Win Rate: 0.55, Wins: 938, Losses: 755, Epsilon: 0.1944, Steps: 36754, Time: 125.68s\n",
      "Ações: Manter=8632, Comprar=16524, Vender=11598\n",
      "Ganhos Totais: 32158.50, Perdas Totais: -33962.75\n",
      "Episode 95/100, Total Reward: 2337.00, Win Rate: 0.57, Wins: 1061, Losses: 790, Epsilon: 0.1924, Steps: 36754, Time: 126.26s\n",
      "Ações: Manter=9022, Comprar=15202, Vender=12530\n",
      "Ganhos Totais: 35499.75, Perdas Totais: -33162.75\n",
      "Episode 96/100, Total Reward: 896.25, Win Rate: 0.56, Wins: 997, Losses: 797, Epsilon: 0.1905, Steps: 36754, Time: 125.27s\n",
      "Ações: Manter=9639, Comprar=15355, Vender=11760\n",
      "Ganhos Totais: 35056.50, Perdas Totais: -34160.25\n",
      "Episode 97/100, Total Reward: -1341.75, Win Rate: 0.54, Wins: 992, Losses: 835, Epsilon: 0.1886, Steps: 36754, Time: 127.22s\n",
      "Ações: Manter=9620, Comprar=15295, Vender=11839\n",
      "Ganhos Totais: 33478.00, Perdas Totais: -34819.75\n",
      "Episode 98/100, Total Reward: -1603.50, Win Rate: 0.55, Wins: 974, Losses: 794, Epsilon: 0.1867, Steps: 36754, Time: 126.04s\n",
      "Ações: Manter=10019, Comprar=13887, Vender=12848\n",
      "Ganhos Totais: 32966.25, Perdas Totais: -34569.75\n",
      "Episode 99/100, Total Reward: -131.00, Win Rate: 0.56, Wins: 935, Losses: 727, Epsilon: 0.1849, Steps: 36754, Time: 125.58s\n",
      "Ações: Manter=11748, Comprar=12088, Vender=12918\n",
      "Ganhos Totais: 32881.50, Perdas Totais: -33012.50\n",
      "Episode 100/100, Total Reward: 861.25, Win Rate: 0.55, Wins: 1007, Losses: 809, Epsilon: 0.1830, Steps: 36754, Time: 127.38s\n",
      "Ações: Manter=8001, Comprar=15599, Vender=13154\n",
      "Ganhos Totais: 35391.00, Perdas Totais: -34529.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 74, Total Reward: 6502.50, Win Rate: 0.59, Wins: 1233, Losses: 853, Ações: {0: 9350, 1: 16633, 2: 10771}, Steps: 36754, Time: 143.49s\n",
      "Rank 2: Episode 92, Total Reward: 4380.50, Win Rate: 0.58, Wins: 1033, Losses: 755, Ações: {0: 8103, 1: 15497, 2: 13154}, Steps: 36754, Time: 125.77s\n",
      "Rank 3: Episode 4, Total Reward: 3732.75, Win Rate: 0.53, Wins: 1545, Losses: 1377, Ações: {0: 11905, 1: 11701, 2: 13148}, Steps: 36754, Time: 126.29s\n",
      "Rank 4: Episode 8, Total Reward: 3516.75, Win Rate: 0.54, Wins: 1551, Losses: 1304, Ações: {0: 11899, 1: 13451, 2: 11404}, Steps: 36754, Time: 106.21s\n",
      "Rank 5: Episode 64, Total Reward: 2954.50, Win Rate: 0.56, Wins: 1241, Losses: 990, Ações: {0: 10800, 1: 12706, 2: 13248}, Steps: 36754, Time: 152.42s\n",
      "Rank 6: Episode 93, Total Reward: 2743.75, Win Rate: 0.57, Wins: 1011, Losses: 764, Ações: {0: 8667, 1: 15339, 2: 12748}, Steps: 36754, Time: 125.44s\n",
      "Rank 7: Episode 49, Total Reward: 2706.50, Win Rate: 0.56, Wins: 1381, Losses: 1082, Ações: {0: 8725, 1: 15905, 2: 12124}, Steps: 36754, Time: 165.98s\n",
      "Rank 8: Episode 70, Total Reward: 2663.50, Win Rate: 0.57, Wins: 1182, Losses: 899, Ações: {0: 14214, 1: 9985, 2: 12555}, Steps: 36754, Time: 149.49s\n",
      "Rank 9: Episode 26, Total Reward: 2598.75, Win Rate: 0.54, Wins: 1501, Losses: 1269, Ações: {0: 12444, 1: 12862, 2: 11448}, Steps: 36754, Time: 109.05s\n",
      "Rank 10: Episode 13, Total Reward: 2400.25, Win Rate: 0.55, Wins: 1417, Losses: 1182, Ações: {0: 12370, 1: 12348, 2: 12036}, Steps: 36754, Time: 107.57s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.8.1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
