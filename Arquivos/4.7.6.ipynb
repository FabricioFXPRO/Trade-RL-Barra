{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -1981.00, Win Rate: 0.49, Wins: 1434, Losses: 1496, Epsilon: 0.4950, Steps: 36754, Time: 150.19s\n",
      "Ações: Manter=11545, Comprar=12908, Vender=12301\n",
      "Ganhos Totais: 37102.75, Perdas Totais: -39083.75\n",
      "Modelo e log do episódio 1 salvos em: 4.7.6\\model_episode_1.pth e 4.7.6\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -1826.00, Win Rate: 0.52, Wins: 1597, Losses: 1489, Epsilon: 0.4900, Steps: 36754, Time: 151.80s\n",
      "Ações: Manter=10727, Comprar=13253, Vender=12774\n",
      "Ganhos Totais: 37960.25, Perdas Totais: -39786.25\n",
      "Modelo e log do episódio 2 salvos em: 4.7.6\\model_episode_2.pth e 4.7.6\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: 1402.50, Win Rate: 0.53, Wins: 1546, Losses: 1353, Epsilon: 0.4851, Steps: 36754, Time: 148.88s\n",
      "Ações: Manter=12000, Comprar=12452, Vender=12302\n",
      "Ganhos Totais: 40505.00, Perdas Totais: -39102.50\n",
      "Modelo e log do episódio 3 salvos em: 4.7.6\\model_episode_3.pth e 4.7.6\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -4283.25, Win Rate: 0.52, Wins: 1504, Losses: 1381, Epsilon: 0.4803, Steps: 36754, Time: 167.22s\n",
      "Ações: Manter=11838, Comprar=12840, Vender=12076\n",
      "Ganhos Totais: 36233.75, Perdas Totais: -40517.00\n",
      "Modelo e log do episódio 4 salvos em: 4.7.6\\model_episode_4.pth e 4.7.6\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -2208.25, Win Rate: 0.52, Wins: 1488, Losses: 1361, Epsilon: 0.4755, Steps: 36754, Time: 175.19s\n",
      "Ações: Manter=11874, Comprar=13082, Vender=11798\n",
      "Ganhos Totais: 37872.00, Perdas Totais: -40080.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.6\\model_episode_5.pth e 4.7.6\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 118.00, Win Rate: 0.55, Wins: 1600, Losses: 1322, Epsilon: 0.4707, Steps: 36754, Time: 184.44s\n",
      "Ações: Manter=11246, Comprar=13331, Vender=12177\n",
      "Ganhos Totais: 38602.75, Perdas Totais: -38484.75\n",
      "Modelo e log do episódio 6 salvos em: 4.7.6\\model_episode_6.pth e 4.7.6\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -2456.75, Win Rate: 0.52, Wins: 1504, Losses: 1372, Epsilon: 0.4660, Steps: 36754, Time: 193.11s\n",
      "Ações: Manter=12046, Comprar=12435, Vender=12273\n",
      "Ganhos Totais: 37646.00, Perdas Totais: -40102.75\n",
      "Modelo e log do episódio 7 salvos em: 4.7.6\\model_episode_7.pth e 4.7.6\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -5205.00, Win Rate: 0.51, Wins: 1431, Losses: 1362, Epsilon: 0.4614, Steps: 36754, Time: 194.55s\n",
      "Ações: Manter=11456, Comprar=13344, Vender=11954\n",
      "Ganhos Totais: 36892.50, Perdas Totais: -42097.50\n",
      "Modelo e log do episódio 8 salvos em: 4.7.6\\model_episode_8.pth e 4.7.6\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -553.75, Win Rate: 0.53, Wins: 1509, Losses: 1361, Epsilon: 0.4568, Steps: 36754, Time: 194.79s\n",
      "Ações: Manter=12004, Comprar=13145, Vender=11605\n",
      "Ganhos Totais: 39099.50, Perdas Totais: -39653.25\n",
      "Modelo e log do episódio 9 salvos em: 4.7.6\\model_episode_9.pth e 4.7.6\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -489.50, Win Rate: 0.53, Wins: 1545, Losses: 1358, Epsilon: 0.4522, Steps: 36754, Time: 193.36s\n",
      "Ações: Manter=12472, Comprar=12750, Vender=11532\n",
      "Ganhos Totais: 37755.25, Perdas Totais: -38244.75\n",
      "Modelo e log do episódio 10 salvos em: 4.7.6\\model_episode_10.pth e 4.7.6\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -3266.25, Win Rate: 0.53, Wins: 1498, Losses: 1342, Epsilon: 0.4477, Steps: 36754, Time: 195.31s\n",
      "Ações: Manter=12176, Comprar=12093, Vender=12485\n",
      "Ganhos Totais: 36817.50, Perdas Totais: -40083.75\n",
      "Modelo e log do episódio 11 salvos em: 4.7.6\\model_episode_11.pth e 4.7.6\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 1723.75, Win Rate: 0.53, Wins: 1488, Losses: 1308, Epsilon: 0.4432, Steps: 36754, Time: 195.89s\n",
      "Ações: Manter=12419, Comprar=10818, Vender=13517\n",
      "Ganhos Totais: 39123.00, Perdas Totais: -37399.25\n",
      "Modelo e log do episódio 12 salvos em: 4.7.6\\model_episode_12.pth e 4.7.6\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -4275.75, Win Rate: 0.52, Wins: 1372, Losses: 1284, Epsilon: 0.4388, Steps: 36754, Time: 196.18s\n",
      "Ações: Manter=12642, Comprar=10581, Vender=13531\n",
      "Ganhos Totais: 35591.75, Perdas Totais: -39867.50\n",
      "Episode 14/100, Total Reward: -1746.00, Win Rate: 0.54, Wins: 1471, Losses: 1237, Epsilon: 0.4344, Steps: 36754, Time: 193.35s\n",
      "Ações: Manter=13042, Comprar=11009, Vender=12703\n",
      "Ganhos Totais: 35862.50, Perdas Totais: -37608.50\n",
      "Modelo e log do episódio 14 salvos em: 4.7.6\\model_episode_14.pth e 4.7.6\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: 2171.50, Win Rate: 0.54, Wins: 1439, Losses: 1226, Epsilon: 0.4300, Steps: 36754, Time: 196.65s\n",
      "Ações: Manter=13084, Comprar=10721, Vender=12949\n",
      "Ganhos Totais: 38428.50, Perdas Totais: -36257.00\n",
      "Modelo e log do episódio 15 salvos em: 4.7.6\\model_episode_15.pth e 4.7.6\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -2459.75, Win Rate: 0.53, Wins: 1375, Losses: 1220, Epsilon: 0.4257, Steps: 36754, Time: 136.19s\n",
      "Ações: Manter=13223, Comprar=10661, Vender=12870\n",
      "Ganhos Totais: 35815.00, Perdas Totais: -38274.75\n",
      "Episode 17/100, Total Reward: -1691.00, Win Rate: 0.53, Wins: 1386, Losses: 1229, Epsilon: 0.4215, Steps: 36754, Time: 127.10s\n",
      "Ações: Manter=12833, Comprar=10911, Vender=13010\n",
      "Ganhos Totais: 36759.50, Perdas Totais: -38450.50\n",
      "Modelo e log do episódio 17 salvos em: 4.7.6\\model_episode_17.pth e 4.7.6\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: 1023.25, Win Rate: 0.54, Wins: 1425, Losses: 1200, Epsilon: 0.4173, Steps: 36754, Time: 131.39s\n",
      "Ações: Manter=12755, Comprar=11141, Vender=12858\n",
      "Ganhos Totais: 37694.00, Perdas Totais: -36670.75\n",
      "Modelo e log do episódio 18 salvos em: 4.7.6\\model_episode_18.pth e 4.7.6\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -3831.50, Win Rate: 0.52, Wins: 1323, Losses: 1236, Epsilon: 0.4131, Steps: 36754, Time: 134.76s\n",
      "Ações: Manter=12708, Comprar=11015, Vender=13031\n",
      "Ganhos Totais: 35354.25, Perdas Totais: -39185.75\n",
      "Episode 20/100, Total Reward: -1431.75, Win Rate: 0.53, Wins: 1341, Losses: 1198, Epsilon: 0.4090, Steps: 36754, Time: 134.22s\n",
      "Ações: Manter=13438, Comprar=10976, Vender=12340\n",
      "Ganhos Totais: 37269.25, Perdas Totais: -38701.00\n",
      "Modelo e log do episódio 20 salvos em: 4.7.6\\model_episode_20.pth e 4.7.6\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -2340.00, Win Rate: 0.51, Wins: 1294, Losses: 1260, Epsilon: 0.4049, Steps: 36754, Time: 136.36s\n",
      "Ações: Manter=13269, Comprar=11140, Vender=12345\n",
      "Ganhos Totais: 36418.75, Perdas Totais: -38758.75\n",
      "Episode 22/100, Total Reward: -1761.25, Win Rate: 0.55, Wins: 1409, Losses: 1175, Epsilon: 0.4008, Steps: 36754, Time: 131.05s\n",
      "Ações: Manter=13114, Comprar=12181, Vender=11459\n",
      "Ganhos Totais: 36026.50, Perdas Totais: -37787.75\n",
      "Episode 23/100, Total Reward: 742.25, Win Rate: 0.55, Wins: 1427, Losses: 1173, Epsilon: 0.3968, Steps: 36754, Time: 127.98s\n",
      "Ações: Manter=12821, Comprar=12758, Vender=11175\n",
      "Ganhos Totais: 38930.25, Perdas Totais: -38188.00\n",
      "Modelo e log do episódio 23 salvos em: 4.7.6\\model_episode_23.pth e 4.7.6\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: -1290.75, Win Rate: 0.54, Wins: 1390, Losses: 1194, Epsilon: 0.3928, Steps: 36754, Time: 129.80s\n",
      "Ações: Manter=12365, Comprar=12400, Vender=11989\n",
      "Ganhos Totais: 36306.00, Perdas Totais: -37596.75\n",
      "Modelo e log do episódio 24 salvos em: 4.7.6\\model_episode_24.pth e 4.7.6\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -3500.25, Win Rate: 0.54, Wins: 1411, Losses: 1192, Epsilon: 0.3889, Steps: 36754, Time: 154.59s\n",
      "Ações: Manter=12346, Comprar=13427, Vender=10981\n",
      "Ganhos Totais: 35340.75, Perdas Totais: -38841.00\n",
      "Episode 26/100, Total Reward: 567.00, Win Rate: 0.54, Wins: 1386, Losses: 1179, Epsilon: 0.3850, Steps: 36754, Time: 153.19s\n",
      "Ações: Manter=11679, Comprar=13543, Vender=11532\n",
      "Ganhos Totais: 37564.25, Perdas Totais: -36997.25\n",
      "Modelo e log do episódio 26 salvos em: 4.7.6\\model_episode_26.pth e 4.7.6\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: 1901.25, Win Rate: 0.55, Wins: 1417, Losses: 1165, Epsilon: 0.3812, Steps: 36754, Time: 160.13s\n",
      "Ações: Manter=12400, Comprar=12876, Vender=11478\n",
      "Ganhos Totais: 37280.50, Perdas Totais: -35379.25\n",
      "Modelo e log do episódio 27 salvos em: 4.7.6\\model_episode_27.pth e 4.7.6\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: -2013.75, Win Rate: 0.53, Wins: 1348, Losses: 1188, Epsilon: 0.3774, Steps: 36754, Time: 143.77s\n",
      "Ações: Manter=12677, Comprar=12267, Vender=11810\n",
      "Ganhos Totais: 35404.75, Perdas Totais: -37418.50\n",
      "Episode 29/100, Total Reward: 421.00, Win Rate: 0.55, Wins: 1411, Losses: 1158, Epsilon: 0.3736, Steps: 36754, Time: 146.34s\n",
      "Ações: Manter=12618, Comprar=12605, Vender=11531\n",
      "Ganhos Totais: 37689.75, Perdas Totais: -37268.75\n",
      "Modelo e log do episódio 29 salvos em: 4.7.6\\model_episode_29.pth e 4.7.6\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: 65.25, Win Rate: 0.55, Wins: 1376, Losses: 1112, Epsilon: 0.3699, Steps: 36754, Time: 140.89s\n",
      "Ações: Manter=14531, Comprar=11891, Vender=10332\n",
      "Ganhos Totais: 36303.75, Perdas Totais: -36238.50\n",
      "Modelo e log do episódio 30 salvos em: 4.7.6\\model_episode_30.pth e 4.7.6\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: 392.50, Win Rate: 0.54, Wins: 1289, Losses: 1115, Epsilon: 0.3662, Steps: 36754, Time: 146.59s\n",
      "Ações: Manter=13442, Comprar=12233, Vender=11079\n",
      "Ganhos Totais: 36807.25, Perdas Totais: -36414.75\n",
      "Modelo e log do episódio 31 salvos em: 4.7.6\\model_episode_31.pth e 4.7.6\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 1670.50, Win Rate: 0.55, Wins: 1304, Losses: 1065, Epsilon: 0.3625, Steps: 36754, Time: 153.09s\n",
      "Ações: Manter=12656, Comprar=13301, Vender=10797\n",
      "Ganhos Totais: 37865.25, Perdas Totais: -36194.75\n",
      "Modelo e log do episódio 32 salvos em: 4.7.6\\model_episode_32.pth e 4.7.6\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 756.50, Win Rate: 0.55, Wins: 1293, Losses: 1058, Epsilon: 0.3589, Steps: 36754, Time: 141.08s\n",
      "Ações: Manter=12647, Comprar=13254, Vender=10853\n",
      "Ganhos Totais: 36217.25, Perdas Totais: -35460.75\n",
      "Modelo e log do episódio 33 salvos em: 4.7.6\\model_episode_33.pth e 4.7.6\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: -879.50, Win Rate: 0.54, Wins: 1206, Losses: 1044, Epsilon: 0.3553, Steps: 36754, Time: 135.59s\n",
      "Ações: Manter=11813, Comprar=13650, Vender=11291\n",
      "Ganhos Totais: 35741.25, Perdas Totais: -36620.75\n",
      "Episode 35/100, Total Reward: 903.50, Win Rate: 0.54, Wins: 1219, Losses: 1051, Epsilon: 0.3517, Steps: 36754, Time: 130.22s\n",
      "Ações: Manter=12549, Comprar=12977, Vender=11228\n",
      "Ganhos Totais: 36614.25, Perdas Totais: -35710.75\n",
      "Modelo e log do episódio 35 salvos em: 4.7.6\\model_episode_35.pth e 4.7.6\\log_episode_35.csv\n",
      "\n",
      "Episode 36/100, Total Reward: 577.50, Win Rate: 0.53, Wins: 1264, Losses: 1108, Epsilon: 0.3482, Steps: 36754, Time: 128.28s\n",
      "Ações: Manter=12552, Comprar=13402, Vender=10800\n",
      "Ganhos Totais: 37146.00, Perdas Totais: -36568.50\n",
      "Modelo e log do episódio 36 salvos em: 4.7.6\\model_episode_36.pth e 4.7.6\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: -3310.00, Win Rate: 0.51, Wins: 1163, Losses: 1109, Epsilon: 0.3447, Steps: 36754, Time: 128.62s\n",
      "Ações: Manter=13033, Comprar=13233, Vender=10488\n",
      "Ganhos Totais: 34668.00, Perdas Totais: -37978.00\n",
      "Episode 38/100, Total Reward: 2301.50, Win Rate: 0.54, Wins: 1239, Losses: 1035, Epsilon: 0.3413, Steps: 36754, Time: 128.22s\n",
      "Ações: Manter=12948, Comprar=13493, Vender=10313\n",
      "Ganhos Totais: 37328.00, Perdas Totais: -35026.50\n",
      "Modelo e log do episódio 38 salvos em: 4.7.6\\model_episode_38.pth e 4.7.6\\log_episode_38.csv\n",
      "\n",
      "Episode 39/100, Total Reward: 2524.50, Win Rate: 0.54, Wins: 1228, Losses: 1026, Epsilon: 0.3379, Steps: 36754, Time: 128.04s\n",
      "Ações: Manter=13450, Comprar=12695, Vender=10609\n",
      "Ganhos Totais: 36173.25, Perdas Totais: -33648.75\n",
      "Modelo e log do episódio 39 salvos em: 4.7.6\\model_episode_39.pth e 4.7.6\\log_episode_39.csv\n",
      "\n",
      "Episode 40/100, Total Reward: 1815.50, Win Rate: 0.53, Wins: 1189, Losses: 1036, Epsilon: 0.3345, Steps: 36754, Time: 128.51s\n",
      "Ações: Manter=13752, Comprar=12286, Vender=10716\n",
      "Ganhos Totais: 35925.75, Perdas Totais: -34110.25\n",
      "Modelo e log do episódio 40 salvos em: 4.7.6\\model_episode_40.pth e 4.7.6\\log_episode_40.csv\n",
      "\n",
      "Episode 41/100, Total Reward: 1009.25, Win Rate: 0.54, Wins: 1214, Losses: 1044, Epsilon: 0.3311, Steps: 36754, Time: 128.59s\n",
      "Ações: Manter=13563, Comprar=12768, Vender=10423\n",
      "Ganhos Totais: 37022.25, Perdas Totais: -36013.00\n",
      "Modelo e log do episódio 41 salvos em: 4.7.6\\model_episode_41.pth e 4.7.6\\log_episode_41.csv\n",
      "\n",
      "Episode 42/100, Total Reward: 1812.00, Win Rate: 0.55, Wins: 1207, Losses: 987, Epsilon: 0.3278, Steps: 36754, Time: 127.95s\n",
      "Ações: Manter=11645, Comprar=14984, Vender=10125\n",
      "Ganhos Totais: 35632.00, Perdas Totais: -33820.00\n",
      "Modelo e log do episódio 42 salvos em: 4.7.6\\model_episode_42.pth e 4.7.6\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: 970.00, Win Rate: 0.55, Wins: 1221, Losses: 1014, Epsilon: 0.3246, Steps: 36754, Time: 128.38s\n",
      "Ações: Manter=11699, Comprar=15043, Vender=10012\n",
      "Ganhos Totais: 35173.00, Perdas Totais: -34203.00\n",
      "Episode 44/100, Total Reward: 4435.25, Win Rate: 0.57, Wins: 1234, Losses: 915, Epsilon: 0.3213, Steps: 36754, Time: 128.51s\n",
      "Ações: Manter=12552, Comprar=14123, Vender=10079\n",
      "Ganhos Totais: 37661.00, Perdas Totais: -33225.75\n",
      "Modelo e log do episódio 44 salvos em: 4.7.6\\model_episode_44.pth e 4.7.6\\log_episode_44.csv\n",
      "\n",
      "Episode 45/100, Total Reward: 4835.00, Win Rate: 0.54, Wins: 1192, Losses: 1010, Epsilon: 0.3181, Steps: 36754, Time: 127.96s\n",
      "Ações: Manter=13033, Comprar=14309, Vender=9412\n",
      "Ganhos Totais: 37629.75, Perdas Totais: -32794.75\n",
      "Modelo e log do episódio 45 salvos em: 4.7.6\\model_episode_45.pth e 4.7.6\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: -1917.75, Win Rate: 0.55, Wins: 1293, Losses: 1051, Epsilon: 0.3149, Steps: 36754, Time: 129.47s\n",
      "Ações: Manter=11109, Comprar=13839, Vender=11806\n",
      "Ganhos Totais: 35666.00, Perdas Totais: -37583.75\n",
      "Episode 47/100, Total Reward: 2474.25, Win Rate: 0.55, Wins: 1220, Losses: 1000, Epsilon: 0.3118, Steps: 36754, Time: 147.99s\n",
      "Ações: Manter=12995, Comprar=14017, Vender=9742\n",
      "Ganhos Totais: 36983.50, Perdas Totais: -34509.25\n",
      "Modelo e log do episódio 47 salvos em: 4.7.6\\model_episode_47.pth e 4.7.6\\log_episode_47.csv\n",
      "\n",
      "Episode 48/100, Total Reward: 1017.50, Win Rate: 0.56, Wins: 1211, Losses: 960, Epsilon: 0.3086, Steps: 36754, Time: 145.79s\n",
      "Ações: Manter=12888, Comprar=14286, Vender=9580\n",
      "Ganhos Totais: 35659.00, Perdas Totais: -34641.50\n",
      "Episode 49/100, Total Reward: 3352.50, Win Rate: 0.56, Wins: 1267, Losses: 1015, Epsilon: 0.3056, Steps: 36754, Time: 146.42s\n",
      "Ações: Manter=12346, Comprar=13768, Vender=10640\n",
      "Ganhos Totais: 37328.75, Perdas Totais: -33976.25\n",
      "Modelo e log do episódio 49 salvos em: 4.7.6\\model_episode_49.pth e 4.7.6\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: -856.00, Win Rate: 0.53, Wins: 1177, Losses: 1027, Epsilon: 0.3025, Steps: 36754, Time: 145.49s\n",
      "Ações: Manter=13177, Comprar=13302, Vender=10275\n",
      "Ganhos Totais: 35518.50, Perdas Totais: -36374.50\n",
      "Episode 51/100, Total Reward: 8048.25, Win Rate: 0.57, Wins: 1293, Losses: 971, Epsilon: 0.2995, Steps: 36754, Time: 145.94s\n",
      "Ações: Manter=12714, Comprar=13167, Vender=10873\n",
      "Ganhos Totais: 40683.25, Perdas Totais: -32635.00\n",
      "Modelo e log do episódio 51 salvos em: 4.7.6\\model_episode_51.pth e 4.7.6\\log_episode_51.csv\n",
      "\n",
      "Episode 52/100, Total Reward: 3135.00, Win Rate: 0.56, Wins: 1242, Losses: 993, Epsilon: 0.2965, Steps: 36754, Time: 146.07s\n",
      "Ações: Manter=13036, Comprar=13244, Vender=10474\n",
      "Ganhos Totais: 37587.50, Perdas Totais: -34452.50\n",
      "Modelo e log do episódio 52 salvos em: 4.7.6\\model_episode_52.pth e 4.7.6\\log_episode_52.csv\n",
      "\n",
      "Episode 53/100, Total Reward: 530.25, Win Rate: 0.55, Wins: 1140, Losses: 942, Epsilon: 0.2935, Steps: 36754, Time: 146.56s\n",
      "Ações: Manter=11842, Comprar=12952, Vender=11960\n",
      "Ganhos Totais: 35847.25, Perdas Totais: -35317.00\n",
      "Episode 54/100, Total Reward: 1671.75, Win Rate: 0.56, Wins: 1184, Losses: 913, Epsilon: 0.2906, Steps: 36754, Time: 146.22s\n",
      "Ações: Manter=11371, Comprar=13313, Vender=12070\n",
      "Ganhos Totais: 35805.50, Perdas Totais: -34133.75\n",
      "Episode 55/100, Total Reward: 1586.75, Win Rate: 0.54, Wins: 1116, Losses: 963, Epsilon: 0.2877, Steps: 36754, Time: 145.97s\n",
      "Ações: Manter=11559, Comprar=13772, Vender=11423\n",
      "Ganhos Totais: 36056.75, Perdas Totais: -34470.00\n",
      "Episode 56/100, Total Reward: -2848.00, Win Rate: 0.53, Wins: 1056, Losses: 937, Epsilon: 0.2848, Steps: 36754, Time: 148.64s\n",
      "Ações: Manter=12064, Comprar=12681, Vender=12009\n",
      "Ganhos Totais: 33456.75, Perdas Totais: -36304.75\n",
      "Episode 57/100, Total Reward: 2872.00, Win Rate: 0.55, Wins: 1172, Losses: 944, Epsilon: 0.2820, Steps: 36754, Time: 130.11s\n",
      "Ações: Manter=14120, Comprar=11795, Vender=10839\n",
      "Ganhos Totais: 37319.75, Perdas Totais: -34447.75\n",
      "Modelo e log do episódio 57 salvos em: 4.7.6\\model_episode_57.pth e 4.7.6\\log_episode_57.csv\n",
      "\n",
      "Episode 58/100, Total Reward: 3239.25, Win Rate: 0.56, Wins: 1176, Losses: 920, Epsilon: 0.2791, Steps: 36754, Time: 129.35s\n",
      "Ações: Manter=12677, Comprar=12896, Vender=11181\n",
      "Ganhos Totais: 37596.75, Perdas Totais: -34357.50\n",
      "Modelo e log do episódio 58 salvos em: 4.7.6\\model_episode_58.pth e 4.7.6\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: 207.75, Win Rate: 0.55, Wins: 1063, Losses: 862, Epsilon: 0.2763, Steps: 36754, Time: 129.83s\n",
      "Ações: Manter=13069, Comprar=12334, Vender=11351\n",
      "Ganhos Totais: 34203.50, Perdas Totais: -33995.75\n",
      "Episode 60/100, Total Reward: 1091.00, Win Rate: 0.56, Wins: 1240, Losses: 990, Epsilon: 0.2736, Steps: 36754, Time: 155.99s\n",
      "Ações: Manter=11174, Comprar=14194, Vender=11386\n",
      "Ganhos Totais: 36738.50, Perdas Totais: -35647.50\n",
      "Episode 61/100, Total Reward: -852.75, Win Rate: 0.55, Wins: 1088, Losses: 874, Epsilon: 0.2708, Steps: 36754, Time: 148.78s\n",
      "Ações: Manter=11994, Comprar=12741, Vender=12019\n",
      "Ganhos Totais: 34449.25, Perdas Totais: -35302.00\n",
      "Episode 62/100, Total Reward: 2139.75, Win Rate: 0.54, Wins: 1200, Losses: 1016, Epsilon: 0.2681, Steps: 36754, Time: 147.65s\n",
      "Ações: Manter=9581, Comprar=15863, Vender=11310\n",
      "Ganhos Totais: 37346.50, Perdas Totais: -35206.75\n",
      "Episode 63/100, Total Reward: 454.25, Win Rate: 0.56, Wins: 1155, Losses: 906, Epsilon: 0.2655, Steps: 36754, Time: 147.19s\n",
      "Ações: Manter=11133, Comprar=14160, Vender=11461\n",
      "Ganhos Totais: 35300.00, Perdas Totais: -34845.75\n",
      "Episode 64/100, Total Reward: 4870.75, Win Rate: 0.56, Wins: 1164, Losses: 922, Epsilon: 0.2628, Steps: 36754, Time: 147.23s\n",
      "Ações: Manter=11454, Comprar=14576, Vender=10724\n",
      "Ganhos Totais: 38219.00, Perdas Totais: -33348.25\n",
      "Modelo e log do episódio 64 salvos em: 4.7.6\\model_episode_64.pth e 4.7.6\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: -1063.50, Win Rate: 0.56, Wins: 1089, Losses: 872, Epsilon: 0.2602, Steps: 36754, Time: 148.10s\n",
      "Ações: Manter=12513, Comprar=13445, Vender=10796\n",
      "Ganhos Totais: 33126.00, Perdas Totais: -34189.50\n",
      "Episode 66/100, Total Reward: 3643.75, Win Rate: 0.58, Wins: 1173, Losses: 866, Epsilon: 0.2576, Steps: 36754, Time: 147.00s\n",
      "Ações: Manter=10422, Comprar=14277, Vender=12055\n",
      "Ganhos Totais: 36852.25, Perdas Totais: -33208.50\n",
      "Modelo e log do episódio 66 salvos em: 4.7.6\\model_episode_66.pth e 4.7.6\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: 1456.00, Win Rate: 0.55, Wins: 1099, Losses: 890, Epsilon: 0.2550, Steps: 36754, Time: 147.38s\n",
      "Ações: Manter=11750, Comprar=13177, Vender=11827\n",
      "Ganhos Totais: 36145.25, Perdas Totais: -34689.25\n",
      "Episode 68/100, Total Reward: 1544.00, Win Rate: 0.56, Wins: 1106, Losses: 864, Epsilon: 0.2524, Steps: 36754, Time: 147.50s\n",
      "Ações: Manter=11625, Comprar=14420, Vender=10709\n",
      "Ganhos Totais: 36438.75, Perdas Totais: -34894.75\n",
      "Episode 69/100, Total Reward: -732.75, Win Rate: 0.56, Wins: 1130, Losses: 879, Epsilon: 0.2499, Steps: 36754, Time: 147.38s\n",
      "Ações: Manter=11480, Comprar=11986, Vender=13288\n",
      "Ganhos Totais: 35045.00, Perdas Totais: -35777.75\n",
      "Episode 70/100, Total Reward: -100.00, Win Rate: 0.56, Wins: 1132, Losses: 874, Epsilon: 0.2474, Steps: 36754, Time: 147.57s\n",
      "Ações: Manter=11913, Comprar=13312, Vender=11529\n",
      "Ganhos Totais: 34195.25, Perdas Totais: -34295.25\n",
      "Episode 71/100, Total Reward: 359.75, Win Rate: 0.56, Wins: 1157, Losses: 902, Epsilon: 0.2449, Steps: 36754, Time: 144.22s\n",
      "Ações: Manter=11467, Comprar=14038, Vender=11249\n",
      "Ganhos Totais: 35931.25, Perdas Totais: -35571.50\n",
      "Episode 72/100, Total Reward: -2100.00, Win Rate: 0.55, Wins: 1062, Losses: 869, Epsilon: 0.2425, Steps: 36754, Time: 143.26s\n",
      "Ações: Manter=10458, Comprar=12899, Vender=13397\n",
      "Ganhos Totais: 33041.00, Perdas Totais: -35141.00\n",
      "Episode 73/100, Total Reward: -87.75, Win Rate: 0.56, Wins: 1164, Losses: 901, Epsilon: 0.2401, Steps: 36754, Time: 148.12s\n",
      "Ações: Manter=9711, Comprar=15231, Vender=11812\n",
      "Ganhos Totais: 35788.50, Perdas Totais: -35876.25\n",
      "Episode 74/100, Total Reward: 674.00, Win Rate: 0.55, Wins: 1003, Losses: 820, Epsilon: 0.2377, Steps: 36754, Time: 147.86s\n",
      "Ações: Manter=11387, Comprar=12487, Vender=12880\n",
      "Ganhos Totais: 33398.50, Perdas Totais: -32724.50\n",
      "Episode 75/100, Total Reward: -551.25, Win Rate: 0.56, Wins: 1077, Losses: 839, Epsilon: 0.2353, Steps: 36754, Time: 147.57s\n",
      "Ações: Manter=11522, Comprar=13529, Vender=11703\n",
      "Ganhos Totais: 33619.75, Perdas Totais: -34171.00\n",
      "Episode 76/100, Total Reward: 5242.25, Win Rate: 0.56, Wins: 1110, Losses: 865, Epsilon: 0.2329, Steps: 36754, Time: 147.04s\n",
      "Ações: Manter=12337, Comprar=12095, Vender=12322\n",
      "Ganhos Totais: 37315.00, Perdas Totais: -32072.75\n",
      "Modelo e log do episódio 76 salvos em: 4.7.6\\model_episode_76.pth e 4.7.6\\log_episode_76.csv\n",
      "\n",
      "Episode 77/100, Total Reward: -848.25, Win Rate: 0.56, Wins: 1077, Losses: 841, Epsilon: 0.2306, Steps: 36754, Time: 147.81s\n",
      "Ações: Manter=11294, Comprar=13272, Vender=12188\n",
      "Ganhos Totais: 33434.50, Perdas Totais: -34282.75\n",
      "Episode 78/100, Total Reward: 7540.75, Win Rate: 0.58, Wins: 1085, Losses: 779, Epsilon: 0.2283, Steps: 36754, Time: 148.11s\n",
      "Ações: Manter=12011, Comprar=12982, Vender=11761\n",
      "Ganhos Totais: 37828.75, Perdas Totais: -30288.00\n",
      "Modelo e log do episódio 78 salvos em: 4.7.6\\model_episode_78.pth e 4.7.6\\log_episode_78.csv\n",
      "\n",
      "Episode 79/100, Total Reward: -1680.25, Win Rate: 0.54, Wins: 974, Losses: 821, Epsilon: 0.2260, Steps: 36754, Time: 147.21s\n",
      "Ações: Manter=10662, Comprar=13258, Vender=12834\n",
      "Ganhos Totais: 33329.75, Perdas Totais: -35010.00\n",
      "Episode 80/100, Total Reward: 4396.25, Win Rate: 0.58, Wins: 1069, Losses: 789, Epsilon: 0.2238, Steps: 36754, Time: 147.75s\n",
      "Ações: Manter=13421, Comprar=10957, Vender=12376\n",
      "Ganhos Totais: 36240.25, Perdas Totais: -31844.00\n",
      "Modelo e log do episódio 80 salvos em: 4.7.6\\model_episode_80.pth e 4.7.6\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: -196.50, Win Rate: 0.57, Wins: 1075, Losses: 820, Epsilon: 0.2215, Steps: 36754, Time: 148.08s\n",
      "Ações: Manter=10319, Comprar=12368, Vender=14067\n",
      "Ganhos Totais: 34580.50, Perdas Totais: -34777.00\n",
      "Episode 82/100, Total Reward: 2152.50, Win Rate: 0.56, Wins: 988, Losses: 768, Epsilon: 0.2193, Steps: 36754, Time: 148.17s\n",
      "Ações: Manter=11053, Comprar=14194, Vender=11507\n",
      "Ganhos Totais: 34870.50, Perdas Totais: -32718.00\n",
      "Episode 83/100, Total Reward: -659.25, Win Rate: 0.55, Wins: 986, Losses: 812, Epsilon: 0.2171, Steps: 36754, Time: 148.32s\n",
      "Ações: Manter=10347, Comprar=13500, Vender=12907\n",
      "Ganhos Totais: 32952.25, Perdas Totais: -33611.50\n",
      "Episode 84/100, Total Reward: 522.25, Win Rate: 0.54, Wins: 930, Losses: 786, Epsilon: 0.2149, Steps: 36754, Time: 147.73s\n",
      "Ações: Manter=10585, Comprar=12687, Vender=13482\n",
      "Ganhos Totais: 33758.25, Perdas Totais: -33236.00\n",
      "Episode 85/100, Total Reward: 460.75, Win Rate: 0.56, Wins: 993, Losses: 777, Epsilon: 0.2128, Steps: 36754, Time: 148.01s\n",
      "Ações: Manter=11075, Comprar=13982, Vender=11697\n",
      "Ganhos Totais: 34946.50, Perdas Totais: -34485.75\n",
      "Episode 86/100, Total Reward: 1384.00, Win Rate: 0.56, Wins: 935, Losses: 746, Epsilon: 0.2107, Steps: 36754, Time: 147.94s\n",
      "Ações: Manter=11201, Comprar=13023, Vender=12530\n",
      "Ganhos Totais: 33532.00, Perdas Totais: -32148.00\n",
      "Episode 87/100, Total Reward: 2666.75, Win Rate: 0.55, Wins: 887, Losses: 728, Epsilon: 0.2086, Steps: 36754, Time: 148.10s\n",
      "Ações: Manter=12140, Comprar=12138, Vender=12476\n",
      "Ganhos Totais: 34421.75, Perdas Totais: -31755.00\n",
      "Episode 88/100, Total Reward: -4208.00, Win Rate: 0.55, Wins: 977, Losses: 810, Epsilon: 0.2065, Steps: 36754, Time: 148.26s\n",
      "Ações: Manter=11220, Comprar=11862, Vender=13672\n",
      "Ganhos Totais: 31416.00, Perdas Totais: -35624.00\n",
      "Episode 89/100, Total Reward: 2408.75, Win Rate: 0.56, Wins: 988, Losses: 770, Epsilon: 0.2044, Steps: 36754, Time: 147.98s\n",
      "Ações: Manter=11389, Comprar=11045, Vender=14320\n",
      "Ganhos Totais: 35192.50, Perdas Totais: -32783.75\n",
      "Episode 90/100, Total Reward: -712.50, Win Rate: 0.54, Wins: 921, Losses: 790, Epsilon: 0.2024, Steps: 36754, Time: 148.44s\n",
      "Ações: Manter=12272, Comprar=13552, Vender=10930\n",
      "Ganhos Totais: 33949.75, Perdas Totais: -34662.25\n",
      "Episode 91/100, Total Reward: 1322.50, Win Rate: 0.55, Wins: 1019, Losses: 823, Epsilon: 0.2003, Steps: 36754, Time: 148.10s\n",
      "Ações: Manter=11765, Comprar=11839, Vender=13150\n",
      "Ganhos Totais: 34934.50, Perdas Totais: -33612.00\n",
      "Episode 92/100, Total Reward: 2409.75, Win Rate: 0.57, Wins: 1023, Losses: 760, Epsilon: 0.1983, Steps: 36754, Time: 148.09s\n",
      "Ações: Manter=10012, Comprar=14472, Vender=12270\n",
      "Ganhos Totais: 36130.25, Perdas Totais: -33720.50\n",
      "Episode 93/100, Total Reward: 850.00, Win Rate: 0.55, Wins: 897, Losses: 744, Epsilon: 0.1964, Steps: 36754, Time: 145.87s\n",
      "Ações: Manter=9693, Comprar=15444, Vender=11617\n",
      "Ganhos Totais: 33144.50, Perdas Totais: -32294.50\n",
      "Episode 94/100, Total Reward: 345.25, Win Rate: 0.57, Wins: 989, Losses: 758, Epsilon: 0.1944, Steps: 36754, Time: 145.19s\n",
      "Ações: Manter=9661, Comprar=14625, Vender=12468\n",
      "Ganhos Totais: 34125.25, Perdas Totais: -33780.00\n",
      "Episode 95/100, Total Reward: -338.50, Win Rate: 0.56, Wins: 987, Losses: 766, Epsilon: 0.1924, Steps: 36754, Time: 146.21s\n",
      "Ações: Manter=9523, Comprar=11855, Vender=15376\n",
      "Ganhos Totais: 34483.25, Perdas Totais: -34821.75\n",
      "Episode 96/100, Total Reward: 1839.75, Win Rate: 0.57, Wins: 1010, Losses: 755, Epsilon: 0.1905, Steps: 36754, Time: 145.21s\n",
      "Ações: Manter=9871, Comprar=12901, Vender=13982\n",
      "Ganhos Totais: 34911.75, Perdas Totais: -33072.00\n",
      "Episode 97/100, Total Reward: -886.75, Win Rate: 0.56, Wins: 952, Losses: 752, Epsilon: 0.1886, Steps: 36754, Time: 145.34s\n",
      "Ações: Manter=9307, Comprar=12614, Vender=14833\n",
      "Ganhos Totais: 33276.50, Perdas Totais: -34163.25\n",
      "Episode 98/100, Total Reward: 3519.25, Win Rate: 0.56, Wins: 984, Losses: 762, Epsilon: 0.1867, Steps: 36754, Time: 146.75s\n",
      "Ações: Manter=10643, Comprar=14082, Vender=12029\n",
      "Ganhos Totais: 35896.75, Perdas Totais: -32377.50\n",
      "Modelo e log do episódio 98 salvos em: 4.7.6\\model_episode_98.pth e 4.7.6\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: -2701.00, Win Rate: 0.55, Wins: 950, Losses: 780, Epsilon: 0.1849, Steps: 36754, Time: 143.85s\n",
      "Ações: Manter=11156, Comprar=13084, Vender=12514\n",
      "Ganhos Totais: 33197.00, Perdas Totais: -35898.00\n",
      "Episode 100/100, Total Reward: -1626.00, Win Rate: 0.56, Wins: 964, Losses: 768, Epsilon: 0.1830, Steps: 36754, Time: 146.16s\n",
      "Ações: Manter=11157, Comprar=10550, Vender=15047\n",
      "Ganhos Totais: 33508.75, Perdas Totais: -35134.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 51, Total Reward: 8048.25, Win Rate: 0.57, Wins: 1293, Losses: 971, Ações: {0: 12714, 1: 13167, 2: 10873}, Steps: 36754, Time: 145.94s\n",
      "Rank 2: Episode 78, Total Reward: 7540.75, Win Rate: 0.58, Wins: 1085, Losses: 779, Ações: {0: 12011, 1: 12982, 2: 11761}, Steps: 36754, Time: 148.11s\n",
      "Rank 3: Episode 76, Total Reward: 5242.25, Win Rate: 0.56, Wins: 1110, Losses: 865, Ações: {0: 12337, 1: 12095, 2: 12322}, Steps: 36754, Time: 147.04s\n",
      "Rank 4: Episode 64, Total Reward: 4870.75, Win Rate: 0.56, Wins: 1164, Losses: 922, Ações: {0: 11454, 1: 14576, 2: 10724}, Steps: 36754, Time: 147.23s\n",
      "Rank 5: Episode 45, Total Reward: 4835.00, Win Rate: 0.54, Wins: 1192, Losses: 1010, Ações: {0: 13033, 1: 14309, 2: 9412}, Steps: 36754, Time: 127.96s\n",
      "Rank 6: Episode 44, Total Reward: 4435.25, Win Rate: 0.57, Wins: 1234, Losses: 915, Ações: {0: 12552, 1: 14123, 2: 10079}, Steps: 36754, Time: 128.51s\n",
      "Rank 7: Episode 80, Total Reward: 4396.25, Win Rate: 0.58, Wins: 1069, Losses: 789, Ações: {0: 13421, 1: 10957, 2: 12376}, Steps: 36754, Time: 147.75s\n",
      "Rank 8: Episode 66, Total Reward: 3643.75, Win Rate: 0.58, Wins: 1173, Losses: 866, Ações: {0: 10422, 1: 14277, 2: 12055}, Steps: 36754, Time: 147.00s\n",
      "Rank 9: Episode 98, Total Reward: 3519.25, Win Rate: 0.56, Wins: 984, Losses: 762, Ações: {0: 10643, 1: 14082, 2: 12029}, Steps: 36754, Time: 146.75s\n",
      "Rank 10: Episode 49, Total Reward: 3352.50, Win Rate: 0.56, Wins: 1267, Losses: 1015, Ações: {0: 12346, 1: 13768, 2: 10640}, Steps: 36754, Time: 146.42s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.6\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
