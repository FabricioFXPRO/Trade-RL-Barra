{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range', 'SMA4', 'SMA8', 'SMA12', 'SMA20', 'SMA50', 'SMA100', 'SMA200',\n",
    "    'StochasticoK', 'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram',\n",
    "    'atr8', 'atr14', 'atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass\n",
    "        else:\n",
    "            # Fechar posição se o gatilho não estiver ativo\n",
    "            if self.position == 1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 3: Criar o Agente Rainbow DQN usando PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Implementação da Camada Noisy Linear\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma_init = sigma_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.zeros(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.zeros(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return nn.functional.linear(x, weight, bias)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt())\n",
    "\n",
    "# Implementação da Rede Rainbow DQN\n",
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Advantage stream\n",
    "        self.advantage = nn.Sequential(\n",
    "            NoisyLinear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, n_actions)\n",
    "        )\n",
    "        # Value stream\n",
    "        self.value = nn.Sequential(\n",
    "            NoisyLinear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        # Dueling Q-values\n",
    "        q_values = value + advantage - advantage.mean()\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()\n",
    "\n",
    "# Implementação do Prioritized Replay Buffer\n",
    "class PrioritizedReplayBuffer(object):\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def push(self, *args):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((*args,))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (*args,)\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        batch = list(zip(*samples))\n",
    "\n",
    "        states = torch.cat(batch[0]).to(device)\n",
    "        actions = torch.tensor(batch[1], dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(batch[2], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.cat(batch[3]).to(device)\n",
    "        dones = torch.tensor(batch[4], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 4: Treinamento do Agente Rainbow DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 3514.50, Win Rate: 0.53, Wins: 218, Losses: 196, Steps: 36754, Time: 381.20s\n",
      "Ações: Manter=8003, Comprar=28751, Vender=0\n",
      "Ganhos Totais: 22800.50, Perdas Totais: -19182.50\n",
      "Modelo e log do episódio 1 salvos em: 4.17\\model_episode_1.pth e 4.17\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: 3105.00, Win Rate: 0.54, Wins: 198, Losses: 172, Steps: 36754, Time: 382.88s\n",
      "Ações: Manter=12697, Comprar=24057, Vender=0\n",
      "Ganhos Totais: 20210.75, Perdas Totais: -17013.00\n",
      "Modelo e log do episódio 2 salvos em: 4.17\\model_episode_2.pth e 4.17\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: 2766.25, Win Rate: 0.53, Wins: 198, Losses: 178, Steps: 36754, Time: 375.13s\n",
      "Ações: Manter=11223, Comprar=25531, Vender=0\n",
      "Ganhos Totais: 20449.50, Perdas Totais: -17589.25\n",
      "Modelo e log do episódio 3 salvos em: 4.17\\model_episode_3.pth e 4.17\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 2230.75, Win Rate: 0.52, Wins: 201, Losses: 185, Steps: 36754, Time: 374.51s\n",
      "Ações: Manter=10638, Comprar=26116, Vender=0\n",
      "Ganhos Totais: 20710.00, Perdas Totais: -18382.75\n",
      "Modelo e log do episódio 4 salvos em: 4.17\\model_episode_4.pth e 4.17\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: 2869.00, Win Rate: 0.52, Wins: 195, Losses: 182, Steps: 36754, Time: 380.71s\n",
      "Ações: Manter=11055, Comprar=25696, Vender=3\n",
      "Ganhos Totais: 20486.00, Perdas Totais: -17522.75\n",
      "Modelo e log do episódio 5 salvos em: 4.17\\model_episode_5.pth e 4.17\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 1979.75, Win Rate: 0.52, Wins: 200, Losses: 185, Steps: 36754, Time: 366.46s\n",
      "Ações: Manter=12248, Comprar=24261, Vender=245\n",
      "Ganhos Totais: 20488.75, Perdas Totais: -18412.75\n",
      "Modelo e log do episódio 6 salvos em: 4.17\\model_episode_6.pth e 4.17\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: 3038.00, Win Rate: 0.53, Wins: 225, Losses: 202, Steps: 36754, Time: 315.99s\n",
      "Ações: Manter=11932, Comprar=24215, Vender=607\n",
      "Ganhos Totais: 22833.75, Perdas Totais: -19688.75\n",
      "Modelo e log do episódio 7 salvos em: 4.17\\model_episode_7.pth e 4.17\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 4478.00, Win Rate: 0.53, Wins: 229, Losses: 200, Steps: 36754, Time: 306.98s\n",
      "Ações: Manter=10803, Comprar=25883, Vender=68\n",
      "Ganhos Totais: 23826.00, Perdas Totais: -19240.75\n",
      "Modelo e log do episódio 8 salvos em: 4.17\\model_episode_8.pth e 4.17\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: 5304.75, Win Rate: 0.54, Wins: 276, Losses: 232, Steps: 36754, Time: 308.87s\n",
      "Ações: Manter=10287, Comprar=25429, Vender=1038\n",
      "Ganhos Totais: 24786.25, Perdas Totais: -19354.50\n",
      "Modelo e log do episódio 9 salvos em: 4.17\\model_episode_9.pth e 4.17\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: 3241.50, Win Rate: 0.53, Wins: 344, Losses: 309, Steps: 36754, Time: 306.71s\n",
      "Ações: Manter=9024, Comprar=25193, Vender=2537\n",
      "Ganhos Totais: 25677.50, Perdas Totais: -22271.50\n",
      "Modelo e log do episódio 10 salvos em: 4.17\\model_episode_10.pth e 4.17\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 2712.50, Win Rate: 0.54, Wins: 545, Losses: 468, Steps: 36754, Time: 308.06s\n",
      "Ações: Manter=8817, Comprar=22008, Vender=5929\n",
      "Ganhos Totais: 28831.00, Perdas Totais: -25864.75\n",
      "Modelo e log do episódio 11 salvos em: 4.17\\model_episode_11.pth e 4.17\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 1215.00, Win Rate: 0.54, Wins: 591, Losses: 495, Steps: 36754, Time: 306.67s\n",
      "Ações: Manter=10097, Comprar=19495, Vender=7162\n",
      "Ganhos Totais: 27100.00, Perdas Totais: -25613.25\n",
      "Modelo e log do episódio 12 salvos em: 4.17\\model_episode_12.pth e 4.17\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 3094.25, Win Rate: 0.57, Wins: 635, Losses: 478, Steps: 36754, Time: 306.51s\n",
      "Ações: Manter=10648, Comprar=19612, Vender=6494\n",
      "Ganhos Totais: 28072.75, Perdas Totais: -24699.50\n",
      "Modelo e log do episódio 13 salvos em: 4.17\\model_episode_13.pth e 4.17\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 652.50, Win Rate: 0.54, Wins: 521, Losses: 442, Steps: 36754, Time: 307.20s\n",
      "Ações: Manter=14359, Comprar=14724, Vender=7671\n",
      "Ganhos Totais: 24346.25, Perdas Totais: -23453.00\n",
      "Modelo e log do episódio 14 salvos em: 4.17\\model_episode_14.pth e 4.17\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: 1209.25, Win Rate: 0.55, Wins: 566, Losses: 459, Steps: 36754, Time: 307.18s\n",
      "Ações: Manter=18339, Comprar=13287, Vender=5128\n",
      "Ganhos Totais: 24195.75, Perdas Totais: -22729.25\n",
      "Modelo e log do episódio 15 salvos em: 4.17\\model_episode_15.pth e 4.17\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: 2815.00, Win Rate: 0.56, Wins: 621, Losses: 495, Steps: 36754, Time: 307.30s\n",
      "Ações: Manter=16893, Comprar=13364, Vender=6497\n",
      "Ganhos Totais: 28247.00, Perdas Totais: -25152.50\n",
      "Modelo e log do episódio 16 salvos em: 4.17\\model_episode_16.pth e 4.17\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -683.25, Win Rate: 0.55, Wins: 541, Losses: 447, Steps: 36754, Time: 307.82s\n",
      "Ações: Manter=18777, Comprar=11872, Vender=6105\n",
      "Ganhos Totais: 23655.75, Perdas Totais: -24090.50\n",
      "Modelo e log do episódio 17 salvos em: 4.17\\model_episode_17.pth e 4.17\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: 2064.00, Win Rate: 0.53, Wins: 531, Losses: 473, Steps: 36754, Time: 308.07s\n",
      "Ações: Manter=19045, Comprar=10996, Vender=6713\n",
      "Ganhos Totais: 24544.75, Perdas Totais: -22228.25\n",
      "Modelo e log do episódio 18 salvos em: 4.17\\model_episode_18.pth e 4.17\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: 1772.25, Win Rate: 0.57, Wins: 585, Losses: 446, Steps: 36754, Time: 328.88s\n",
      "Ações: Manter=18569, Comprar=10877, Vender=7308\n",
      "Ganhos Totais: 24547.50, Perdas Totais: -22516.75\n",
      "Modelo e log do episódio 19 salvos em: 4.17\\model_episode_19.pth e 4.17\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 759.50, Win Rate: 0.54, Wins: 557, Losses: 466, Steps: 36754, Time: 356.46s\n",
      "Ações: Manter=18979, Comprar=10585, Vender=7190\n",
      "Ganhos Totais: 24123.00, Perdas Totais: -23106.75\n",
      "Modelo e log do episódio 20 salvos em: 4.17\\model_episode_20.pth e 4.17\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: 3764.00, Win Rate: 0.56, Wins: 558, Losses: 433, Steps: 36754, Time: 312.01s\n",
      "Ações: Manter=19637, Comprar=10139, Vender=6978\n",
      "Ganhos Totais: 25562.75, Perdas Totais: -21550.25\n",
      "Modelo e log do episódio 21 salvos em: 4.17\\model_episode_21.pth e 4.17\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: 979.50, Win Rate: 0.56, Wins: 533, Losses: 427, Steps: 36754, Time: 309.47s\n",
      "Ações: Manter=17749, Comprar=10384, Vender=8621\n",
      "Ganhos Totais: 25415.25, Perdas Totais: -24195.25\n",
      "Modelo e log do episódio 22 salvos em: 4.17\\model_episode_22.pth e 4.17\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: 2119.75, Win Rate: 0.53, Wins: 520, Losses: 453, Steps: 36754, Time: 310.30s\n",
      "Ações: Manter=15676, Comprar=12292, Vender=8786\n",
      "Ganhos Totais: 26589.75, Perdas Totais: -24225.50\n",
      "Modelo e log do episódio 23 salvos em: 4.17\\model_episode_23.pth e 4.17\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: 5243.25, Win Rate: 0.58, Wins: 566, Losses: 413, Steps: 36754, Time: 311.26s\n",
      "Ações: Manter=19685, Comprar=8204, Vender=8865\n",
      "Ganhos Totais: 27103.00, Perdas Totais: -21614.50\n",
      "Modelo e log do episódio 24 salvos em: 4.17\\model_episode_24.pth e 4.17\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -715.00, Win Rate: 0.53, Wins: 578, Losses: 514, Steps: 36754, Time: 309.97s\n",
      "Ações: Manter=19167, Comprar=8989, Vender=8598\n",
      "Ganhos Totais: 23228.00, Perdas Totais: -23669.00\n",
      "Modelo e log do episódio 25 salvos em: 4.17\\model_episode_25.pth e 4.17\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -709.50, Win Rate: 0.55, Wins: 503, Losses: 414, Steps: 36754, Time: 310.45s\n",
      "Ações: Manter=24412, Comprar=8604, Vender=3738\n",
      "Ganhos Totais: 22615.50, Perdas Totais: -23095.75\n",
      "Modelo e log do episódio 26 salvos em: 4.17\\model_episode_26.pth e 4.17\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: 1138.75, Win Rate: 0.52, Wins: 422, Losses: 391, Steps: 36754, Time: 310.23s\n",
      "Ações: Manter=23257, Comprar=5479, Vender=8018\n",
      "Ganhos Totais: 23903.75, Perdas Totais: -22561.25\n",
      "Modelo e log do episódio 27 salvos em: 4.17\\model_episode_27.pth e 4.17\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: 642.75, Win Rate: 0.54, Wins: 509, Losses: 430, Steps: 36754, Time: 310.32s\n",
      "Ações: Manter=22858, Comprar=5549, Vender=8347\n",
      "Ganhos Totais: 24525.50, Perdas Totais: -23646.75\n",
      "Modelo e log do episódio 28 salvos em: 4.17\\model_episode_28.pth e 4.17\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 2380.00, Win Rate: 0.58, Wins: 537, Losses: 396, Steps: 36754, Time: 310.28s\n",
      "Ações: Manter=24275, Comprar=8311, Vender=4168\n",
      "Ganhos Totais: 23994.00, Perdas Totais: -21380.50\n",
      "Modelo e log do episódio 29 salvos em: 4.17\\model_episode_29.pth e 4.17\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: 2050.50, Win Rate: 0.53, Wins: 498, Losses: 433, Steps: 36754, Time: 309.76s\n",
      "Ações: Manter=18632, Comprar=10394, Vender=7728\n",
      "Ganhos Totais: 24466.00, Perdas Totais: -22182.00\n",
      "Modelo e log do episódio 30 salvos em: 4.17\\model_episode_30.pth e 4.17\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: 2568.25, Win Rate: 0.54, Wins: 652, Losses: 559, Steps: 36754, Time: 310.22s\n",
      "Ações: Manter=17691, Comprar=9039, Vender=10024\n",
      "Ganhos Totais: 26740.75, Perdas Totais: -23868.75\n",
      "Modelo e log do episódio 31 salvos em: 4.17\\model_episode_31.pth e 4.17\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 3433.75, Win Rate: 0.54, Wins: 847, Losses: 719, Steps: 36754, Time: 309.54s\n",
      "Ações: Manter=14812, Comprar=17393, Vender=4549\n",
      "Ganhos Totais: 31592.00, Perdas Totais: -27764.25\n",
      "Modelo e log do episódio 32 salvos em: 4.17\\model_episode_32.pth e 4.17\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 3419.25, Win Rate: 0.55, Wins: 653, Losses: 526, Steps: 36754, Time: 309.66s\n",
      "Ações: Manter=19034, Comprar=13154, Vender=4566\n",
      "Ganhos Totais: 26366.50, Perdas Totais: -22651.50\n",
      "Modelo e log do episódio 33 salvos em: 4.17\\model_episode_33.pth e 4.17\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: 4237.75, Win Rate: 0.58, Wins: 669, Losses: 492, Steps: 36754, Time: 309.77s\n",
      "Ações: Manter=21074, Comprar=9778, Vender=5902\n",
      "Ganhos Totais: 25459.75, Perdas Totais: -20930.00\n",
      "Modelo e log do episódio 34 salvos em: 4.17\\model_episode_34.pth e 4.17\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: 4064.25, Win Rate: 0.56, Wins: 565, Losses: 450, Steps: 36754, Time: 309.02s\n",
      "Ações: Manter=14513, Comprar=11556, Vender=10685\n",
      "Ganhos Totais: 28192.00, Perdas Totais: -23873.25\n",
      "Modelo e log do episódio 35 salvos em: 4.17\\model_episode_35.pth e 4.17\\log_episode_35.csv\n",
      "\n",
      "Episode 36/100, Total Reward: 5228.25, Win Rate: 0.56, Wins: 583, Losses: 456, Steps: 36754, Time: 308.97s\n",
      "Ações: Manter=17787, Comprar=15070, Vender=3897\n",
      "Ganhos Totais: 28220.75, Perdas Totais: -22732.00\n",
      "Modelo e log do episódio 36 salvos em: 4.17\\model_episode_36.pth e 4.17\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: 3295.75, Win Rate: 0.56, Wins: 712, Losses: 554, Steps: 36754, Time: 310.15s\n",
      "Ações: Manter=16328, Comprar=15078, Vender=5348\n",
      "Ganhos Totais: 27749.50, Perdas Totais: -24135.75\n",
      "Modelo e log do episódio 37 salvos em: 4.17\\model_episode_37.pth e 4.17\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: 2460.00, Win Rate: 0.56, Wins: 552, Losses: 442, Steps: 36754, Time: 310.23s\n",
      "Ações: Manter=13002, Comprar=15873, Vender=7879\n",
      "Ganhos Totais: 28970.25, Perdas Totais: -26261.25\n",
      "Modelo e log do episódio 38 salvos em: 4.17\\model_episode_38.pth e 4.17\\log_episode_38.csv\n",
      "\n",
      "Episode 39/100, Total Reward: -1710.00, Win Rate: 0.54, Wins: 488, Losses: 417, Steps: 36754, Time: 309.57s\n",
      "Ações: Manter=14166, Comprar=15401, Vender=7187\n",
      "Ganhos Totais: 24394.25, Perdas Totais: -25877.50\n",
      "Modelo e log do episódio 39 salvos em: 4.17\\model_episode_39.pth e 4.17\\log_episode_39.csv\n",
      "\n",
      "Episode 40/100, Total Reward: 1442.50, Win Rate: 0.53, Wins: 570, Losses: 504, Steps: 36754, Time: 310.03s\n",
      "Ações: Manter=14940, Comprar=13245, Vender=8569\n",
      "Ganhos Totais: 27781.00, Perdas Totais: -26068.50\n",
      "Modelo e log do episódio 40 salvos em: 4.17\\model_episode_40.pth e 4.17\\log_episode_40.csv\n",
      "\n",
      "Episode 41/100, Total Reward: 2577.00, Win Rate: 0.54, Wins: 619, Losses: 538, Steps: 36754, Time: 309.90s\n",
      "Ações: Manter=15521, Comprar=12490, Vender=8743\n",
      "Ganhos Totais: 27712.75, Perdas Totais: -24845.50\n",
      "Modelo e log do episódio 41 salvos em: 4.17\\model_episode_41.pth e 4.17\\log_episode_41.csv\n",
      "\n",
      "Episode 42/100, Total Reward: 3315.25, Win Rate: 0.54, Wins: 686, Losses: 585, Steps: 36754, Time: 310.08s\n",
      "Ações: Manter=16506, Comprar=13897, Vender=6351\n",
      "Ganhos Totais: 27514.25, Perdas Totais: -23879.00\n",
      "Modelo e log do episódio 42 salvos em: 4.17\\model_episode_42.pth e 4.17\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: 6375.50, Win Rate: 0.54, Wins: 790, Losses: 676, Steps: 36754, Time: 310.07s\n",
      "Ações: Manter=16457, Comprar=16221, Vender=4076\n",
      "Ganhos Totais: 30660.50, Perdas Totais: -23916.50\n",
      "Modelo e log do episódio 43 salvos em: 4.17\\model_episode_43.pth e 4.17\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -1203.00, Win Rate: 0.56, Wins: 528, Losses: 416, Steps: 36754, Time: 309.69s\n",
      "Ações: Manter=19592, Comprar=9134, Vender=8028\n",
      "Ganhos Totais: 24201.50, Perdas Totais: -25167.50\n",
      "Modelo e log do episódio 44 salvos em: 4.17\\model_episode_44.pth e 4.17\\log_episode_44.csv\n",
      "\n",
      "Episode 45/100, Total Reward: 1953.50, Win Rate: 0.53, Wins: 823, Losses: 734, Steps: 36754, Time: 310.48s\n",
      "Ações: Manter=14979, Comprar=16014, Vender=5761\n",
      "Ganhos Totais: 31147.00, Perdas Totais: -28801.25\n",
      "Modelo e log do episódio 45 salvos em: 4.17\\model_episode_45.pth e 4.17\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: 6747.75, Win Rate: 0.57, Wins: 786, Losses: 590, Steps: 36754, Time: 310.00s\n",
      "Ações: Manter=9395, Comprar=16520, Vender=10839\n",
      "Ganhos Totais: 31860.75, Perdas Totais: -24767.00\n",
      "Modelo e log do episódio 46 salvos em: 4.17\\model_episode_46.pth e 4.17\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: 4258.75, Win Rate: 0.56, Wins: 654, Losses: 524, Steps: 36754, Time: 311.06s\n",
      "Ações: Manter=11513, Comprar=18388, Vender=6853\n",
      "Ganhos Totais: 29676.75, Perdas Totais: -25123.25\n",
      "Modelo e log do episódio 47 salvos em: 4.17\\model_episode_47.pth e 4.17\\log_episode_47.csv\n",
      "\n",
      "Episode 48/100, Total Reward: 1022.75, Win Rate: 0.54, Wins: 547, Losses: 466, Steps: 36754, Time: 310.45s\n",
      "Ações: Manter=10158, Comprar=19608, Vender=6988\n",
      "Ganhos Totais: 28255.75, Perdas Totais: -26979.25\n",
      "Modelo e log do episódio 48 salvos em: 4.17\\model_episode_48.pth e 4.17\\log_episode_48.csv\n",
      "\n",
      "Episode 49/100, Total Reward: 5641.25, Win Rate: 0.54, Wins: 664, Losses: 555, Steps: 36754, Time: 309.74s\n",
      "Ações: Manter=10762, Comprar=20140, Vender=5852\n",
      "Ganhos Totais: 31866.75, Perdas Totais: -25919.75\n",
      "Modelo e log do episódio 49 salvos em: 4.17\\model_episode_49.pth e 4.17\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: 4108.00, Win Rate: 0.56, Wins: 735, Losses: 576, Steps: 36754, Time: 309.29s\n",
      "Ações: Manter=11101, Comprar=16747, Vender=8906\n",
      "Ganhos Totais: 30736.75, Perdas Totais: -26299.75\n",
      "Modelo e log do episódio 50 salvos em: 4.17\\model_episode_50.pth e 4.17\\log_episode_50.csv\n",
      "\n",
      "Episode 51/100, Total Reward: 2468.75, Win Rate: 0.58, Wins: 676, Losses: 493, Steps: 36754, Time: 309.62s\n",
      "Ações: Manter=15913, Comprar=15121, Vender=5720\n",
      "Ganhos Totais: 28826.00, Perdas Totais: -26063.25\n",
      "Modelo e log do episódio 51 salvos em: 4.17\\model_episode_51.pth e 4.17\\log_episode_51.csv\n",
      "\n",
      "Episode 52/100, Total Reward: 7697.00, Win Rate: 0.58, Wins: 605, Losses: 437, Steps: 36754, Time: 309.48s\n",
      "Ações: Manter=16220, Comprar=13562, Vender=6972\n",
      "Ganhos Totais: 30786.75, Perdas Totais: -22828.75\n",
      "Modelo e log do episódio 52 salvos em: 4.17\\model_episode_52.pth e 4.17\\log_episode_52.csv\n",
      "\n",
      "Episode 53/100, Total Reward: 8231.75, Win Rate: 0.58, Wins: 529, Losses: 390, Steps: 36754, Time: 310.19s\n",
      "Ações: Manter=16523, Comprar=14778, Vender=5453\n",
      "Ganhos Totais: 30574.25, Perdas Totais: -22112.75\n",
      "Modelo e log do episódio 53 salvos em: 4.17\\model_episode_53.pth e 4.17\\log_episode_53.csv\n",
      "\n",
      "Episode 54/100, Total Reward: 6831.00, Win Rate: 0.59, Wins: 844, Losses: 590, Steps: 36754, Time: 310.39s\n",
      "Ações: Manter=11654, Comprar=14520, Vender=10580\n",
      "Ganhos Totais: 33150.25, Perdas Totais: -25959.25\n",
      "Modelo e log do episódio 54 salvos em: 4.17\\model_episode_54.pth e 4.17\\log_episode_54.csv\n",
      "\n",
      "Episode 55/100, Total Reward: 4660.00, Win Rate: 0.58, Wins: 634, Losses: 457, Steps: 36754, Time: 309.82s\n",
      "Ações: Manter=19874, Comprar=8271, Vender=8609\n",
      "Ganhos Totais: 28174.75, Perdas Totais: -23241.50\n",
      "Modelo e log do episódio 55 salvos em: 4.17\\model_episode_55.pth e 4.17\\log_episode_55.csv\n",
      "\n",
      "Episode 56/100, Total Reward: 1893.75, Win Rate: 0.54, Wins: 595, Losses: 502, Steps: 36754, Time: 309.39s\n",
      "Ações: Manter=16034, Comprar=6823, Vender=13897\n",
      "Ganhos Totais: 28995.25, Perdas Totais: -26826.50\n",
      "Modelo e log do episódio 56 salvos em: 4.17\\model_episode_56.pth e 4.17\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: 6726.25, Win Rate: 0.59, Wins: 992, Losses: 702, Steps: 36754, Time: 310.26s\n",
      "Ações: Manter=17553, Comprar=8352, Vender=10849\n",
      "Ganhos Totais: 34058.00, Perdas Totais: -26905.50\n",
      "Modelo e log do episódio 57 salvos em: 4.17\\model_episode_57.pth e 4.17\\log_episode_57.csv\n",
      "\n",
      "Episode 58/100, Total Reward: 4460.25, Win Rate: 0.57, Wins: 731, Losses: 541, Steps: 36754, Time: 309.82s\n",
      "Ações: Manter=17638, Comprar=8171, Vender=10945\n",
      "Ganhos Totais: 31634.00, Perdas Totais: -26854.75\n",
      "Modelo e log do episódio 58 salvos em: 4.17\\model_episode_58.pth e 4.17\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: 2019.50, Win Rate: 0.56, Wins: 570, Losses: 442, Steps: 36754, Time: 310.09s\n",
      "Ações: Manter=21492, Comprar=7849, Vender=7413\n",
      "Ganhos Totais: 27741.75, Perdas Totais: -25469.00\n",
      "Modelo e log do episódio 59 salvos em: 4.17\\model_episode_59.pth e 4.17\\log_episode_59.csv\n",
      "\n",
      "Episode 60/100, Total Reward: 4902.00, Win Rate: 0.59, Wins: 748, Losses: 518, Steps: 36754, Time: 309.93s\n",
      "Ações: Manter=16595, Comprar=15017, Vender=5142\n",
      "Ganhos Totais: 30919.25, Perdas Totais: -25700.25\n",
      "Modelo e log do episódio 60 salvos em: 4.17\\model_episode_60.pth e 4.17\\log_episode_60.csv\n",
      "\n",
      "Episode 61/100, Total Reward: 3175.00, Win Rate: 0.59, Wins: 794, Losses: 561, Steps: 36754, Time: 310.35s\n",
      "Ações: Manter=17452, Comprar=11964, Vender=7338\n",
      "Ganhos Totais: 30222.00, Perdas Totais: -26707.00\n",
      "Modelo e log do episódio 61 salvos em: 4.17\\model_episode_61.pth e 4.17\\log_episode_61.csv\n",
      "\n",
      "Episode 62/100, Total Reward: 1592.75, Win Rate: 0.59, Wins: 920, Losses: 643, Steps: 36754, Time: 310.51s\n",
      "Ações: Manter=14821, Comprar=15415, Vender=6518\n",
      "Ganhos Totais: 30597.25, Perdas Totais: -28612.25\n",
      "Modelo e log do episódio 62 salvos em: 4.17\\model_episode_62.pth e 4.17\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: 6548.00, Win Rate: 0.59, Wins: 939, Losses: 660, Steps: 36754, Time: 310.34s\n",
      "Ações: Manter=16833, Comprar=8183, Vender=11738\n",
      "Ganhos Totais: 33876.75, Perdas Totais: -26928.00\n",
      "Modelo e log do episódio 63 salvos em: 4.17\\model_episode_63.pth e 4.17\\log_episode_63.csv\n",
      "\n",
      "Episode 64/100, Total Reward: 5254.00, Win Rate: 0.58, Wins: 719, Losses: 525, Steps: 36754, Time: 309.83s\n",
      "Ações: Manter=18065, Comprar=8763, Vender=9926\n",
      "Ganhos Totais: 29908.00, Perdas Totais: -24342.25\n",
      "Modelo e log do episódio 64 salvos em: 4.17\\model_episode_64.pth e 4.17\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: 6162.75, Win Rate: 0.59, Wins: 767, Losses: 529, Steps: 36754, Time: 309.72s\n",
      "Ações: Manter=15599, Comprar=13424, Vender=7731\n",
      "Ganhos Totais: 33127.25, Perdas Totais: -26639.25\n",
      "Modelo e log do episódio 65 salvos em: 4.17\\model_episode_65.pth e 4.17\\log_episode_65.csv\n",
      "\n",
      "Episode 66/100, Total Reward: 4057.00, Win Rate: 0.57, Wins: 815, Losses: 615, Steps: 36754, Time: 310.21s\n",
      "Ações: Manter=15687, Comprar=13972, Vender=7095\n",
      "Ganhos Totais: 31707.25, Perdas Totais: -27291.50\n",
      "Modelo e log do episódio 66 salvos em: 4.17\\model_episode_66.pth e 4.17\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: -561.50, Win Rate: 0.58, Wins: 655, Losses: 472, Steps: 36754, Time: 309.60s\n",
      "Ações: Manter=19864, Comprar=8735, Vender=8155\n",
      "Ganhos Totais: 25071.00, Perdas Totais: -25350.25\n",
      "Modelo e log do episódio 67 salvos em: 4.17\\model_episode_67.pth e 4.17\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: 4612.75, Win Rate: 0.59, Wins: 699, Losses: 481, Steps: 36754, Time: 309.62s\n",
      "Ações: Manter=15725, Comprar=13382, Vender=7647\n",
      "Ganhos Totais: 30903.50, Perdas Totais: -25995.50\n",
      "Modelo e log do episódio 68 salvos em: 4.17\\model_episode_68.pth e 4.17\\log_episode_68.csv\n",
      "\n",
      "Episode 69/100, Total Reward: 3567.75, Win Rate: 0.59, Wins: 683, Losses: 466, Steps: 36754, Time: 309.96s\n",
      "Ações: Manter=15969, Comprar=9157, Vender=11628\n",
      "Ganhos Totais: 29915.75, Perdas Totais: -26059.00\n",
      "Modelo e log do episódio 69 salvos em: 4.17\\model_episode_69.pth e 4.17\\log_episode_69.csv\n",
      "\n",
      "Episode 70/100, Total Reward: 5358.75, Win Rate: 0.56, Wins: 666, Losses: 513, Steps: 36754, Time: 310.05s\n",
      "Ações: Manter=13987, Comprar=15647, Vender=7120\n",
      "Ganhos Totais: 30883.75, Perdas Totais: -25229.00\n",
      "Modelo e log do episódio 70 salvos em: 4.17\\model_episode_70.pth e 4.17\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: 3859.00, Win Rate: 0.57, Wins: 581, Losses: 443, Steps: 36754, Time: 309.30s\n",
      "Ações: Manter=13590, Comprar=12962, Vender=10202\n",
      "Ganhos Totais: 29965.75, Perdas Totais: -25850.00\n",
      "Modelo e log do episódio 71 salvos em: 4.17\\model_episode_71.pth e 4.17\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: 5048.25, Win Rate: 0.58, Wins: 732, Losses: 541, Steps: 36754, Time: 309.77s\n",
      "Ações: Manter=14056, Comprar=15059, Vender=7639\n",
      "Ganhos Totais: 31477.75, Perdas Totais: -26110.75\n",
      "Modelo e log do episódio 72 salvos em: 4.17\\model_episode_72.pth e 4.17\\log_episode_72.csv\n",
      "\n",
      "Episode 73/100, Total Reward: 5348.75, Win Rate: 0.57, Wins: 535, Losses: 405, Steps: 36754, Time: 309.68s\n",
      "Ações: Manter=14589, Comprar=16257, Vender=5908\n",
      "Ganhos Totais: 29284.25, Perdas Totais: -23700.00\n",
      "Modelo e log do episódio 73 salvos em: 4.17\\model_episode_73.pth e 4.17\\log_episode_73.csv\n",
      "\n",
      "Episode 74/100, Total Reward: 4071.75, Win Rate: 0.59, Wins: 817, Losses: 562, Steps: 36754, Time: 309.95s\n",
      "Ações: Manter=13582, Comprar=11961, Vender=11211\n",
      "Ganhos Totais: 31675.75, Perdas Totais: -27258.25\n",
      "Modelo e log do episódio 74 salvos em: 4.17\\model_episode_74.pth e 4.17\\log_episode_74.csv\n",
      "\n",
      "Episode 75/100, Total Reward: 3272.00, Win Rate: 0.60, Wins: 948, Losses: 635, Steps: 36754, Time: 310.08s\n",
      "Ações: Manter=14771, Comprar=13048, Vender=8935\n",
      "Ganhos Totais: 32453.25, Perdas Totais: -28784.50\n",
      "Modelo e log do episódio 75 salvos em: 4.17\\model_episode_75.pth e 4.17\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: 3705.75, Win Rate: 0.58, Wins: 708, Losses: 521, Steps: 36754, Time: 310.34s\n",
      "Ações: Manter=15433, Comprar=16699, Vender=4622\n",
      "Ganhos Totais: 30969.75, Perdas Totais: -26956.00\n",
      "Modelo e log do episódio 76 salvos em: 4.17\\model_episode_76.pth e 4.17\\log_episode_76.csv\n",
      "\n",
      "Episode 77/100, Total Reward: 565.25, Win Rate: 0.53, Wins: 862, Losses: 754, Steps: 36754, Time: 309.57s\n",
      "Ações: Manter=14296, Comprar=17183, Vender=5275\n",
      "Ganhos Totais: 30419.25, Perdas Totais: -29448.25\n",
      "Modelo e log do episódio 77 salvos em: 4.17\\model_episode_77.pth e 4.17\\log_episode_77.csv\n",
      "\n",
      "Episode 78/100, Total Reward: 5167.00, Win Rate: 0.60, Wins: 620, Losses: 417, Steps: 36754, Time: 309.10s\n",
      "Ações: Manter=13594, Comprar=17597, Vender=5563\n",
      "Ganhos Totais: 29730.25, Perdas Totais: -24303.75\n",
      "Modelo e log do episódio 78 salvos em: 4.17\\model_episode_78.pth e 4.17\\log_episode_78.csv\n",
      "\n",
      "Episode 79/100, Total Reward: 5590.75, Win Rate: 0.57, Wins: 645, Losses: 484, Steps: 36754, Time: 309.68s\n",
      "Ações: Manter=19647, Comprar=11599, Vender=5508\n",
      "Ganhos Totais: 30049.25, Perdas Totais: -24176.00\n",
      "Modelo e log do episódio 79 salvos em: 4.17\\model_episode_79.pth e 4.17\\log_episode_79.csv\n",
      "\n",
      "Episode 80/100, Total Reward: 1262.25, Win Rate: 0.57, Wins: 561, Losses: 416, Steps: 36754, Time: 309.73s\n",
      "Ações: Manter=15531, Comprar=15967, Vender=5256\n",
      "Ganhos Totais: 27060.75, Perdas Totais: -25554.25\n",
      "Modelo e log do episódio 80 salvos em: 4.17\\model_episode_80.pth e 4.17\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: 4256.00, Win Rate: 0.57, Wins: 788, Losses: 602, Steps: 36754, Time: 310.04s\n",
      "Ações: Manter=8326, Comprar=23688, Vender=4740\n",
      "Ganhos Totais: 32057.00, Perdas Totais: -27452.00\n",
      "Modelo e log do episódio 81 salvos em: 4.17\\model_episode_81.pth e 4.17\\log_episode_81.csv\n",
      "\n",
      "Episode 82/100, Total Reward: 2052.00, Win Rate: 0.54, Wins: 842, Losses: 716, Steps: 36754, Time: 310.28s\n",
      "Ações: Manter=16972, Comprar=8805, Vender=10977\n",
      "Ganhos Totais: 29072.25, Perdas Totais: -26627.50\n",
      "Modelo e log do episódio 82 salvos em: 4.17\\model_episode_82.pth e 4.17\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: -293.00, Win Rate: 0.59, Wins: 615, Losses: 422, Steps: 36754, Time: 310.30s\n",
      "Ações: Manter=18961, Comprar=9987, Vender=7806\n",
      "Ganhos Totais: 26346.50, Perdas Totais: -26379.75\n",
      "Modelo e log do episódio 83 salvos em: 4.17\\model_episode_83.pth e 4.17\\log_episode_83.csv\n",
      "\n",
      "Episode 84/100, Total Reward: 6491.25, Win Rate: 0.57, Wins: 774, Losses: 580, Steps: 36754, Time: 310.48s\n",
      "Ações: Manter=10484, Comprar=19393, Vender=6877\n",
      "Ganhos Totais: 33251.00, Perdas Totais: -26420.00\n",
      "Modelo e log do episódio 84 salvos em: 4.17\\model_episode_84.pth e 4.17\\log_episode_84.csv\n",
      "\n",
      "Episode 85/100, Total Reward: 6836.25, Win Rate: 0.61, Wins: 865, Losses: 563, Steps: 36754, Time: 309.94s\n",
      "Ações: Manter=9389, Comprar=24370, Vender=2995\n",
      "Ganhos Totais: 33328.75, Perdas Totais: -26133.50\n",
      "Modelo e log do episódio 85 salvos em: 4.17\\model_episode_85.pth e 4.17\\log_episode_85.csv\n",
      "\n",
      "Episode 86/100, Total Reward: 5445.00, Win Rate: 0.57, Wins: 771, Losses: 581, Steps: 36754, Time: 310.92s\n",
      "Ações: Manter=11086, Comprar=19257, Vender=6411\n",
      "Ganhos Totais: 31926.25, Perdas Totais: -26141.00\n",
      "Modelo e log do episódio 86 salvos em: 4.17\\model_episode_86.pth e 4.17\\log_episode_86.csv\n",
      "\n",
      "Episode 87/100, Total Reward: 6336.75, Win Rate: 0.60, Wins: 597, Losses: 399, Steps: 36754, Time: 310.67s\n",
      "Ações: Manter=20413, Comprar=13660, Vender=2681\n",
      "Ganhos Totais: 26936.50, Perdas Totais: -20350.00\n",
      "Modelo e log do episódio 87 salvos em: 4.17\\model_episode_87.pth e 4.17\\log_episode_87.csv\n",
      "\n",
      "Episode 88/100, Total Reward: 3086.75, Win Rate: 0.57, Wins: 564, Losses: 433, Steps: 36754, Time: 309.81s\n",
      "Ações: Manter=10003, Comprar=20518, Vender=6233\n",
      "Ganhos Totais: 27947.25, Perdas Totais: -24611.00\n",
      "Modelo e log do episódio 88 salvos em: 4.17\\model_episode_88.pth e 4.17\\log_episode_88.csv\n",
      "\n",
      "Episode 89/100, Total Reward: -1206.75, Win Rate: 0.56, Wins: 661, Losses: 511, Steps: 36754, Time: 310.74s\n",
      "Ações: Manter=12501, Comprar=12715, Vender=11538\n",
      "Ganhos Totais: 27833.00, Perdas Totais: -28746.50\n",
      "Modelo e log do episódio 89 salvos em: 4.17\\model_episode_89.pth e 4.17\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: 2302.50, Win Rate: 0.56, Wins: 471, Losses: 366, Steps: 36754, Time: 309.95s\n",
      "Ações: Manter=19702, Comprar=9719, Vender=7333\n",
      "Ganhos Totais: 24901.75, Perdas Totais: -22389.50\n",
      "Modelo e log do episódio 90 salvos em: 4.17\\model_episode_90.pth e 4.17\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: 6024.00, Win Rate: 0.59, Wins: 599, Losses: 411, Steps: 36754, Time: 310.15s\n",
      "Ações: Manter=14211, Comprar=17858, Vender=4685\n",
      "Ganhos Totais: 29615.75, Perdas Totais: -23338.75\n",
      "Modelo e log do episódio 91 salvos em: 4.17\\model_episode_91.pth e 4.17\\log_episode_91.csv\n",
      "\n",
      "Episode 92/100, Total Reward: 8311.00, Win Rate: 0.60, Wins: 572, Losses: 377, Steps: 36754, Time: 311.01s\n",
      "Ações: Manter=15426, Comprar=15097, Vender=6231\n",
      "Ganhos Totais: 30288.00, Perdas Totais: -21739.50\n",
      "Modelo e log do episódio 92 salvos em: 4.17\\model_episode_92.pth e 4.17\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: 1962.75, Win Rate: 0.60, Wins: 611, Losses: 399, Steps: 36754, Time: 311.16s\n",
      "Ações: Manter=9375, Comprar=23602, Vender=3777\n",
      "Ganhos Totais: 27860.25, Perdas Totais: -25644.50\n",
      "Modelo e log do episódio 93 salvos em: 4.17\\model_episode_93.pth e 4.17\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: 3087.00, Win Rate: 0.56, Wins: 748, Losses: 577, Steps: 36754, Time: 310.44s\n",
      "Ações: Manter=5023, Comprar=25019, Vender=6712\n",
      "Ganhos Totais: 32221.00, Perdas Totais: -28802.50\n",
      "Modelo e log do episódio 94 salvos em: 4.17\\model_episode_94.pth e 4.17\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: 5772.75, Win Rate: 0.58, Wins: 689, Losses: 508, Steps: 36754, Time: 310.45s\n",
      "Ações: Manter=6703, Comprar=22520, Vender=7531\n",
      "Ganhos Totais: 32648.00, Perdas Totais: -26575.25\n",
      "Modelo e log do episódio 95 salvos em: 4.17\\model_episode_95.pth e 4.17\\log_episode_95.csv\n",
      "\n",
      "Episode 96/100, Total Reward: 6413.00, Win Rate: 0.57, Wins: 595, Losses: 447, Steps: 36754, Time: 310.12s\n",
      "Ações: Manter=4298, Comprar=23443, Vender=9013\n",
      "Ganhos Totais: 31076.25, Perdas Totais: -24402.00\n",
      "Modelo e log do episódio 96 salvos em: 4.17\\model_episode_96.pth e 4.17\\log_episode_96.csv\n",
      "\n",
      "Episode 97/100, Total Reward: 8438.50, Win Rate: 0.59, Wins: 596, Losses: 413, Steps: 36754, Time: 310.69s\n",
      "Ações: Manter=5413, Comprar=23996, Vender=7345\n",
      "Ganhos Totais: 31979.50, Perdas Totais: -23288.50\n",
      "Modelo e log do episódio 97 salvos em: 4.17\\model_episode_97.pth e 4.17\\log_episode_97.csv\n",
      "\n",
      "Episode 98/100, Total Reward: -90.75, Win Rate: 0.57, Wins: 601, Losses: 452, Steps: 36754, Time: 310.72s\n",
      "Ações: Manter=9369, Comprar=24453, Vender=2932\n",
      "Ganhos Totais: 27853.00, Perdas Totais: -27679.00\n",
      "Modelo e log do episódio 98 salvos em: 4.17\\model_episode_98.pth e 4.17\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: 2589.25, Win Rate: 0.55, Wins: 771, Losses: 623, Steps: 36754, Time: 310.47s\n",
      "Ações: Manter=13533, Comprar=10783, Vender=12438\n",
      "Ganhos Totais: 32235.75, Perdas Totais: -29297.75\n",
      "Modelo e log do episódio 99 salvos em: 4.17\\model_episode_99.pth e 4.17\\log_episode_99.csv\n",
      "\n",
      "Episode 100/100, Total Reward: 2129.75, Win Rate: 0.56, Wins: 736, Losses: 572, Steps: 36754, Time: 310.57s\n",
      "Ações: Manter=6672, Comprar=15905, Vender=14177\n",
      "Ganhos Totais: 29766.75, Perdas Totais: -27308.50\n",
      "Modelo e log do episódio 100 salvos em: 4.17\\model_episode_100.pth e 4.17\\log_episode_100.csv\n",
      "\n",
      "\n",
      "Treinamento finalizado.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configurações do treinamento\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "memory_size = 10000\n",
    "target_update = 1000\n",
    "beta_start = 0.4\n",
    "beta_frames = num_episodes * len(data)\n",
    "alpha = 0.6\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Inicializar o Prioritized Replay Buffer\n",
    "replay_buffer = PrioritizedReplayBuffer(memory_size, alpha=alpha)\n",
    "\n",
    "# Inicializar a lista de melhores episódios\n",
    "best_episodes = []\n",
    "\n",
    "# Função para selecionar ação usando Noisy Nets\n",
    "def select_action(state):\n",
    "    with torch.no_grad():\n",
    "        q_values = q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "save_dir = \"4.17\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "beta = beta_start\n",
    "frame_idx = 0  # Contador de frames para ajustar beta\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        replay_buffer.push(obs, action, reward, obs_next, done)\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Resetar ruído das Noisy Nets\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(replay_buffer.buffer) >= batch_size:\n",
    "            beta = min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "            states, actions_batch, rewards_batch, next_states, dones, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando Double DQN\n",
    "            with torch.no_grad():\n",
    "                next_actions = q_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q_values = target_net(next_states).gather(1, next_actions)\n",
    "                target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular o erro para Prioritized Replay\n",
    "            td_errors = (q_values - target_q_values).detach().cpu().numpy().flatten()\n",
    "            new_priorities = np.abs(td_errors) + 1e-6\n",
    "            replay_buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "            # Calcular a perda ponderada\n",
    "            loss = (weights * nn.functional.mse_loss(q_values, target_q_values, reduction='none')).mean()\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Atualizar a rede alvo\n",
    "            if frame_idx % target_update == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista dos melhores e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log do episódio se for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "# Exibir os top 10 episódios ao final do treinamento\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
