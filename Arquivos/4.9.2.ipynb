{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -299.75, Win Rate: 0.49, Wins: 394, Losses: 414, Epsilon: 0.4950, Steps: 36754, Time: 156.92s\n",
      "Ações: Manter=11436, Comprar=12653, Vender=12665\n",
      "Ganhos Totais: 8363.50, Perdas Totais: -8663.25\n",
      "Modelo e log do episódio 1 salvos em: 4.9.2\\model_episode_1.pth e 4.9.2\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: 156.25, Win Rate: 0.53, Wins: 472, Losses: 419, Epsilon: 0.4900, Steps: 36754, Time: 159.02s\n",
      "Ações: Manter=11559, Comprar=12406, Vender=12789\n",
      "Ganhos Totais: 9891.25, Perdas Totais: -9735.00\n",
      "Modelo e log do episódio 2 salvos em: 4.9.2\\model_episode_2.pth e 4.9.2\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: 35.50, Win Rate: 0.54, Wins: 486, Losses: 414, Epsilon: 0.4851, Steps: 36754, Time: 185.42s\n",
      "Ações: Manter=11561, Comprar=12076, Vender=13117\n",
      "Ganhos Totais: 9291.25, Perdas Totais: -9255.75\n",
      "Modelo e log do episódio 3 salvos em: 4.9.2\\model_episode_3.pth e 4.9.2\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 645.50, Win Rate: 0.52, Wins: 445, Losses: 418, Epsilon: 0.4803, Steps: 36754, Time: 185.96s\n",
      "Ações: Manter=12436, Comprar=11731, Vender=12587\n",
      "Ganhos Totais: 10049.75, Perdas Totais: -9404.25\n",
      "Modelo e log do episódio 4 salvos em: 4.9.2\\model_episode_4.pth e 4.9.2\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -227.75, Win Rate: 0.54, Wins: 485, Losses: 419, Epsilon: 0.4755, Steps: 36754, Time: 201.85s\n",
      "Ações: Manter=13960, Comprar=11048, Vender=11746\n",
      "Ganhos Totais: 10201.50, Perdas Totais: -10429.25\n",
      "Modelo e log do episódio 5 salvos em: 4.9.2\\model_episode_5.pth e 4.9.2\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -1122.25, Win Rate: 0.51, Wins: 438, Losses: 414, Epsilon: 0.4707, Steps: 36754, Time: 179.41s\n",
      "Ações: Manter=13781, Comprar=11251, Vender=11722\n",
      "Ganhos Totais: 8980.25, Perdas Totais: -10102.50\n",
      "Modelo e log do episódio 6 salvos em: 4.9.2\\model_episode_6.pth e 4.9.2\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -1261.25, Win Rate: 0.48, Wins: 388, Losses: 419, Epsilon: 0.4660, Steps: 36754, Time: 175.30s\n",
      "Ações: Manter=11734, Comprar=11664, Vender=13356\n",
      "Ganhos Totais: 8203.00, Perdas Totais: -9464.25\n",
      "Modelo e log do episódio 7 salvos em: 4.9.2\\model_episode_7.pth e 4.9.2\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -379.75, Win Rate: 0.52, Wins: 445, Losses: 417, Epsilon: 0.4614, Steps: 36754, Time: 175.61s\n",
      "Ações: Manter=13803, Comprar=11275, Vender=11676\n",
      "Ganhos Totais: 9706.50, Perdas Totais: -10086.25\n",
      "Modelo e log do episódio 8 salvos em: 4.9.2\\model_episode_8.pth e 4.9.2\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -924.75, Win Rate: 0.51, Wins: 437, Losses: 417, Epsilon: 0.4568, Steps: 36754, Time: 176.66s\n",
      "Ações: Manter=13917, Comprar=11253, Vender=11584\n",
      "Ganhos Totais: 9117.00, Perdas Totais: -10041.75\n",
      "Modelo e log do episódio 9 salvos em: 4.9.2\\model_episode_9.pth e 4.9.2\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: 251.00, Win Rate: 0.51, Wins: 434, Losses: 416, Epsilon: 0.4522, Steps: 36754, Time: 175.12s\n",
      "Ações: Manter=11954, Comprar=12570, Vender=12230\n",
      "Ganhos Totais: 9279.25, Perdas Totais: -9028.25\n",
      "Modelo e log do episódio 10 salvos em: 4.9.2\\model_episode_10.pth e 4.9.2\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 524.25, Win Rate: 0.52, Wins: 454, Losses: 419, Epsilon: 0.4477, Steps: 36754, Time: 177.22s\n",
      "Ações: Manter=12536, Comprar=11635, Vender=12583\n",
      "Ganhos Totais: 9711.00, Perdas Totais: -9186.75\n",
      "Modelo e log do episódio 11 salvos em: 4.9.2\\model_episode_11.pth e 4.9.2\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -1349.25, Win Rate: 0.52, Wins: 439, Losses: 410, Epsilon: 0.4432, Steps: 36754, Time: 178.35s\n",
      "Ações: Manter=13376, Comprar=11580, Vender=11798\n",
      "Ganhos Totais: 9611.75, Perdas Totais: -10961.00\n",
      "Episode 13/100, Total Reward: 881.50, Win Rate: 0.51, Wins: 435, Losses: 414, Epsilon: 0.4388, Steps: 36754, Time: 163.26s\n",
      "Ações: Manter=11397, Comprar=13301, Vender=12056\n",
      "Ganhos Totais: 9866.75, Perdas Totais: -8985.25\n",
      "Modelo e log do episódio 13 salvos em: 4.9.2\\model_episode_13.pth e 4.9.2\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -244.00, Win Rate: 0.51, Wins: 440, Losses: 416, Epsilon: 0.4344, Steps: 36754, Time: 149.16s\n",
      "Ações: Manter=12714, Comprar=10685, Vender=13355\n",
      "Ganhos Totais: 10375.50, Perdas Totais: -10619.50\n",
      "Modelo e log do episódio 14 salvos em: 4.9.2\\model_episode_14.pth e 4.9.2\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -2050.25, Win Rate: 0.51, Wins: 434, Losses: 412, Epsilon: 0.4300, Steps: 36754, Time: 146.86s\n",
      "Ações: Manter=10921, Comprar=12466, Vender=13367\n",
      "Ganhos Totais: 8658.50, Perdas Totais: -10708.75\n",
      "Episode 16/100, Total Reward: 532.25, Win Rate: 0.52, Wins: 452, Losses: 418, Epsilon: 0.4257, Steps: 36754, Time: 146.53s\n",
      "Ações: Manter=11062, Comprar=12373, Vender=13319\n",
      "Ganhos Totais: 10446.25, Perdas Totais: -9914.00\n",
      "Modelo e log do episódio 16 salvos em: 4.9.2\\model_episode_16.pth e 4.9.2\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -674.00, Win Rate: 0.51, Wins: 431, Losses: 409, Epsilon: 0.4215, Steps: 36754, Time: 146.35s\n",
      "Ações: Manter=14356, Comprar=10522, Vender=11876\n",
      "Ganhos Totais: 9380.50, Perdas Totais: -10054.50\n",
      "Episode 18/100, Total Reward: 216.25, Win Rate: 0.53, Wins: 468, Losses: 412, Epsilon: 0.4173, Steps: 36754, Time: 146.85s\n",
      "Ações: Manter=11415, Comprar=11283, Vender=14056\n",
      "Ganhos Totais: 10682.50, Perdas Totais: -10466.25\n",
      "Modelo e log do episódio 18 salvos em: 4.9.2\\model_episode_18.pth e 4.9.2\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -325.50, Win Rate: 0.50, Wins: 424, Losses: 420, Epsilon: 0.4131, Steps: 36754, Time: 146.98s\n",
      "Ações: Manter=13768, Comprar=10498, Vender=12488\n",
      "Ganhos Totais: 9415.00, Perdas Totais: -9740.50\n",
      "Episode 20/100, Total Reward: -74.25, Win Rate: 0.53, Wins: 459, Losses: 414, Epsilon: 0.4090, Steps: 36754, Time: 147.42s\n",
      "Ações: Manter=11002, Comprar=12743, Vender=13009\n",
      "Ganhos Totais: 10283.25, Perdas Totais: -10357.50\n",
      "Modelo e log do episódio 20 salvos em: 4.9.2\\model_episode_20.pth e 4.9.2\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -1111.25, Win Rate: 0.52, Wins: 446, Losses: 414, Epsilon: 0.4049, Steps: 36754, Time: 147.81s\n",
      "Ações: Manter=14331, Comprar=10415, Vender=12008\n",
      "Ganhos Totais: 10070.50, Perdas Totais: -11181.75\n",
      "Episode 22/100, Total Reward: -1801.00, Win Rate: 0.51, Wins: 426, Losses: 412, Epsilon: 0.4008, Steps: 36754, Time: 148.10s\n",
      "Ações: Manter=13386, Comprar=10063, Vender=13305\n",
      "Ganhos Totais: 10336.00, Perdas Totais: -12137.00\n",
      "Episode 23/100, Total Reward: -2797.50, Win Rate: 0.49, Wins: 382, Losses: 405, Epsilon: 0.3968, Steps: 36754, Time: 147.73s\n",
      "Ações: Manter=13624, Comprar=9131, Vender=13999\n",
      "Ganhos Totais: 9549.50, Perdas Totais: -12347.00\n",
      "Episode 24/100, Total Reward: 3.25, Win Rate: 0.53, Wins: 456, Losses: 408, Epsilon: 0.3928, Steps: 36754, Time: 147.30s\n",
      "Ações: Manter=13827, Comprar=9875, Vender=13052\n",
      "Ganhos Totais: 10761.00, Perdas Totais: -10757.75\n",
      "Modelo e log do episódio 24 salvos em: 4.9.2\\model_episode_24.pth e 4.9.2\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -576.50, Win Rate: 0.52, Wins: 438, Losses: 412, Epsilon: 0.3889, Steps: 36754, Time: 147.25s\n",
      "Ações: Manter=13510, Comprar=8551, Vender=14693\n",
      "Ganhos Totais: 10694.50, Perdas Totais: -11271.00\n",
      "Episode 26/100, Total Reward: -2291.00, Win Rate: 0.50, Wins: 407, Losses: 414, Epsilon: 0.3850, Steps: 36754, Time: 147.31s\n",
      "Ações: Manter=13096, Comprar=9527, Vender=14131\n",
      "Ganhos Totais: 9304.00, Perdas Totais: -11595.00\n",
      "Episode 27/100, Total Reward: -20.50, Win Rate: 0.52, Wins: 438, Losses: 411, Epsilon: 0.3812, Steps: 36754, Time: 147.84s\n",
      "Ações: Manter=12584, Comprar=11124, Vender=13046\n",
      "Ganhos Totais: 9313.75, Perdas Totais: -9334.25\n",
      "Modelo e log do episódio 27 salvos em: 4.9.2\\model_episode_27.pth e 4.9.2\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: -1984.50, Win Rate: 0.54, Wins: 482, Losses: 407, Epsilon: 0.3774, Steps: 36754, Time: 147.97s\n",
      "Ações: Manter=14473, Comprar=9226, Vender=13055\n",
      "Ganhos Totais: 10480.25, Perdas Totais: -12464.75\n",
      "Episode 29/100, Total Reward: -2346.25, Win Rate: 0.49, Wins: 400, Losses: 415, Epsilon: 0.3736, Steps: 36754, Time: 147.59s\n",
      "Ações: Manter=14984, Comprar=8347, Vender=13423\n",
      "Ganhos Totais: 9170.00, Perdas Totais: -11516.25\n",
      "Episode 30/100, Total Reward: -1062.00, Win Rate: 0.51, Wins: 423, Losses: 411, Epsilon: 0.3699, Steps: 36754, Time: 147.98s\n",
      "Ações: Manter=13583, Comprar=10087, Vender=13084\n",
      "Ganhos Totais: 9831.50, Perdas Totais: -10893.50\n",
      "Episode 31/100, Total Reward: -2226.00, Win Rate: 0.50, Wins: 417, Losses: 411, Epsilon: 0.3662, Steps: 36754, Time: 148.21s\n",
      "Ações: Manter=11531, Comprar=9887, Vender=15336\n",
      "Ganhos Totais: 9378.00, Perdas Totais: -11604.00\n",
      "Episode 32/100, Total Reward: -3527.75, Win Rate: 0.49, Wins: 396, Losses: 413, Epsilon: 0.3625, Steps: 36754, Time: 148.12s\n",
      "Ações: Manter=11019, Comprar=8766, Vender=16969\n",
      "Ganhos Totais: 9048.00, Perdas Totais: -12575.75\n",
      "Episode 33/100, Total Reward: 237.00, Win Rate: 0.51, Wins: 422, Losses: 410, Epsilon: 0.3589, Steps: 36754, Time: 148.28s\n",
      "Ações: Manter=11529, Comprar=10532, Vender=14693\n",
      "Ganhos Totais: 11327.75, Perdas Totais: -11090.75\n",
      "Modelo e log do episódio 33 salvos em: 4.9.2\\model_episode_33.pth e 4.9.2\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: -2585.25, Win Rate: 0.51, Wins: 421, Losses: 412, Epsilon: 0.3553, Steps: 36754, Time: 147.98s\n",
      "Ações: Manter=13633, Comprar=9487, Vender=13634\n",
      "Ganhos Totais: 9634.75, Perdas Totais: -12220.00\n",
      "Episode 35/100, Total Reward: -1784.50, Win Rate: 0.52, Wins: 440, Losses: 400, Epsilon: 0.3517, Steps: 36754, Time: 148.23s\n",
      "Ações: Manter=14574, Comprar=9592, Vender=12588\n",
      "Ganhos Totais: 10965.50, Perdas Totais: -12750.00\n",
      "Episode 36/100, Total Reward: -1292.25, Win Rate: 0.52, Wins: 437, Losses: 402, Epsilon: 0.3482, Steps: 36754, Time: 148.21s\n",
      "Ações: Manter=11621, Comprar=13418, Vender=11715\n",
      "Ganhos Totais: 10692.00, Perdas Totais: -11984.25\n",
      "Episode 37/100, Total Reward: -717.25, Win Rate: 0.51, Wins: 435, Losses: 413, Epsilon: 0.3447, Steps: 36754, Time: 148.16s\n",
      "Ações: Manter=10145, Comprar=13963, Vender=12646\n",
      "Ganhos Totais: 9955.00, Perdas Totais: -10672.25\n",
      "Episode 38/100, Total Reward: -1533.25, Win Rate: 0.54, Wins: 478, Losses: 407, Epsilon: 0.3413, Steps: 36754, Time: 148.26s\n",
      "Ações: Manter=10731, Comprar=12862, Vender=13161\n",
      "Ganhos Totais: 9698.25, Perdas Totais: -11231.50\n",
      "Episode 39/100, Total Reward: 1463.25, Win Rate: 0.52, Wins: 440, Losses: 411, Epsilon: 0.3379, Steps: 36754, Time: 148.22s\n",
      "Ações: Manter=9685, Comprar=16555, Vender=10514\n",
      "Ganhos Totais: 10697.00, Perdas Totais: -9233.75\n",
      "Modelo e log do episódio 39 salvos em: 4.9.2\\model_episode_39.pth e 4.9.2\\log_episode_39.csv\n",
      "\n",
      "Episode 40/100, Total Reward: 593.00, Win Rate: 0.54, Wins: 482, Losses: 417, Epsilon: 0.3345, Steps: 36754, Time: 147.92s\n",
      "Ações: Manter=9596, Comprar=17811, Vender=9347\n",
      "Ganhos Totais: 11662.00, Perdas Totais: -11069.00\n",
      "Modelo e log do episódio 40 salvos em: 4.9.2\\model_episode_40.pth e 4.9.2\\log_episode_40.csv\n",
      "\n",
      "Episode 41/100, Total Reward: 1817.50, Win Rate: 0.52, Wins: 450, Losses: 408, Epsilon: 0.3311, Steps: 36754, Time: 148.46s\n",
      "Ações: Manter=9691, Comprar=14691, Vender=12372\n",
      "Ganhos Totais: 11063.75, Perdas Totais: -9246.25\n",
      "Modelo e log do episódio 41 salvos em: 4.9.2\\model_episode_41.pth e 4.9.2\\log_episode_41.csv\n",
      "\n",
      "Episode 42/100, Total Reward: 413.50, Win Rate: 0.54, Wins: 471, Losses: 400, Epsilon: 0.3278, Steps: 36754, Time: 147.89s\n",
      "Ações: Manter=17087, Comprar=9625, Vender=10042\n",
      "Ganhos Totais: 13140.50, Perdas Totais: -12727.00\n",
      "Modelo e log do episódio 42 salvos em: 4.9.2\\model_episode_42.pth e 4.9.2\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: 1111.00, Win Rate: 0.52, Wins: 408, Losses: 376, Epsilon: 0.3246, Steps: 36754, Time: 148.45s\n",
      "Ações: Manter=20871, Comprar=10693, Vender=5190\n",
      "Ganhos Totais: 14330.50, Perdas Totais: -13219.50\n",
      "Modelo e log do episódio 43 salvos em: 4.9.2\\model_episode_43.pth e 4.9.2\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: 366.25, Win Rate: 0.54, Wins: 417, Losses: 359, Epsilon: 0.3213, Steps: 36754, Time: 148.42s\n",
      "Ações: Manter=21596, Comprar=5461, Vender=9697\n",
      "Ganhos Totais: 14634.00, Perdas Totais: -14267.75\n",
      "Modelo e log do episódio 44 salvos em: 4.9.2\\model_episode_44.pth e 4.9.2\\log_episode_44.csv\n",
      "\n",
      "Episode 45/100, Total Reward: -2232.00, Win Rate: 0.52, Wins: 418, Losses: 382, Epsilon: 0.3181, Steps: 36754, Time: 148.45s\n",
      "Ações: Manter=15804, Comprar=9280, Vender=11670\n",
      "Ganhos Totais: 12014.25, Perdas Totais: -14246.25\n",
      "Episode 46/100, Total Reward: -1572.00, Win Rate: 0.52, Wins: 438, Losses: 407, Epsilon: 0.3149, Steps: 36754, Time: 148.69s\n",
      "Ações: Manter=7375, Comprar=5837, Vender=23542\n",
      "Ganhos Totais: 11245.00, Perdas Totais: -12817.00\n",
      "Episode 47/100, Total Reward: 1504.75, Win Rate: 0.53, Wins: 415, Losses: 364, Epsilon: 0.3118, Steps: 36754, Time: 149.66s\n",
      "Ações: Manter=15969, Comprar=5298, Vender=15487\n",
      "Ganhos Totais: 14256.75, Perdas Totais: -12752.00\n",
      "Modelo e log do episódio 47 salvos em: 4.9.2\\model_episode_47.pth e 4.9.2\\log_episode_47.csv\n",
      "\n",
      "Episode 48/100, Total Reward: -2139.75, Win Rate: 0.48, Wins: 366, Losses: 404, Epsilon: 0.3086, Steps: 36754, Time: 149.40s\n",
      "Ações: Manter=6533, Comprar=5653, Vender=24568\n",
      "Ganhos Totais: 11124.50, Perdas Totais: -13264.25\n",
      "Episode 49/100, Total Reward: -1234.50, Win Rate: 0.54, Wins: 483, Losses: 408, Epsilon: 0.3056, Steps: 36754, Time: 149.00s\n",
      "Ações: Manter=7989, Comprar=10258, Vender=18507\n",
      "Ganhos Totais: 11365.75, Perdas Totais: -12600.25\n",
      "Episode 50/100, Total Reward: 2628.75, Win Rate: 0.53, Wins: 397, Losses: 359, Epsilon: 0.3025, Steps: 36754, Time: 149.42s\n",
      "Ações: Manter=22282, Comprar=4981, Vender=9491\n",
      "Ganhos Totais: 14316.50, Perdas Totais: -11687.75\n",
      "Modelo e log do episódio 50 salvos em: 4.9.2\\model_episode_50.pth e 4.9.2\\log_episode_50.csv\n",
      "\n",
      "Episode 51/100, Total Reward: -572.00, Win Rate: 0.51, Wins: 381, Losses: 365, Epsilon: 0.2995, Steps: 36754, Time: 149.33s\n",
      "Ações: Manter=16843, Comprar=4748, Vender=15163\n",
      "Ganhos Totais: 13354.00, Perdas Totais: -13926.00\n",
      "Episode 52/100, Total Reward: 682.25, Win Rate: 0.51, Wins: 371, Losses: 354, Epsilon: 0.2965, Steps: 36754, Time: 150.05s\n",
      "Ações: Manter=18368, Comprar=9328, Vender=9058\n",
      "Ganhos Totais: 13745.25, Perdas Totais: -13063.00\n",
      "Modelo e log do episódio 52 salvos em: 4.9.2\\model_episode_52.pth e 4.9.2\\log_episode_52.csv\n",
      "\n",
      "Episode 53/100, Total Reward: 898.50, Win Rate: 0.55, Wins: 439, Losses: 357, Epsilon: 0.2935, Steps: 36754, Time: 149.60s\n",
      "Ações: Manter=13874, Comprar=16563, Vender=6317\n",
      "Ganhos Totais: 15008.50, Perdas Totais: -14110.00\n",
      "Modelo e log do episódio 53 salvos em: 4.9.2\\model_episode_53.pth e 4.9.2\\log_episode_53.csv\n",
      "\n",
      "Episode 54/100, Total Reward: -984.25, Win Rate: 0.48, Wins: 345, Losses: 371, Epsilon: 0.2906, Steps: 36754, Time: 149.22s\n",
      "Ações: Manter=11437, Comprar=15162, Vender=10155\n",
      "Ganhos Totais: 12729.75, Perdas Totais: -13714.00\n",
      "Episode 55/100, Total Reward: -1420.25, Win Rate: 0.50, Wins: 388, Losses: 382, Epsilon: 0.2877, Steps: 36754, Time: 149.56s\n",
      "Ações: Manter=8178, Comprar=4764, Vender=23812\n",
      "Ganhos Totais: 12947.25, Perdas Totais: -14367.50\n",
      "Episode 56/100, Total Reward: 725.75, Win Rate: 0.51, Wins: 389, Losses: 373, Epsilon: 0.2848, Steps: 36754, Time: 149.12s\n",
      "Ações: Manter=8770, Comprar=22588, Vender=5396\n",
      "Ganhos Totais: 14641.25, Perdas Totais: -13915.50\n",
      "Modelo e log do episódio 56 salvos em: 4.9.2\\model_episode_56.pth e 4.9.2\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: 413.75, Win Rate: 0.52, Wins: 360, Losses: 335, Epsilon: 0.2820, Steps: 36754, Time: 149.53s\n",
      "Ações: Manter=27603, Comprar=4668, Vender=4483\n",
      "Ganhos Totais: 15233.50, Perdas Totais: -14819.75\n",
      "Episode 58/100, Total Reward: -602.50, Win Rate: 0.52, Wins: 362, Losses: 333, Epsilon: 0.2791, Steps: 36754, Time: 149.80s\n",
      "Ações: Manter=27568, Comprar=4441, Vender=4745\n",
      "Ganhos Totais: 14477.00, Perdas Totais: -15079.50\n",
      "Episode 59/100, Total Reward: 943.75, Win Rate: 0.53, Wins: 380, Losses: 333, Epsilon: 0.2763, Steps: 36754, Time: 150.01s\n",
      "Ações: Manter=27472, Comprar=4886, Vender=4396\n",
      "Ganhos Totais: 15042.50, Perdas Totais: -14098.75\n",
      "Modelo e log do episódio 59 salvos em: 4.9.2\\model_episode_59.pth e 4.9.2\\log_episode_59.csv\n",
      "\n",
      "Episode 60/100, Total Reward: -2099.25, Win Rate: 0.55, Wins: 404, Losses: 325, Epsilon: 0.2736, Steps: 36754, Time: 149.77s\n",
      "Ações: Manter=27335, Comprar=4579, Vender=4840\n",
      "Ganhos Totais: 14402.50, Perdas Totais: -16501.75\n",
      "Episode 61/100, Total Reward: -2169.75, Win Rate: 0.48, Wins: 345, Losses: 371, Epsilon: 0.2708, Steps: 36754, Time: 149.91s\n",
      "Ações: Manter=12158, Comprar=4637, Vender=19959\n",
      "Ganhos Totais: 12684.00, Perdas Totais: -14853.75\n",
      "Episode 62/100, Total Reward: 2764.00, Win Rate: 0.53, Wins: 386, Losses: 340, Epsilon: 0.2681, Steps: 36754, Time: 150.11s\n",
      "Ações: Manter=16661, Comprar=4414, Vender=15679\n",
      "Ganhos Totais: 15769.00, Perdas Totais: -13005.00\n",
      "Modelo e log do episódio 62 salvos em: 4.9.2\\model_episode_62.pth e 4.9.2\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: 104.75, Win Rate: 0.54, Wins: 415, Losses: 353, Epsilon: 0.2655, Steps: 36754, Time: 150.05s\n",
      "Ações: Manter=11851, Comprar=15597, Vender=9306\n",
      "Ganhos Totais: 14494.75, Perdas Totais: -14390.00\n",
      "Episode 64/100, Total Reward: -735.50, Win Rate: 0.50, Wins: 367, Losses: 363, Epsilon: 0.2628, Steps: 36754, Time: 150.24s\n",
      "Ações: Manter=10647, Comprar=4386, Vender=21721\n",
      "Ganhos Totais: 14636.00, Perdas Totais: -15371.50\n",
      "Episode 65/100, Total Reward: -1057.75, Win Rate: 0.50, Wins: 345, Losses: 339, Epsilon: 0.2602, Steps: 36754, Time: 150.22s\n",
      "Ações: Manter=27877, Comprar=4559, Vender=4318\n",
      "Ganhos Totais: 14373.50, Perdas Totais: -15431.25\n",
      "Episode 66/100, Total Reward: -824.50, Win Rate: 0.52, Wins: 363, Losses: 335, Epsilon: 0.2576, Steps: 36754, Time: 150.27s\n",
      "Ações: Manter=27982, Comprar=4012, Vender=4760\n",
      "Ganhos Totais: 14708.50, Perdas Totais: -15533.00\n",
      "Episode 67/100, Total Reward: 1169.50, Win Rate: 0.52, Wins: 359, Losses: 327, Epsilon: 0.2550, Steps: 36754, Time: 150.40s\n",
      "Ações: Manter=28025, Comprar=4225, Vender=4504\n",
      "Ganhos Totais: 14915.00, Perdas Totais: -13745.50\n",
      "Modelo e log do episódio 67 salvos em: 4.9.2\\model_episode_67.pth e 4.9.2\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: -1692.50, Win Rate: 0.51, Wins: 333, Losses: 324, Epsilon: 0.2524, Steps: 36754, Time: 150.19s\n",
      "Ações: Manter=28580, Comprar=3904, Vender=4270\n",
      "Ganhos Totais: 14191.00, Perdas Totais: -15883.50\n",
      "Episode 69/100, Total Reward: -3549.25, Win Rate: 0.51, Wins: 342, Losses: 330, Epsilon: 0.2499, Steps: 36754, Time: 150.09s\n",
      "Ações: Manter=28336, Comprar=4119, Vender=4299\n",
      "Ganhos Totais: 13530.50, Perdas Totais: -17079.75\n",
      "Episode 70/100, Total Reward: -490.25, Win Rate: 0.51, Wins: 318, Losses: 307, Epsilon: 0.2474, Steps: 36754, Time: 149.95s\n",
      "Ações: Manter=28328, Comprar=4105, Vender=4321\n",
      "Ganhos Totais: 14006.00, Perdas Totais: -14496.25\n",
      "Episode 71/100, Total Reward: -627.50, Win Rate: 0.51, Wins: 326, Losses: 318, Epsilon: 0.2449, Steps: 36754, Time: 151.47s\n",
      "Ações: Manter=28077, Comprar=4174, Vender=4503\n",
      "Ganhos Totais: 14202.25, Perdas Totais: -14829.75\n",
      "Episode 72/100, Total Reward: -1775.50, Win Rate: 0.49, Wins: 313, Losses: 323, Epsilon: 0.2425, Steps: 36754, Time: 150.47s\n",
      "Ações: Manter=27823, Comprar=3630, Vender=5301\n",
      "Ganhos Totais: 13969.25, Perdas Totais: -15744.75\n",
      "Episode 73/100, Total Reward: -6479.25, Win Rate: 0.48, Wins: 336, Losses: 371, Epsilon: 0.2401, Steps: 36754, Time: 150.24s\n",
      "Ações: Manter=6381, Comprar=3667, Vender=26706\n",
      "Ganhos Totais: 11788.50, Perdas Totais: -18267.75\n",
      "Episode 74/100, Total Reward: 1798.00, Win Rate: 0.54, Wins: 354, Losses: 305, Epsilon: 0.2377, Steps: 36754, Time: 150.53s\n",
      "Ações: Manter=28432, Comprar=3715, Vender=4607\n",
      "Ganhos Totais: 15725.00, Perdas Totais: -13927.00\n",
      "Modelo e log do episódio 74 salvos em: 4.9.2\\model_episode_74.pth e 4.9.2\\log_episode_74.csv\n",
      "\n",
      "Episode 75/100, Total Reward: 2678.25, Win Rate: 0.50, Wins: 322, Losses: 320, Epsilon: 0.2353, Steps: 36754, Time: 150.30s\n",
      "Ações: Manter=24591, Comprar=3384, Vender=8779\n",
      "Ganhos Totais: 15381.50, Perdas Totais: -12703.25\n",
      "Modelo e log do episódio 75 salvos em: 4.9.2\\model_episode_75.pth e 4.9.2\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: -673.50, Win Rate: 0.49, Wins: 311, Losses: 327, Epsilon: 0.2329, Steps: 36754, Time: 150.80s\n",
      "Ações: Manter=21346, Comprar=3382, Vender=12026\n",
      "Ganhos Totais: 13315.75, Perdas Totais: -13989.25\n",
      "Episode 77/100, Total Reward: -3403.75, Win Rate: 0.47, Wins: 328, Losses: 364, Epsilon: 0.2306, Steps: 36754, Time: 150.79s\n",
      "Ações: Manter=12398, Comprar=3611, Vender=20745\n",
      "Ganhos Totais: 13170.00, Perdas Totais: -16573.75\n",
      "Episode 78/100, Total Reward: -4228.25, Win Rate: 0.50, Wins: 343, Losses: 348, Epsilon: 0.2283, Steps: 36754, Time: 150.22s\n",
      "Ações: Manter=12463, Comprar=3527, Vender=20764\n",
      "Ganhos Totais: 12791.50, Perdas Totais: -17019.75\n",
      "Episode 79/100, Total Reward: -2195.75, Win Rate: 0.48, Wins: 289, Losses: 319, Epsilon: 0.2260, Steps: 36754, Time: 148.77s\n",
      "Ações: Manter=26266, Comprar=3505, Vender=6983\n",
      "Ganhos Totais: 13102.50, Perdas Totais: -15298.25\n",
      "Episode 80/100, Total Reward: -70.25, Win Rate: 0.49, Wins: 341, Losses: 360, Epsilon: 0.2238, Steps: 36754, Time: 138.47s\n",
      "Ações: Manter=5079, Comprar=3667, Vender=28008\n",
      "Ganhos Totais: 15373.75, Perdas Totais: -15444.00\n",
      "Episode 81/100, Total Reward: -1591.25, Win Rate: 0.47, Wins: 323, Losses: 362, Epsilon: 0.2215, Steps: 36754, Time: 137.99s\n",
      "Ações: Manter=11359, Comprar=3618, Vender=21777\n",
      "Ganhos Totais: 14133.25, Perdas Totais: -15724.50\n",
      "Episode 82/100, Total Reward: -637.50, Win Rate: 0.50, Wins: 361, Losses: 365, Epsilon: 0.2193, Steps: 36754, Time: 137.52s\n",
      "Ações: Manter=3423, Comprar=3614, Vender=29717\n",
      "Ganhos Totais: 14391.00, Perdas Totais: -15028.50\n",
      "Episode 83/100, Total Reward: -1521.50, Win Rate: 0.47, Wins: 328, Losses: 368, Epsilon: 0.2171, Steps: 36754, Time: 137.99s\n",
      "Ações: Manter=3339, Comprar=3428, Vender=29987\n",
      "Ganhos Totais: 13025.75, Perdas Totais: -14547.25\n",
      "Episode 84/100, Total Reward: -1004.25, Win Rate: 0.50, Wins: 369, Losses: 370, Epsilon: 0.2149, Steps: 36754, Time: 137.63s\n",
      "Ações: Manter=7088, Comprar=3707, Vender=25959\n",
      "Ganhos Totais: 14502.00, Perdas Totais: -15506.25\n",
      "Episode 85/100, Total Reward: -386.00, Win Rate: 0.50, Wins: 313, Losses: 309, Epsilon: 0.2128, Steps: 36754, Time: 137.43s\n",
      "Ações: Manter=28940, Comprar=3149, Vender=4665\n",
      "Ganhos Totais: 15207.00, Perdas Totais: -15593.00\n",
      "Episode 86/100, Total Reward: 380.25, Win Rate: 0.51, Wins: 334, Losses: 325, Epsilon: 0.2107, Steps: 36754, Time: 137.63s\n",
      "Ações: Manter=20366, Comprar=3239, Vender=13149\n",
      "Ganhos Totais: 15404.50, Perdas Totais: -15024.25\n",
      "Episode 87/100, Total Reward: -3250.00, Win Rate: 0.49, Wins: 351, Losses: 369, Epsilon: 0.2086, Steps: 36754, Time: 137.32s\n",
      "Ações: Manter=3924, Comprar=2937, Vender=29893\n",
      "Ganhos Totais: 13617.75, Perdas Totais: -16867.75\n",
      "Episode 88/100, Total Reward: -45.50, Win Rate: 0.50, Wins: 316, Losses: 317, Epsilon: 0.2065, Steps: 36754, Time: 137.84s\n",
      "Ações: Manter=23167, Comprar=3167, Vender=10420\n",
      "Ganhos Totais: 14758.50, Perdas Totais: -14804.00\n",
      "Episode 89/100, Total Reward: -1628.00, Win Rate: 0.52, Wins: 349, Losses: 326, Epsilon: 0.2044, Steps: 36754, Time: 137.44s\n",
      "Ações: Manter=16774, Comprar=3479, Vender=16501\n",
      "Ganhos Totais: 12923.50, Perdas Totais: -14551.50\n",
      "Episode 90/100, Total Reward: 2450.00, Win Rate: 0.54, Wins: 327, Losses: 284, Epsilon: 0.2024, Steps: 36754, Time: 138.15s\n",
      "Ações: Manter=29257, Comprar=3464, Vender=4033\n",
      "Ganhos Totais: 16867.50, Perdas Totais: -14417.50\n",
      "Modelo e log do episódio 90 salvos em: 4.9.2\\model_episode_90.pth e 4.9.2\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: -5379.75, Win Rate: 0.48, Wins: 295, Losses: 315, Epsilon: 0.2003, Steps: 36754, Time: 137.55s\n",
      "Ações: Manter=29067, Comprar=3279, Vender=4408\n",
      "Ganhos Totais: 12476.50, Perdas Totais: -17856.25\n",
      "Episode 92/100, Total Reward: -3601.75, Win Rate: 0.51, Wins: 343, Losses: 332, Epsilon: 0.1983, Steps: 36754, Time: 138.49s\n",
      "Ações: Manter=16036, Comprar=15571, Vender=5147\n",
      "Ganhos Totais: 14653.75, Perdas Totais: -18255.50\n",
      "Episode 93/100, Total Reward: -2717.00, Win Rate: 0.52, Wins: 359, Losses: 332, Epsilon: 0.1964, Steps: 36754, Time: 137.94s\n",
      "Ações: Manter=13397, Comprar=3093, Vender=20264\n",
      "Ganhos Totais: 13830.75, Perdas Totais: -16547.75\n",
      "Episode 94/100, Total Reward: -1945.00, Win Rate: 0.48, Wins: 335, Losses: 360, Epsilon: 0.1944, Steps: 36754, Time: 137.94s\n",
      "Ações: Manter=3664, Comprar=3079, Vender=30011\n",
      "Ganhos Totais: 13819.00, Perdas Totais: -15764.00\n",
      "Episode 95/100, Total Reward: -3908.25, Win Rate: 0.48, Wins: 326, Losses: 355, Epsilon: 0.1924, Steps: 36754, Time: 138.17s\n",
      "Ações: Manter=7059, Comprar=3072, Vender=26623\n",
      "Ganhos Totais: 13684.50, Perdas Totais: -17592.75\n",
      "Episode 96/100, Total Reward: -2802.50, Win Rate: 0.49, Wins: 296, Losses: 307, Epsilon: 0.1905, Steps: 36754, Time: 137.74s\n",
      "Ações: Manter=29781, Comprar=3047, Vender=3926\n",
      "Ganhos Totais: 13646.50, Perdas Totais: -16449.00\n",
      "Episode 97/100, Total Reward: -4689.50, Win Rate: 0.50, Wins: 304, Losses: 309, Epsilon: 0.1886, Steps: 36754, Time: 138.08s\n",
      "Ações: Manter=29671, Comprar=2993, Vender=4090\n",
      "Ganhos Totais: 12872.50, Perdas Totais: -17562.00\n",
      "Episode 98/100, Total Reward: 2532.75, Win Rate: 0.57, Wins: 367, Losses: 281, Epsilon: 0.1867, Steps: 36754, Time: 139.13s\n",
      "Ações: Manter=29694, Comprar=3172, Vender=3888\n",
      "Ganhos Totais: 16835.75, Perdas Totais: -14303.00\n",
      "Modelo e log do episódio 98 salvos em: 4.9.2\\model_episode_98.pth e 4.9.2\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: -774.00, Win Rate: 0.52, Wins: 347, Losses: 314, Epsilon: 0.1849, Steps: 36754, Time: 138.14s\n",
      "Ações: Manter=15512, Comprar=3353, Vender=17889\n",
      "Ganhos Totais: 15283.50, Perdas Totais: -16057.50\n",
      "Episode 100/100, Total Reward: -482.00, Win Rate: 0.51, Wins: 327, Losses: 319, Epsilon: 0.1830, Steps: 36754, Time: 137.99s\n",
      "Ações: Manter=27610, Comprar=3048, Vender=6096\n",
      "Ganhos Totais: 15613.00, Perdas Totais: -16095.00\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 62, Total Reward: 2764.00, Win Rate: 0.53, Wins: 386, Losses: 340, Ações: {0: 16661, 1: 4414, 2: 15679}, Steps: 36754, Time: 150.11s\n",
      "Rank 2: Episode 75, Total Reward: 2678.25, Win Rate: 0.50, Wins: 322, Losses: 320, Ações: {0: 24591, 1: 3384, 2: 8779}, Steps: 36754, Time: 150.30s\n",
      "Rank 3: Episode 50, Total Reward: 2628.75, Win Rate: 0.53, Wins: 397, Losses: 359, Ações: {0: 22282, 1: 4981, 2: 9491}, Steps: 36754, Time: 149.42s\n",
      "Rank 4: Episode 98, Total Reward: 2532.75, Win Rate: 0.57, Wins: 367, Losses: 281, Ações: {0: 29694, 1: 3172, 2: 3888}, Steps: 36754, Time: 139.13s\n",
      "Rank 5: Episode 90, Total Reward: 2450.00, Win Rate: 0.54, Wins: 327, Losses: 284, Ações: {0: 29257, 1: 3464, 2: 4033}, Steps: 36754, Time: 138.15s\n",
      "Rank 6: Episode 41, Total Reward: 1817.50, Win Rate: 0.52, Wins: 450, Losses: 408, Ações: {0: 9691, 1: 14691, 2: 12372}, Steps: 36754, Time: 148.46s\n",
      "Rank 7: Episode 74, Total Reward: 1798.00, Win Rate: 0.54, Wins: 354, Losses: 305, Ações: {0: 28432, 1: 3715, 2: 4607}, Steps: 36754, Time: 150.53s\n",
      "Rank 8: Episode 47, Total Reward: 1504.75, Win Rate: 0.53, Wins: 415, Losses: 364, Ações: {0: 15969, 1: 5298, 2: 15487}, Steps: 36754, Time: 149.66s\n",
      "Rank 9: Episode 39, Total Reward: 1463.25, Win Rate: 0.52, Wins: 440, Losses: 411, Ações: {0: 9685, 1: 16555, 2: 10514}, Steps: 36754, Time: 148.22s\n",
      "Rank 10: Episode 67, Total Reward: 1169.50, Win Rate: 0.52, Wins: 359, Losses: 327, Ações: {0: 28025, 1: 4225, 2: 4504}, Steps: 36754, Time: 150.40s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "        self.operation_limit = 0  # Variável de controle para limitar operações após perda\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        self.operation_limit = 0  # Resetar a limitação de operações ao resetar o ambiente\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo e o agente não estiver limitado por perda anterior\n",
    "        if gatilho == 1 and self.operation_limit == 0:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "\n",
    "                    # Atualizar operation_limit caso a operação tenha dado prejuízo\n",
    "                    if profit < 0:\n",
    "                        self.operation_limit = 1\n",
    "\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "\n",
    "                    # Atualizar operation_limit caso a operação tenha dado prejuízo\n",
    "                    if profit < 0:\n",
    "                        self.operation_limit = 1\n",
    "        elif gatilho == 0:\n",
    "            # Quando o gatilho for zero, liberar a operação novamente\n",
    "            self.operation_limit = 0\n",
    "\n",
    "            # Fechar qualquer posição aberta\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.48\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.9.2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
