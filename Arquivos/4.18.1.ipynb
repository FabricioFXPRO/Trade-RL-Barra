{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass\n",
    "        else:\n",
    "            # Fechar posição se o gatilho não estiver ativo\n",
    "            if self.position == 1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 3: Criar o Agente Rainbow DQN usando PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Implementação da Camada Noisy Linear\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma_init = sigma_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.zeros(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.zeros(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return nn.functional.linear(x, weight, bias)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt())\n",
    "\n",
    "# Implementação da Rede Rainbow DQN\n",
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Advantage stream\n",
    "        self.advantage = nn.Sequential(\n",
    "            NoisyLinear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, n_actions)\n",
    "        )\n",
    "        # Value stream\n",
    "        self.value = nn.Sequential(\n",
    "            NoisyLinear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        # Dueling Q-values\n",
    "        q_values = value + advantage - advantage.mean()\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()\n",
    "\n",
    "# Implementação do Prioritized Replay Buffer\n",
    "class PrioritizedReplayBuffer(object):\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def push(self, *args):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((*args,))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (*args,)\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        batch = list(zip(*samples))\n",
    "\n",
    "        states = torch.cat(batch[0]).to(device)\n",
    "        actions = torch.tensor(batch[1], dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(batch[2], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.cat(batch[3]).to(device)\n",
    "        dones = torch.tensor(batch[4], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 4: Treinamento do Agente Rainbow DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -2780.00, Win Rate: 0.53, Wins: 1265, Losses: 1109, Steps: 36754, Time: 313.17s\n",
      "Ações: Manter=9379, Comprar=11203, Vender=16172\n",
      "Ganhos Totais: 34029.75, Perdas Totais: -36213.25\n",
      "Modelo e log do episódio 1 salvos em: 4.18.1\\model_episode_1.pth e 4.18.1\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -4181.25, Win Rate: 0.52, Wins: 1001, Losses: 933, Steps: 36754, Time: 304.11s\n",
      "Ações: Manter=13734, Comprar=9038, Vender=13982\n",
      "Ganhos Totais: 31559.75, Perdas Totais: -35255.25\n",
      "Modelo e log do episódio 2 salvos em: 4.18.1\\model_episode_2.pth e 4.18.1\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1633.25, Win Rate: 0.53, Wins: 1191, Losses: 1053, Steps: 36754, Time: 294.06s\n",
      "Ações: Manter=12407, Comprar=13166, Vender=11181\n",
      "Ganhos Totais: 33929.50, Perdas Totais: -34997.50\n",
      "Modelo e log do episódio 3 salvos em: 4.18.1\\model_episode_3.pth e 4.18.1\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -1443.25, Win Rate: 0.55, Wins: 1043, Losses: 869, Steps: 36754, Time: 290.28s\n",
      "Ações: Manter=13106, Comprar=15143, Vender=8505\n",
      "Ganhos Totais: 31552.75, Perdas Totais: -32515.25\n",
      "Modelo e log do episódio 4 salvos em: 4.18.1\\model_episode_4.pth e 4.18.1\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -4985.50, Win Rate: 0.53, Wins: 652, Losses: 576, Steps: 36754, Time: 290.51s\n",
      "Ações: Manter=13325, Comprar=9581, Vender=13848\n",
      "Ganhos Totais: 25578.00, Perdas Totais: -30255.75\n",
      "Modelo e log do episódio 5 salvos em: 4.18.1\\model_episode_5.pth e 4.18.1\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 1432.50, Win Rate: 0.54, Wins: 716, Losses: 607, Steps: 36754, Time: 289.48s\n",
      "Ações: Manter=14436, Comprar=17169, Vender=5149\n",
      "Ganhos Totais: 29079.50, Perdas Totais: -27313.50\n",
      "Modelo e log do episódio 6 salvos em: 4.18.1\\model_episode_6.pth e 4.18.1\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: 3001.50, Win Rate: 0.53, Wins: 234, Losses: 206, Steps: 36754, Time: 289.78s\n",
      "Ações: Manter=11722, Comprar=24897, Vender=135\n",
      "Ganhos Totais: 21760.00, Perdas Totais: -18648.50\n",
      "Modelo e log do episódio 7 salvos em: 4.18.1\\model_episode_7.pth e 4.18.1\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 3334.50, Win Rate: 0.52, Wins: 232, Losses: 211, Steps: 36754, Time: 289.89s\n",
      "Ações: Manter=13863, Comprar=22845, Vender=46\n",
      "Ganhos Totais: 23082.00, Perdas Totais: -19636.50\n",
      "Modelo e log do episódio 8 salvos em: 4.18.1\\model_episode_8.pth e 4.18.1\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: 5235.50, Win Rate: 0.54, Wins: 227, Losses: 194, Steps: 36754, Time: 290.12s\n",
      "Ações: Manter=14305, Comprar=22428, Vender=21\n",
      "Ganhos Totais: 23405.25, Perdas Totais: -18064.50\n",
      "Modelo e log do episódio 9 salvos em: 4.18.1\\model_episode_9.pth e 4.18.1\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: 6200.25, Win Rate: 0.56, Wins: 225, Losses: 177, Steps: 36754, Time: 290.70s\n",
      "Ações: Manter=11493, Comprar=25254, Vender=7\n",
      "Ganhos Totais: 23181.75, Perdas Totais: -16881.00\n",
      "Modelo e log do episódio 10 salvos em: 4.18.1\\model_episode_10.pth e 4.18.1\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 5900.25, Win Rate: 0.57, Wins: 228, Losses: 174, Steps: 36754, Time: 291.12s\n",
      "Ações: Manter=13757, Comprar=22993, Vender=4\n",
      "Ganhos Totais: 22771.25, Perdas Totais: -16770.50\n",
      "Modelo e log do episódio 11 salvos em: 4.18.1\\model_episode_11.pth e 4.18.1\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 6175.25, Win Rate: 0.56, Wins: 222, Losses: 175, Steps: 36754, Time: 291.30s\n",
      "Ações: Manter=19578, Comprar=17176, Vender=0\n",
      "Ganhos Totais: 22589.00, Perdas Totais: -16314.50\n",
      "Modelo e log do episódio 12 salvos em: 4.18.1\\model_episode_12.pth e 4.18.1\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 4265.00, Win Rate: 0.54, Wins: 231, Losses: 194, Steps: 36754, Time: 290.77s\n",
      "Ações: Manter=14836, Comprar=21896, Vender=22\n",
      "Ganhos Totais: 22901.50, Perdas Totais: -18530.00\n",
      "Modelo e log do episódio 13 salvos em: 4.18.1\\model_episode_13.pth e 4.18.1\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 3214.25, Win Rate: 0.54, Wins: 217, Losses: 184, Steps: 36754, Time: 293.59s\n",
      "Ações: Manter=20804, Comprar=15923, Vender=27\n",
      "Ganhos Totais: 20403.75, Perdas Totais: -17088.75\n",
      "Modelo e log do episódio 14 salvos em: 4.18.1\\model_episode_14.pth e 4.18.1\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: 4284.75, Win Rate: 0.54, Wins: 209, Losses: 175, Steps: 36754, Time: 290.83s\n",
      "Ações: Manter=18794, Comprar=17958, Vender=2\n",
      "Ganhos Totais: 20497.50, Perdas Totais: -16116.50\n",
      "Modelo e log do episódio 15 salvos em: 4.18.1\\model_episode_15.pth e 4.18.1\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: 4511.25, Win Rate: 0.54, Wins: 233, Losses: 196, Steps: 36754, Time: 292.02s\n",
      "Ações: Manter=315, Comprar=36439, Vender=0\n",
      "Ganhos Totais: 24084.25, Perdas Totais: -19465.75\n",
      "Modelo e log do episódio 16 salvos em: 4.18.1\\model_episode_16.pth e 4.18.1\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: 4519.50, Win Rate: 0.54, Wins: 233, Losses: 196, Steps: 36754, Time: 290.96s\n",
      "Ações: Manter=91, Comprar=36663, Vender=0\n",
      "Ganhos Totais: 24092.50, Perdas Totais: -19465.75\n",
      "Modelo e log do episódio 17 salvos em: 4.18.1\\model_episode_17.pth e 4.18.1\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: 4519.50, Win Rate: 0.54, Wins: 233, Losses: 196, Steps: 36754, Time: 290.64s\n",
      "Ações: Manter=35, Comprar=36719, Vender=0\n",
      "Ganhos Totais: 24092.50, Perdas Totais: -19465.75\n",
      "Modelo e log do episódio 18 salvos em: 4.18.1\\model_episode_18.pth e 4.18.1\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: 4684.25, Win Rate: 0.55, Wins: 232, Losses: 193, Steps: 36754, Time: 290.25s\n",
      "Ações: Manter=11049, Comprar=25705, Vender=0\n",
      "Ganhos Totais: 23687.75, Perdas Totais: -18897.25\n",
      "Modelo e log do episódio 19 salvos em: 4.18.1\\model_episode_19.pth e 4.18.1\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 1102.50, Win Rate: 0.51, Wins: 493, Losses: 465, Steps: 36754, Time: 290.57s\n",
      "Ações: Manter=11171, Comprar=23208, Vender=2375\n",
      "Ganhos Totais: 24671.00, Perdas Totais: -23329.00\n",
      "Episode 21/100, Total Reward: 4302.00, Win Rate: 0.57, Wins: 480, Losses: 358, Steps: 36754, Time: 289.79s\n",
      "Ações: Manter=10315, Comprar=25528, Vender=911\n",
      "Ganhos Totais: 26311.00, Perdas Totais: -21799.00\n",
      "Modelo e log do episódio 21 salvos em: 4.18.1\\model_episode_21.pth e 4.18.1\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -172.25, Win Rate: 0.49, Wins: 573, Losses: 597, Steps: 36754, Time: 290.17s\n",
      "Ações: Manter=740, Comprar=14145, Vender=21869\n",
      "Ganhos Totais: 28231.75, Perdas Totais: -28111.00\n",
      "Episode 23/100, Total Reward: -3721.50, Win Rate: 0.50, Wins: 431, Losses: 435, Steps: 36754, Time: 290.35s\n",
      "Ações: Manter=11733, Comprar=9445, Vender=15576\n",
      "Ganhos Totais: 24492.75, Perdas Totais: -27997.25\n",
      "Episode 24/100, Total Reward: 4304.00, Win Rate: 0.54, Wins: 183, Losses: 153, Steps: 36754, Time: 290.72s\n",
      "Ações: Manter=22738, Comprar=14010, Vender=6\n",
      "Ganhos Totais: 19111.50, Perdas Totais: -14723.25\n",
      "Modelo e log do episódio 24 salvos em: 4.18.1\\model_episode_24.pth e 4.18.1\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 4680.25, Win Rate: 0.54, Wins: 212, Losses: 180, Steps: 36754, Time: 290.41s\n",
      "Ações: Manter=17340, Comprar=19414, Vender=0\n",
      "Ganhos Totais: 21528.00, Perdas Totais: -16749.75\n",
      "Modelo e log do episódio 25 salvos em: 4.18.1\\model_episode_25.pth e 4.18.1\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: 7067.50, Win Rate: 0.55, Wins: 246, Losses: 203, Steps: 36754, Time: 290.62s\n",
      "Ações: Manter=23863, Comprar=10207, Vender=2684\n",
      "Ganhos Totais: 23037.00, Perdas Totais: -15857.25\n",
      "Modelo e log do episódio 26 salvos em: 4.18.1\\model_episode_26.pth e 4.18.1\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: 5460.00, Win Rate: 0.59, Wins: 188, Losses: 133, Steps: 36754, Time: 291.16s\n",
      "Ações: Manter=25177, Comprar=11576, Vender=1\n",
      "Ganhos Totais: 17409.00, Perdas Totais: -11868.75\n",
      "Modelo e log do episódio 27 salvos em: 4.18.1\\model_episode_27.pth e 4.18.1\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: 4803.00, Win Rate: 0.55, Wins: 204, Losses: 168, Steps: 36754, Time: 289.61s\n",
      "Ações: Manter=19979, Comprar=16775, Vender=0\n",
      "Ganhos Totais: 20419.00, Perdas Totais: -15523.00\n",
      "Modelo e log do episódio 28 salvos em: 4.18.1\\model_episode_28.pth e 4.18.1\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 4599.50, Win Rate: 0.59, Wins: 271, Losses: 192, Steps: 36754, Time: 290.55s\n",
      "Ações: Manter=19182, Comprar=17481, Vender=91\n",
      "Ganhos Totais: 21136.25, Perdas Totais: -16420.75\n",
      "Modelo e log do episódio 29 salvos em: 4.18.1\\model_episode_29.pth e 4.18.1\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: 3470.75, Win Rate: 0.58, Wins: 297, Losses: 218, Steps: 36754, Time: 289.76s\n",
      "Ações: Manter=12255, Comprar=24399, Vender=100\n",
      "Ganhos Totais: 23193.75, Perdas Totais: -19594.25\n",
      "Episode 31/100, Total Reward: 4815.25, Win Rate: 0.59, Wins: 285, Losses: 198, Steps: 36754, Time: 290.08s\n",
      "Ações: Manter=13117, Comprar=23552, Vender=85\n",
      "Ganhos Totais: 23150.75, Perdas Totais: -18214.75\n",
      "Modelo e log do episódio 31 salvos em: 4.18.1\\model_episode_31.pth e 4.18.1\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 4104.50, Win Rate: 0.56, Wins: 271, Losses: 210, Steps: 36754, Time: 290.23s\n",
      "Ações: Manter=16371, Comprar=20282, Vender=101\n",
      "Ganhos Totais: 22507.00, Perdas Totais: -18282.25\n",
      "Episode 33/100, Total Reward: 4525.50, Win Rate: 0.58, Wins: 275, Losses: 203, Steps: 36754, Time: 291.15s\n",
      "Ações: Manter=13784, Comprar=22881, Vender=89\n",
      "Ganhos Totais: 23072.00, Perdas Totais: -18427.00\n",
      "Episode 34/100, Total Reward: 4434.50, Win Rate: 0.60, Wins: 301, Losses: 202, Steps: 36754, Time: 290.85s\n",
      "Ações: Manter=14021, Comprar=22618, Vender=115\n",
      "Ganhos Totais: 23028.25, Perdas Totais: -18468.00\n",
      "Episode 35/100, Total Reward: 2816.25, Win Rate: 0.56, Wins: 262, Losses: 207, Steps: 36754, Time: 290.91s\n",
      "Ações: Manter=16631, Comprar=20050, Vender=73\n",
      "Ganhos Totais: 22305.25, Perdas Totais: -19371.50\n",
      "Episode 36/100, Total Reward: 3661.00, Win Rate: 0.57, Wins: 281, Losses: 211, Steps: 36754, Time: 291.14s\n",
      "Ações: Manter=12935, Comprar=23738, Vender=81\n",
      "Ganhos Totais: 23953.50, Perdas Totais: -20169.50\n",
      "Episode 37/100, Total Reward: 4039.75, Win Rate: 0.58, Wins: 291, Losses: 211, Steps: 36754, Time: 290.74s\n",
      "Ações: Manter=13518, Comprar=23152, Vender=84\n",
      "Ganhos Totais: 23999.50, Perdas Totais: -19834.25\n",
      "Episode 38/100, Total Reward: 2414.50, Win Rate: 0.57, Wins: 431, Losses: 319, Steps: 36754, Time: 290.90s\n",
      "Ações: Manter=13467, Comprar=22790, Vender=497\n",
      "Ganhos Totais: 24978.25, Perdas Totais: -22375.75\n",
      "Episode 39/100, Total Reward: 3892.25, Win Rate: 0.55, Wins: 455, Losses: 378, Steps: 36754, Time: 291.39s\n",
      "Ações: Manter=15644, Comprar=16921, Vender=4189\n",
      "Ganhos Totais: 26737.75, Perdas Totais: -22637.00\n",
      "Episode 40/100, Total Reward: 3839.75, Win Rate: 0.52, Wins: 378, Losses: 345, Steps: 36754, Time: 291.46s\n",
      "Ações: Manter=8314, Comprar=15575, Vender=12865\n",
      "Ganhos Totais: 26121.75, Perdas Totais: -22100.50\n",
      "Episode 41/100, Total Reward: 3028.00, Win Rate: 0.52, Wins: 379, Losses: 346, Steps: 36754, Time: 291.03s\n",
      "Ações: Manter=19037, Comprar=9197, Vender=8520\n",
      "Ganhos Totais: 24207.50, Perdas Totais: -20997.75\n",
      "Episode 42/100, Total Reward: 3481.25, Win Rate: 0.52, Wins: 436, Losses: 401, Steps: 36754, Time: 291.28s\n",
      "Ações: Manter=11822, Comprar=19224, Vender=5708\n",
      "Ganhos Totais: 27251.75, Perdas Totais: -23561.00\n",
      "Episode 43/100, Total Reward: 4890.75, Win Rate: 0.52, Wins: 467, Losses: 427, Steps: 36754, Time: 291.00s\n",
      "Ações: Manter=7607, Comprar=17586, Vender=11561\n",
      "Ganhos Totais: 28297.50, Perdas Totais: -23182.25\n",
      "Modelo e log do episódio 43 salvos em: 4.18.1\\model_episode_43.pth e 4.18.1\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: 1943.00, Win Rate: 0.53, Wins: 435, Losses: 391, Steps: 36754, Time: 290.74s\n",
      "Ações: Manter=7168, Comprar=17134, Vender=12452\n",
      "Ganhos Totais: 26435.25, Perdas Totais: -24285.00\n",
      "Episode 45/100, Total Reward: 3616.75, Win Rate: 0.52, Wins: 391, Losses: 366, Steps: 36754, Time: 291.08s\n",
      "Ações: Manter=5358, Comprar=18189, Vender=13207\n",
      "Ganhos Totais: 27317.25, Perdas Totais: -23511.00\n",
      "Episode 46/100, Total Reward: 1903.50, Win Rate: 0.52, Wins: 363, Losses: 335, Steps: 36754, Time: 291.18s\n",
      "Ações: Manter=11277, Comprar=18860, Vender=6617\n",
      "Ganhos Totais: 25120.75, Perdas Totais: -23042.50\n",
      "Episode 47/100, Total Reward: 3027.50, Win Rate: 0.54, Wins: 357, Losses: 303, Steps: 36754, Time: 290.92s\n",
      "Ações: Manter=12557, Comprar=21943, Vender=2254\n",
      "Ganhos Totais: 24173.00, Perdas Totais: -20980.25\n",
      "Episode 48/100, Total Reward: 4667.50, Win Rate: 0.49, Wins: 399, Losses: 417, Steps: 36754, Time: 291.35s\n",
      "Ações: Manter=15257, Comprar=15844, Vender=5653\n",
      "Ganhos Totais: 25733.75, Perdas Totais: -20861.25\n",
      "Episode 49/100, Total Reward: 5428.50, Win Rate: 0.55, Wins: 302, Losses: 247, Steps: 36754, Time: 291.01s\n",
      "Ações: Manter=12444, Comprar=24037, Vender=273\n",
      "Ganhos Totais: 25700.75, Perdas Totais: -20134.75\n",
      "Modelo e log do episódio 49 salvos em: 4.18.1\\model_episode_49.pth e 4.18.1\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: 3628.25, Win Rate: 0.51, Wins: 356, Losses: 348, Steps: 36754, Time: 291.40s\n",
      "Ações: Manter=14779, Comprar=17818, Vender=4157\n",
      "Ganhos Totais: 25327.25, Perdas Totais: -21522.75\n",
      "Episode 51/100, Total Reward: 5276.75, Win Rate: 0.53, Wins: 448, Losses: 404, Steps: 36754, Time: 291.63s\n",
      "Ações: Manter=11641, Comprar=19119, Vender=5994\n",
      "Ganhos Totais: 28405.00, Perdas Totais: -22915.00\n",
      "Modelo e log do episódio 51 salvos em: 4.18.1\\model_episode_51.pth e 4.18.1\\log_episode_51.csv\n",
      "\n",
      "Episode 52/100, Total Reward: 3138.25, Win Rate: 0.49, Wins: 363, Losses: 385, Steps: 36754, Time: 291.12s\n",
      "Ações: Manter=12925, Comprar=16389, Vender=7440\n",
      "Ganhos Totais: 25378.25, Perdas Totais: -22052.75\n",
      "Episode 53/100, Total Reward: 2893.75, Win Rate: 0.50, Wins: 408, Losses: 408, Steps: 36754, Time: 291.27s\n",
      "Ações: Manter=8575, Comprar=17720, Vender=10459\n",
      "Ganhos Totais: 27185.25, Perdas Totais: -24086.75\n",
      "Episode 54/100, Total Reward: 2154.00, Win Rate: 0.51, Wins: 377, Losses: 359, Steps: 36754, Time: 291.73s\n",
      "Ações: Manter=11142, Comprar=20845, Vender=4767\n",
      "Ganhos Totais: 25960.00, Perdas Totais: -23621.75\n",
      "Episode 55/100, Total Reward: 3556.50, Win Rate: 0.50, Wins: 372, Losses: 366, Steps: 36754, Time: 290.78s\n",
      "Ações: Manter=9955, Comprar=20129, Vender=6670\n",
      "Ganhos Totais: 26447.00, Perdas Totais: -22705.75\n",
      "Episode 56/100, Total Reward: 4408.00, Win Rate: 0.57, Wins: 446, Losses: 336, Steps: 36754, Time: 291.40s\n",
      "Ações: Manter=10390, Comprar=24701, Vender=1663\n",
      "Ganhos Totais: 26679.25, Perdas Totais: -22075.50\n",
      "Episode 57/100, Total Reward: 3494.25, Win Rate: 0.57, Wins: 321, Losses: 247, Steps: 36754, Time: 290.72s\n",
      "Ações: Manter=6311, Comprar=30268, Vender=175\n",
      "Ganhos Totais: 24885.50, Perdas Totais: -21249.00\n",
      "Episode 58/100, Total Reward: 6833.25, Win Rate: 0.51, Wins: 448, Losses: 427, Steps: 36754, Time: 290.93s\n",
      "Ações: Manter=15529, Comprar=16783, Vender=4442\n",
      "Ganhos Totais: 25983.50, Perdas Totais: -18930.25\n",
      "Modelo e log do episódio 58 salvos em: 4.18.1\\model_episode_58.pth e 4.18.1\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: 4829.75, Win Rate: 0.52, Wins: 373, Losses: 340, Steps: 36754, Time: 290.90s\n",
      "Ações: Manter=16988, Comprar=16999, Vender=2767\n",
      "Ganhos Totais: 24068.75, Perdas Totais: -19060.25\n",
      "Episode 60/100, Total Reward: 5987.75, Win Rate: 0.60, Wins: 285, Losses: 191, Steps: 36754, Time: 290.65s\n",
      "Ações: Manter=16118, Comprar=20520, Vender=116\n",
      "Ganhos Totais: 21923.50, Perdas Totais: -15816.75\n",
      "Modelo e log do episódio 60 salvos em: 4.18.1\\model_episode_60.pth e 4.18.1\\log_episode_60.csv\n",
      "\n",
      "Episode 61/100, Total Reward: 1738.75, Win Rate: 0.53, Wins: 436, Losses: 386, Steps: 36754, Time: 290.80s\n",
      "Ações: Manter=9673, Comprar=14327, Vender=12754\n",
      "Ganhos Totais: 26341.75, Perdas Totais: -24397.00\n",
      "Episode 62/100, Total Reward: 657.75, Win Rate: 0.50, Wins: 364, Losses: 370, Steps: 36754, Time: 291.33s\n",
      "Ações: Manter=13006, Comprar=12302, Vender=11446\n",
      "Ganhos Totais: 23289.50, Perdas Totais: -22447.75\n",
      "Episode 63/100, Total Reward: 4144.75, Win Rate: 0.52, Wins: 354, Losses: 329, Steps: 36754, Time: 292.11s\n",
      "Ações: Manter=13453, Comprar=19260, Vender=4041\n",
      "Ganhos Totais: 24225.25, Perdas Totais: -19909.50\n",
      "Episode 64/100, Total Reward: 5277.50, Win Rate: 0.51, Wins: 326, Losses: 308, Steps: 36754, Time: 290.97s\n",
      "Ações: Manter=10609, Comprar=20354, Vender=5791\n",
      "Ganhos Totais: 24597.00, Perdas Totais: -19160.25\n",
      "Modelo e log do episódio 64 salvos em: 4.18.1\\model_episode_64.pth e 4.18.1\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: 3957.75, Win Rate: 0.49, Wins: 483, Losses: 506, Steps: 36754, Time: 290.86s\n",
      "Ações: Manter=10531, Comprar=17188, Vender=9035\n",
      "Ganhos Totais: 28412.75, Perdas Totais: -24207.00\n",
      "Episode 66/100, Total Reward: 6129.75, Win Rate: 0.56, Wins: 218, Losses: 172, Steps: 36754, Time: 291.29s\n",
      "Ações: Manter=14413, Comprar=22331, Vender=10\n",
      "Ganhos Totais: 21441.75, Perdas Totais: -15214.50\n",
      "Modelo e log do episódio 66 salvos em: 4.18.1\\model_episode_66.pth e 4.18.1\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: 7069.75, Win Rate: 0.51, Wins: 313, Losses: 304, Steps: 36754, Time: 290.87s\n",
      "Ações: Manter=10602, Comprar=21622, Vender=4530\n",
      "Ganhos Totais: 25903.75, Perdas Totais: -18679.25\n",
      "Modelo e log do episódio 67 salvos em: 4.18.1\\model_episode_67.pth e 4.18.1\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: 3814.50, Win Rate: 0.48, Wins: 337, Losses: 363, Steps: 36754, Time: 290.31s\n",
      "Ações: Manter=10230, Comprar=22369, Vender=4155\n",
      "Ganhos Totais: 25692.00, Perdas Totais: -21702.00\n",
      "Episode 69/100, Total Reward: 2799.00, Win Rate: 0.50, Wins: 419, Losses: 414, Steps: 36754, Time: 291.19s\n",
      "Ações: Manter=7505, Comprar=18003, Vender=11246\n",
      "Ganhos Totais: 27331.25, Perdas Totais: -24323.50\n",
      "Episode 70/100, Total Reward: 3354.25, Win Rate: 0.47, Wins: 427, Losses: 476, Steps: 36754, Time: 291.75s\n",
      "Ações: Manter=10830, Comprar=18971, Vender=6953\n",
      "Ganhos Totais: 25660.50, Perdas Totais: -22079.75\n",
      "Episode 71/100, Total Reward: 2154.50, Win Rate: 0.50, Wins: 423, Losses: 429, Steps: 36754, Time: 290.69s\n",
      "Ações: Manter=7441, Comprar=20199, Vender=9114\n",
      "Ganhos Totais: 26816.00, Perdas Totais: -24447.75\n",
      "Episode 72/100, Total Reward: 5332.50, Win Rate: 0.54, Wins: 277, Losses: 240, Steps: 36754, Time: 291.01s\n",
      "Ações: Manter=13639, Comprar=22072, Vender=1043\n",
      "Ganhos Totais: 22630.75, Perdas Totais: -17169.00\n",
      "Episode 73/100, Total Reward: 4799.50, Win Rate: 0.54, Wins: 219, Losses: 189, Steps: 36754, Time: 291.41s\n",
      "Ações: Manter=15571, Comprar=20185, Vender=998\n",
      "Ganhos Totais: 20639.00, Perdas Totais: -15737.50\n",
      "Episode 74/100, Total Reward: 1786.50, Win Rate: 0.49, Wins: 254, Losses: 267, Steps: 36754, Time: 291.55s\n",
      "Ações: Manter=8468, Comprar=23989, Vender=4297\n",
      "Ganhos Totais: 22780.25, Perdas Totais: -20863.25\n",
      "Episode 75/100, Total Reward: 848.00, Win Rate: 0.44, Wins: 299, Losses: 374, Steps: 36754, Time: 291.12s\n",
      "Ações: Manter=12429, Comprar=15151, Vender=9174\n",
      "Ganhos Totais: 25094.00, Perdas Totais: -24077.00\n",
      "Episode 76/100, Total Reward: -147.75, Win Rate: 0.48, Wins: 409, Losses: 446, Steps: 36754, Time: 291.55s\n",
      "Ações: Manter=11350, Comprar=16104, Vender=9300\n",
      "Ganhos Totais: 24222.50, Perdas Totais: -24156.00\n",
      "Episode 77/100, Total Reward: 3181.25, Win Rate: 0.50, Wins: 386, Losses: 380, Steps: 36754, Time: 290.80s\n",
      "Ações: Manter=9665, Comprar=19327, Vender=7762\n",
      "Ganhos Totais: 25922.50, Perdas Totais: -22549.25\n",
      "Episode 78/100, Total Reward: 5400.25, Win Rate: 0.51, Wins: 242, Losses: 230, Steps: 36754, Time: 289.99s\n",
      "Ações: Manter=14451, Comprar=20962, Vender=1341\n",
      "Ganhos Totais: 22258.00, Perdas Totais: -16739.50\n",
      "Episode 79/100, Total Reward: 5890.00, Win Rate: 0.53, Wins: 280, Losses: 253, Steps: 36754, Time: 290.42s\n",
      "Ações: Manter=6441, Comprar=28655, Vender=1658\n",
      "Ganhos Totais: 25434.25, Perdas Totais: -19410.75\n",
      "Modelo e log do episódio 79 salvos em: 4.18.1\\model_episode_79.pth e 4.18.1\\log_episode_79.csv\n",
      "\n",
      "Episode 80/100, Total Reward: 7001.25, Win Rate: 0.54, Wins: 268, Losses: 228, Steps: 36754, Time: 290.28s\n",
      "Ações: Manter=12936, Comprar=23094, Vender=724\n",
      "Ganhos Totais: 23623.50, Perdas Totais: -16498.25\n",
      "Modelo e log do episódio 80 salvos em: 4.18.1\\model_episode_80.pth e 4.18.1\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: 975.75, Win Rate: 0.49, Wins: 345, Losses: 353, Steps: 36754, Time: 290.63s\n",
      "Ações: Manter=5192, Comprar=16616, Vender=14946\n",
      "Ganhos Totais: 24718.00, Perdas Totais: -23566.75\n",
      "Episode 82/100, Total Reward: 1657.00, Win Rate: 0.48, Wins: 231, Losses: 255, Steps: 36754, Time: 290.25s\n",
      "Ações: Manter=17016, Comprar=18564, Vender=1174\n",
      "Ganhos Totais: 20741.25, Perdas Totais: -18962.50\n",
      "Episode 83/100, Total Reward: 1869.25, Win Rate: 0.51, Wins: 513, Losses: 490, Steps: 36754, Time: 290.65s\n",
      "Ações: Manter=12601, Comprar=16891, Vender=7262\n",
      "Ganhos Totais: 27690.75, Perdas Totais: -25570.25\n",
      "Episode 84/100, Total Reward: 6459.75, Win Rate: 0.50, Wins: 491, Losses: 490, Steps: 36754, Time: 290.89s\n",
      "Ações: Manter=7974, Comprar=19278, Vender=9502\n",
      "Ganhos Totais: 29631.25, Perdas Totais: -22924.75\n",
      "Modelo e log do episódio 84 salvos em: 4.18.1\\model_episode_84.pth e 4.18.1\\log_episode_84.csv\n",
      "\n",
      "Episode 85/100, Total Reward: 912.00, Win Rate: 0.51, Wins: 381, Losses: 363, Steps: 36754, Time: 290.73s\n",
      "Ações: Manter=8709, Comprar=15198, Vender=12847\n",
      "Ganhos Totais: 24409.00, Perdas Totais: -23311.00\n",
      "Episode 86/100, Total Reward: 5849.75, Win Rate: 0.56, Wins: 445, Losses: 351, Steps: 36754, Time: 290.78s\n",
      "Ações: Manter=14002, Comprar=18033, Vender=4719\n",
      "Ganhos Totais: 27533.50, Perdas Totais: -21484.75\n",
      "Episode 87/100, Total Reward: 6045.50, Win Rate: 0.57, Wins: 978, Losses: 723, Steps: 36754, Time: 293.24s\n",
      "Ações: Manter=7658, Comprar=24956, Vender=4140\n",
      "Ganhos Totais: 34246.75, Perdas Totais: -27772.50\n",
      "Modelo e log do episódio 87 salvos em: 4.18.1\\model_episode_87.pth e 4.18.1\\log_episode_87.csv\n",
      "\n",
      "Episode 88/100, Total Reward: 9368.75, Win Rate: 0.59, Wins: 1279, Losses: 883, Steps: 36754, Time: 290.81s\n",
      "Ações: Manter=4278, Comprar=25447, Vender=7029\n",
      "Ganhos Totais: 39076.50, Perdas Totais: -29164.50\n",
      "Modelo e log do episódio 88 salvos em: 4.18.1\\model_episode_88.pth e 4.18.1\\log_episode_88.csv\n",
      "\n",
      "Episode 89/100, Total Reward: 5746.00, Win Rate: 0.59, Wins: 1331, Losses: 922, Steps: 36754, Time: 290.75s\n",
      "Ações: Manter=5550, Comprar=22225, Vender=8979\n",
      "Ganhos Totais: 35095.50, Perdas Totais: -28782.00\n",
      "Episode 90/100, Total Reward: 3765.50, Win Rate: 0.56, Wins: 955, Losses: 743, Steps: 36754, Time: 290.91s\n",
      "Ações: Manter=16304, Comprar=14055, Vender=6395\n",
      "Ganhos Totais: 30870.00, Perdas Totais: -26678.00\n",
      "Episode 91/100, Total Reward: 1776.25, Win Rate: 0.58, Wins: 889, Losses: 657, Steps: 36754, Time: 291.36s\n",
      "Ações: Manter=12301, Comprar=17586, Vender=6867\n",
      "Ganhos Totais: 28853.75, Perdas Totais: -26689.50\n",
      "Episode 92/100, Total Reward: 4500.25, Win Rate: 0.57, Wins: 1076, Losses: 819, Steps: 36754, Time: 291.68s\n",
      "Ações: Manter=14074, Comprar=14395, Vender=8285\n",
      "Ganhos Totais: 31497.25, Perdas Totais: -26521.50\n",
      "Episode 93/100, Total Reward: 5005.25, Win Rate: 0.59, Wins: 1705, Losses: 1166, Steps: 36754, Time: 290.89s\n",
      "Ações: Manter=12785, Comprar=16081, Vender=7888\n",
      "Ganhos Totais: 38137.00, Perdas Totais: -32409.75\n",
      "Episode 94/100, Total Reward: 6405.75, Win Rate: 0.59, Wins: 1578, Losses: 1108, Steps: 36754, Time: 290.72s\n",
      "Ações: Manter=6654, Comprar=18388, Vender=11712\n",
      "Ganhos Totais: 38081.00, Perdas Totais: -30998.25\n",
      "Modelo e log do episódio 94 salvos em: 4.18.1\\model_episode_94.pth e 4.18.1\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: 797.25, Win Rate: 0.58, Wins: 1234, Losses: 900, Steps: 36754, Time: 291.01s\n",
      "Ações: Manter=10858, Comprar=16944, Vender=8952\n",
      "Ganhos Totais: 32647.25, Perdas Totais: -31313.50\n",
      "Episode 96/100, Total Reward: 4508.75, Win Rate: 0.60, Wins: 1064, Losses: 712, Steps: 36754, Time: 291.20s\n",
      "Ações: Manter=13948, Comprar=15576, Vender=7230\n",
      "Ganhos Totais: 32690.75, Perdas Totais: -27734.50\n",
      "Episode 97/100, Total Reward: 4892.75, Win Rate: 0.59, Wins: 1166, Losses: 816, Steps: 36754, Time: 290.77s\n",
      "Ações: Manter=10707, Comprar=13943, Vender=12104\n",
      "Ganhos Totais: 33847.25, Perdas Totais: -28457.25\n",
      "Episode 98/100, Total Reward: 4255.00, Win Rate: 0.55, Wins: 442, Losses: 358, Steps: 36754, Time: 291.77s\n",
      "Ações: Manter=22626, Comprar=6693, Vender=7435\n",
      "Ganhos Totais: 24498.50, Perdas Totais: -20042.50\n",
      "Episode 99/100, Total Reward: 4810.75, Win Rate: 0.59, Wins: 729, Losses: 508, Steps: 36754, Time: 286.76s\n",
      "Ações: Manter=14058, Comprar=12125, Vender=10571\n",
      "Ganhos Totais: 28436.00, Perdas Totais: -23314.50\n",
      "Episode 100/100, Total Reward: 3098.25, Win Rate: 0.57, Wins: 586, Losses: 450, Steps: 36754, Time: 277.03s\n",
      "Ações: Manter=18655, Comprar=7021, Vender=11078\n",
      "Ganhos Totais: 26654.50, Perdas Totais: -23296.50\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 88, Total Reward: 9368.75, Win Rate: 0.59, Wins: 1279, Losses: 883, Ações: {0: 4278, 1: 25447, 2: 7029}, Steps: 36754, Time: 290.81s\n",
      "Rank 2: Episode 67, Total Reward: 7069.75, Win Rate: 0.51, Wins: 313, Losses: 304, Ações: {0: 10602, 1: 21622, 2: 4530}, Steps: 36754, Time: 290.87s\n",
      "Rank 3: Episode 26, Total Reward: 7067.50, Win Rate: 0.55, Wins: 246, Losses: 203, Ações: {0: 23863, 1: 10207, 2: 2684}, Steps: 36754, Time: 290.62s\n",
      "Rank 4: Episode 80, Total Reward: 7001.25, Win Rate: 0.54, Wins: 268, Losses: 228, Ações: {0: 12936, 1: 23094, 2: 724}, Steps: 36754, Time: 290.28s\n",
      "Rank 5: Episode 58, Total Reward: 6833.25, Win Rate: 0.51, Wins: 448, Losses: 427, Ações: {0: 15529, 1: 16783, 2: 4442}, Steps: 36754, Time: 290.93s\n",
      "Rank 6: Episode 84, Total Reward: 6459.75, Win Rate: 0.50, Wins: 491, Losses: 490, Ações: {0: 7974, 1: 19278, 2: 9502}, Steps: 36754, Time: 290.89s\n",
      "Rank 7: Episode 94, Total Reward: 6405.75, Win Rate: 0.59, Wins: 1578, Losses: 1108, Ações: {0: 6654, 1: 18388, 2: 11712}, Steps: 36754, Time: 290.72s\n",
      "Rank 8: Episode 10, Total Reward: 6200.25, Win Rate: 0.56, Wins: 225, Losses: 177, Ações: {0: 11493, 1: 25254, 2: 7}, Steps: 36754, Time: 290.70s\n",
      "Rank 9: Episode 12, Total Reward: 6175.25, Win Rate: 0.56, Wins: 222, Losses: 175, Ações: {0: 19578, 1: 17176, 2: 0}, Steps: 36754, Time: 291.30s\n",
      "Rank 10: Episode 66, Total Reward: 6129.75, Win Rate: 0.56, Wins: 218, Losses: 172, Ações: {0: 14413, 1: 22331, 2: 10}, Steps: 36754, Time: 291.29s\n"
     ]
    }
   ],
   "source": [
    "# Configurações do treinamento\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "memory_size = 10000\n",
    "target_update = 1000\n",
    "beta_start = 0.4\n",
    "beta_frames = num_episodes * len(data)\n",
    "alpha = 0.6\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Inicializar o Prioritized Replay Buffer\n",
    "replay_buffer = PrioritizedReplayBuffer(memory_size, alpha=alpha)\n",
    "\n",
    "# Inicializar a lista de melhores episódios\n",
    "best_episodes = []\n",
    "\n",
    "# Função para selecionar ação usando Noisy Nets\n",
    "def select_action(state):\n",
    "    with torch.no_grad():\n",
    "        q_values = q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "save_dir = \"4.18.1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "beta = beta_start\n",
    "frame_idx = 0  # Contador de frames para ajustar beta\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        replay_buffer.push(obs, action, reward, obs_next, done)\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Resetar ruído das Noisy Nets\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(replay_buffer.buffer) >= batch_size:\n",
    "            beta = min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "            states, actions_batch, rewards_batch, next_states, dones, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando Double DQN\n",
    "            with torch.no_grad():\n",
    "                next_actions = q_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q_values = target_net(next_states).gather(1, next_actions)\n",
    "                target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular o erro para Prioritized Replay\n",
    "            td_errors = (q_values - target_q_values).detach().cpu().numpy().flatten()\n",
    "            new_priorities = np.abs(td_errors) + 1e-6\n",
    "            replay_buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "            # Calcular a perda ponderada\n",
    "            loss = (weights * nn.functional.mse_loss(q_values, target_q_values, reduction='none')).mean()\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Atualizar a rede alvo\n",
    "            if frame_idx % target_update == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista dos melhores e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log do episódio se for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "# Exibir os top 10 episódios ao final do treinamento\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
