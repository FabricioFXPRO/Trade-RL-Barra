{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -1106.50, Win Rate: 0.30, Wins: 898, Losses: 2090, Epsilon: 0.4950, Steps: 36754, Time: 116.00s\n",
      "Ações: Manter=12043, Comprar=11985, Vender=12726\n",
      "Ganhos Totais: 8768.00, Perdas Totais: -9874.50\n",
      "\n",
      "Modelo e log do episódio 1 salvos em: 4.5.1\\model_episode_1.pth e 4.5.1\\log_episode_1.csv\n",
      "Episode 2/100, Total Reward: 635.00, Win Rate: 0.32, Wins: 956, Losses: 2012, Epsilon: 0.4900, Steps: 36754, Time: 132.10s\n",
      "Ações: Manter=11516, Comprar=12360, Vender=12878\n",
      "Ganhos Totais: 9673.00, Perdas Totais: -9038.00\n",
      "\n",
      "Modelo e log do episódio 2 salvos em: 4.5.1\\model_episode_2.pth e 4.5.1\\log_episode_2.csv\n",
      "Episode 3/100, Total Reward: -1324.25, Win Rate: 0.29, Wins: 849, Losses: 2071, Epsilon: 0.4851, Steps: 36754, Time: 124.71s\n",
      "Ações: Manter=11561, Comprar=12653, Vender=12540\n",
      "Ganhos Totais: 8496.25, Perdas Totais: -9820.50\n",
      "\n",
      "Modelo e log do episódio 3 salvos em: 4.5.1\\model_episode_3.pth e 4.5.1\\log_episode_3.csv\n",
      "Episode 4/100, Total Reward: -745.25, Win Rate: 0.32, Wins: 878, Losses: 1875, Epsilon: 0.4803, Steps: 36754, Time: 125.68s\n",
      "Ações: Manter=12768, Comprar=12444, Vender=11542\n",
      "Ganhos Totais: 8421.75, Perdas Totais: -9167.00\n",
      "\n",
      "Modelo e log do episódio 4 salvos em: 4.5.1\\model_episode_4.pth e 4.5.1\\log_episode_4.csv\n",
      "Episode 5/100, Total Reward: -1648.75, Win Rate: 0.31, Wins: 850, Losses: 1858, Epsilon: 0.4755, Steps: 36754, Time: 125.90s\n",
      "Ações: Manter=12862, Comprar=12911, Vender=10981\n",
      "Ganhos Totais: 7809.75, Perdas Totais: -9458.50\n",
      "\n",
      "Modelo e log do episódio 5 salvos em: 4.5.1\\model_episode_5.pth e 4.5.1\\log_episode_5.csv\n",
      "Episode 6/100, Total Reward: -422.00, Win Rate: 0.33, Wins: 898, Losses: 1854, Epsilon: 0.4707, Steps: 36754, Time: 124.81s\n",
      "Ações: Manter=12501, Comprar=12472, Vender=11781\n",
      "Ganhos Totais: 8726.50, Perdas Totais: -9148.50\n",
      "\n",
      "Modelo e log do episódio 6 salvos em: 4.5.1\\model_episode_6.pth e 4.5.1\\log_episode_6.csv\n",
      "Episode 7/100, Total Reward: -888.75, Win Rate: 0.33, Wins: 893, Losses: 1847, Epsilon: 0.4660, Steps: 36754, Time: 123.93s\n",
      "Ações: Manter=12413, Comprar=12804, Vender=11537\n",
      "Ganhos Totais: 8485.00, Perdas Totais: -9373.75\n",
      "\n",
      "Modelo e log do episódio 7 salvos em: 4.5.1\\model_episode_7.pth e 4.5.1\\log_episode_7.csv\n",
      "Episode 8/100, Total Reward: -4.50, Win Rate: 0.31, Wins: 865, Losses: 1882, Epsilon: 0.4614, Steps: 36754, Time: 123.93s\n",
      "Ações: Manter=12001, Comprar=12820, Vender=11933\n",
      "Ganhos Totais: 8844.25, Perdas Totais: -8848.75\n",
      "\n",
      "Modelo e log do episódio 8 salvos em: 4.5.1\\model_episode_8.pth e 4.5.1\\log_episode_8.csv\n",
      "Episode 9/100, Total Reward: -203.50, Win Rate: 0.31, Wins: 824, Losses: 1829, Epsilon: 0.4568, Steps: 36754, Time: 123.45s\n",
      "Ações: Manter=12825, Comprar=11734, Vender=12195\n",
      "Ganhos Totais: 8688.50, Perdas Totais: -8892.00\n",
      "\n",
      "Modelo e log do episódio 9 salvos em: 4.5.1\\model_episode_9.pth e 4.5.1\\log_episode_9.csv\n",
      "Episode 10/100, Total Reward: 168.25, Win Rate: 0.34, Wins: 916, Losses: 1765, Epsilon: 0.4522, Steps: 36754, Time: 123.56s\n",
      "Ações: Manter=12378, Comprar=12569, Vender=11807\n",
      "Ganhos Totais: 8667.00, Perdas Totais: -8498.75\n",
      "\n",
      "Modelo e log do episódio 10 salvos em: 4.5.1\\model_episode_10.pth e 4.5.1\\log_episode_10.csv\n",
      "Episode 11/100, Total Reward: -76.00, Win Rate: 0.33, Wins: 857, Losses: 1774, Epsilon: 0.4477, Steps: 36754, Time: 124.35s\n",
      "Ações: Manter=11026, Comprar=13149, Vender=12579\n",
      "Ganhos Totais: 8116.75, Perdas Totais: -8192.75\n",
      "\n",
      "Modelo e log do episódio 11 salvos em: 4.5.1\\model_episode_11.pth e 4.5.1\\log_episode_11.csv\n",
      "Episode 12/100, Total Reward: -1772.25, Win Rate: 0.32, Wins: 871, Losses: 1823, Epsilon: 0.4432, Steps: 36754, Time: 125.00s\n",
      "Ações: Manter=12317, Comprar=12992, Vender=11445\n",
      "Ganhos Totais: 8102.25, Perdas Totais: -9874.50\n",
      "\n",
      "Episode 13/100, Total Reward: 78.50, Win Rate: 0.31, Wins: 808, Losses: 1765, Epsilon: 0.4388, Steps: 36754, Time: 127.44s\n",
      "Ações: Manter=10928, Comprar=12543, Vender=13283\n",
      "Ganhos Totais: 8377.25, Perdas Totais: -8298.75\n",
      "\n",
      "Modelo e log do episódio 13 salvos em: 4.5.1\\model_episode_13.pth e 4.5.1\\log_episode_13.csv\n",
      "Episode 14/100, Total Reward: -1063.00, Win Rate: 0.29, Wins: 759, Losses: 1820, Epsilon: 0.4344, Steps: 36754, Time: 135.61s\n",
      "Ações: Manter=10944, Comprar=10929, Vender=14881\n",
      "Ganhos Totais: 7849.50, Perdas Totais: -8912.50\n",
      "\n",
      "Modelo e log do episódio 14 salvos em: 4.5.1\\model_episode_14.pth e 4.5.1\\log_episode_14.csv\n",
      "Episode 15/100, Total Reward: 128.00, Win Rate: 0.33, Wins: 884, Losses: 1825, Epsilon: 0.4300, Steps: 36754, Time: 128.30s\n",
      "Ações: Manter=12115, Comprar=12658, Vender=11981\n",
      "Ganhos Totais: 8774.75, Perdas Totais: -8646.75\n",
      "\n",
      "Modelo e log do episódio 15 salvos em: 4.5.1\\model_episode_15.pth e 4.5.1\\log_episode_15.csv\n",
      "Episode 16/100, Total Reward: -55.00, Win Rate: 0.33, Wins: 881, Losses: 1828, Epsilon: 0.4257, Steps: 36754, Time: 127.74s\n",
      "Ações: Manter=11628, Comprar=12807, Vender=12319\n",
      "Ganhos Totais: 8776.50, Perdas Totais: -8831.50\n",
      "\n",
      "Modelo e log do episódio 16 salvos em: 4.5.1\\model_episode_16.pth e 4.5.1\\log_episode_16.csv\n",
      "Episode 17/100, Total Reward: -497.50, Win Rate: 0.33, Wins: 872, Losses: 1800, Epsilon: 0.4215, Steps: 36754, Time: 130.53s\n",
      "Ações: Manter=10908, Comprar=12889, Vender=12957\n",
      "Ganhos Totais: 8313.75, Perdas Totais: -8811.25\n",
      "\n",
      "Modelo e log do episódio 17 salvos em: 4.5.1\\model_episode_17.pth e 4.5.1\\log_episode_17.csv\n",
      "Episode 18/100, Total Reward: -1361.25, Win Rate: 0.33, Wins: 866, Losses: 1798, Epsilon: 0.4173, Steps: 36754, Time: 129.78s\n",
      "Ações: Manter=12178, Comprar=13418, Vender=11158\n",
      "Ganhos Totais: 8139.25, Perdas Totais: -9500.50\n",
      "\n",
      "Episode 19/100, Total Reward: -390.00, Win Rate: 0.32, Wins: 853, Losses: 1795, Epsilon: 0.4131, Steps: 36754, Time: 131.63s\n",
      "Ações: Manter=13056, Comprar=10946, Vender=12752\n",
      "Ganhos Totais: 8604.50, Perdas Totais: -8994.50\n",
      "\n",
      "Modelo e log do episódio 19 salvos em: 4.5.1\\model_episode_19.pth e 4.5.1\\log_episode_19.csv\n",
      "Episode 20/100, Total Reward: -290.75, Win Rate: 0.32, Wins: 821, Losses: 1710, Epsilon: 0.4090, Steps: 36754, Time: 137.24s\n",
      "Ações: Manter=11266, Comprar=12165, Vender=13323\n",
      "Ganhos Totais: 7703.75, Perdas Totais: -7994.50\n",
      "\n",
      "Modelo e log do episódio 20 salvos em: 4.5.1\\model_episode_20.pth e 4.5.1\\log_episode_20.csv\n",
      "Episode 21/100, Total Reward: -514.00, Win Rate: 0.33, Wins: 852, Losses: 1753, Epsilon: 0.4049, Steps: 36754, Time: 130.18s\n",
      "Ações: Manter=13363, Comprar=11493, Vender=11898\n",
      "Ganhos Totais: 8703.25, Perdas Totais: -9217.25\n",
      "\n",
      "Episode 22/100, Total Reward: -416.25, Win Rate: 0.34, Wins: 849, Losses: 1676, Epsilon: 0.4008, Steps: 36754, Time: 141.20s\n",
      "Ações: Manter=13368, Comprar=10698, Vender=12688\n",
      "Ganhos Totais: 7966.75, Perdas Totais: -8383.00\n",
      "\n",
      "Episode 23/100, Total Reward: -959.25, Win Rate: 0.31, Wins: 814, Losses: 1774, Epsilon: 0.3968, Steps: 36754, Time: 146.56s\n",
      "Ações: Manter=11261, Comprar=13120, Vender=12373\n",
      "Ganhos Totais: 8201.75, Perdas Totais: -9161.00\n",
      "\n",
      "Episode 24/100, Total Reward: -636.50, Win Rate: 0.33, Wins: 825, Losses: 1669, Epsilon: 0.3928, Steps: 36754, Time: 142.63s\n",
      "Ações: Manter=12859, Comprar=13800, Vender=10095\n",
      "Ganhos Totais: 7769.75, Perdas Totais: -8406.25\n",
      "\n",
      "Episode 25/100, Total Reward: -976.00, Win Rate: 0.33, Wins: 862, Losses: 1722, Epsilon: 0.3889, Steps: 36754, Time: 150.19s\n",
      "Ações: Manter=12494, Comprar=12533, Vender=11727\n",
      "Ganhos Totais: 7914.50, Perdas Totais: -8890.50\n",
      "\n",
      "Episode 26/100, Total Reward: -456.50, Win Rate: 0.32, Wins: 827, Losses: 1754, Epsilon: 0.3850, Steps: 36754, Time: 149.38s\n",
      "Ações: Manter=10908, Comprar=12217, Vender=13629\n",
      "Ganhos Totais: 8259.75, Perdas Totais: -8716.25\n",
      "\n",
      "Episode 27/100, Total Reward: -1406.75, Win Rate: 0.33, Wins: 852, Losses: 1762, Epsilon: 0.3812, Steps: 36754, Time: 148.42s\n",
      "Ações: Manter=12393, Comprar=12055, Vender=12306\n",
      "Ganhos Totais: 8036.25, Perdas Totais: -9443.00\n",
      "\n",
      "Episode 28/100, Total Reward: -1217.50, Win Rate: 0.33, Wins: 833, Losses: 1726, Epsilon: 0.3774, Steps: 36754, Time: 146.79s\n",
      "Ações: Manter=11386, Comprar=11410, Vender=13958\n",
      "Ganhos Totais: 7884.50, Perdas Totais: -9102.00\n",
      "\n",
      "Episode 29/100, Total Reward: -637.25, Win Rate: 0.35, Wins: 903, Losses: 1706, Epsilon: 0.3736, Steps: 36754, Time: 149.98s\n",
      "Ações: Manter=14835, Comprar=9212, Vender=12707\n",
      "Ganhos Totais: 8478.25, Perdas Totais: -9115.50\n",
      "\n",
      "Episode 30/100, Total Reward: -753.50, Win Rate: 0.33, Wins: 841, Losses: 1713, Epsilon: 0.3699, Steps: 36754, Time: 146.11s\n",
      "Ações: Manter=12157, Comprar=13308, Vender=11289\n",
      "Ganhos Totais: 7979.50, Perdas Totais: -8733.00\n",
      "\n",
      "Episode 31/100, Total Reward: -135.00, Win Rate: 0.32, Wins: 812, Losses: 1694, Epsilon: 0.3662, Steps: 36754, Time: 152.06s\n",
      "Ações: Manter=10959, Comprar=12138, Vender=13657\n",
      "Ganhos Totais: 8081.75, Perdas Totais: -8216.75\n",
      "\n",
      "Modelo e log do episódio 31 salvos em: 4.5.1\\model_episode_31.pth e 4.5.1\\log_episode_31.csv\n",
      "Episode 32/100, Total Reward: -1503.00, Win Rate: 0.32, Wins: 792, Losses: 1705, Epsilon: 0.3625, Steps: 36754, Time: 149.97s\n",
      "Ações: Manter=12397, Comprar=11414, Vender=12943\n",
      "Ganhos Totais: 7445.75, Perdas Totais: -8948.75\n",
      "\n",
      "Episode 33/100, Total Reward: -1284.00, Win Rate: 0.31, Wins: 743, Losses: 1676, Epsilon: 0.3589, Steps: 36754, Time: 150.89s\n",
      "Ações: Manter=9990, Comprar=14887, Vender=11877\n",
      "Ganhos Totais: 7066.50, Perdas Totais: -8350.50\n",
      "\n",
      "Episode 34/100, Total Reward: -1206.25, Win Rate: 0.33, Wins: 830, Losses: 1690, Epsilon: 0.3553, Steps: 36754, Time: 151.36s\n",
      "Ações: Manter=11765, Comprar=12592, Vender=12397\n",
      "Ganhos Totais: 7713.25, Perdas Totais: -8919.50\n",
      "\n",
      "Episode 35/100, Total Reward: -415.50, Win Rate: 0.33, Wins: 802, Losses: 1661, Epsilon: 0.3517, Steps: 36754, Time: 155.72s\n",
      "Ações: Manter=12130, Comprar=11938, Vender=12686\n",
      "Ganhos Totais: 8058.00, Perdas Totais: -8473.50\n",
      "\n",
      "Episode 36/100, Total Reward: 420.50, Win Rate: 0.35, Wins: 866, Losses: 1628, Epsilon: 0.3482, Steps: 36754, Time: 134.53s\n",
      "Ações: Manter=12911, Comprar=12665, Vender=11178\n",
      "Ganhos Totais: 8820.50, Perdas Totais: -8400.00\n",
      "\n",
      "Modelo e log do episódio 36 salvos em: 4.5.1\\model_episode_36.pth e 4.5.1\\log_episode_36.csv\n",
      "Episode 37/100, Total Reward: -35.00, Win Rate: 0.34, Wins: 819, Losses: 1622, Epsilon: 0.3447, Steps: 36754, Time: 130.94s\n",
      "Ações: Manter=11530, Comprar=12764, Vender=12460\n",
      "Ganhos Totais: 8048.00, Perdas Totais: -8083.00\n",
      "\n",
      "Modelo e log do episódio 37 salvos em: 4.5.1\\model_episode_37.pth e 4.5.1\\log_episode_37.csv\n",
      "Episode 38/100, Total Reward: -1738.50, Win Rate: 0.31, Wins: 744, Losses: 1652, Epsilon: 0.3413, Steps: 36754, Time: 129.61s\n",
      "Ações: Manter=11840, Comprar=15019, Vender=9895\n",
      "Ganhos Totais: 7402.75, Perdas Totais: -9141.25\n",
      "\n",
      "Episode 39/100, Total Reward: -339.75, Win Rate: 0.34, Wins: 827, Losses: 1596, Epsilon: 0.3379, Steps: 36754, Time: 136.29s\n",
      "Ações: Manter=14938, Comprar=9878, Vender=11938\n",
      "Ganhos Totais: 8191.00, Perdas Totais: -8530.75\n",
      "\n",
      "Episode 40/100, Total Reward: -1436.50, Win Rate: 0.31, Wins: 738, Losses: 1618, Epsilon: 0.3345, Steps: 36754, Time: 145.16s\n",
      "Ações: Manter=10314, Comprar=14811, Vender=11629\n",
      "Ganhos Totais: 6770.00, Perdas Totais: -8206.50\n",
      "\n",
      "Episode 41/100, Total Reward: -578.25, Win Rate: 0.31, Wins: 762, Losses: 1664, Epsilon: 0.3311, Steps: 36754, Time: 157.41s\n",
      "Ações: Manter=11925, Comprar=12478, Vender=12351\n",
      "Ganhos Totais: 7399.50, Perdas Totais: -7977.75\n",
      "\n",
      "Episode 42/100, Total Reward: 807.25, Win Rate: 0.34, Wins: 839, Losses: 1640, Epsilon: 0.3278, Steps: 36754, Time: 166.75s\n",
      "Ações: Manter=12421, Comprar=10694, Vender=13639\n",
      "Ganhos Totais: 8657.50, Perdas Totais: -7850.25\n",
      "\n",
      "Modelo e log do episódio 42 salvos em: 4.5.1\\model_episode_42.pth e 4.5.1\\log_episode_42.csv\n",
      "Episode 43/100, Total Reward: -475.00, Win Rate: 0.33, Wins: 824, Losses: 1675, Epsilon: 0.3246, Steps: 36754, Time: 152.87s\n",
      "Ações: Manter=12624, Comprar=12514, Vender=11616\n",
      "Ganhos Totais: 8434.25, Perdas Totais: -8909.25\n",
      "\n",
      "Episode 44/100, Total Reward: -383.25, Win Rate: 0.32, Wins: 778, Losses: 1636, Epsilon: 0.3213, Steps: 36754, Time: 163.88s\n",
      "Ações: Manter=11762, Comprar=12890, Vender=12102\n",
      "Ganhos Totais: 7890.25, Perdas Totais: -8273.50\n",
      "\n",
      "Episode 45/100, Total Reward: 259.00, Win Rate: 0.33, Wins: 769, Losses: 1566, Epsilon: 0.3181, Steps: 36754, Time: 172.67s\n",
      "Ações: Manter=10267, Comprar=13941, Vender=12546\n",
      "Ganhos Totais: 7709.00, Perdas Totais: -7450.00\n",
      "\n",
      "Modelo e log do episódio 45 salvos em: 4.5.1\\model_episode_45.pth e 4.5.1\\log_episode_45.csv\n",
      "Episode 46/100, Total Reward: 380.00, Win Rate: 0.34, Wins: 841, Losses: 1654, Epsilon: 0.3149, Steps: 36754, Time: 169.06s\n",
      "Ações: Manter=11087, Comprar=12550, Vender=13117\n",
      "Ganhos Totais: 8181.00, Perdas Totais: -7801.00\n",
      "\n",
      "Modelo e log do episódio 46 salvos em: 4.5.1\\model_episode_46.pth e 4.5.1\\log_episode_46.csv\n",
      "Episode 47/100, Total Reward: -244.25, Win Rate: 0.32, Wins: 765, Losses: 1612, Epsilon: 0.3118, Steps: 36754, Time: 168.01s\n",
      "Ações: Manter=11669, Comprar=13965, Vender=11120\n",
      "Ganhos Totais: 7983.00, Perdas Totais: -8227.25\n",
      "\n",
      "Episode 48/100, Total Reward: -65.00, Win Rate: 0.34, Wins: 827, Losses: 1623, Epsilon: 0.3086, Steps: 36754, Time: 170.22s\n",
      "Ações: Manter=11230, Comprar=13458, Vender=12066\n",
      "Ganhos Totais: 8245.50, Perdas Totais: -8310.50\n",
      "\n",
      "Episode 49/100, Total Reward: -1518.50, Win Rate: 0.33, Wins: 783, Losses: 1616, Epsilon: 0.3056, Steps: 36754, Time: 168.58s\n",
      "Ações: Manter=13478, Comprar=10545, Vender=12731\n",
      "Ganhos Totais: 7406.00, Perdas Totais: -8924.50\n",
      "\n",
      "Episode 50/100, Total Reward: -333.75, Win Rate: 0.32, Wins: 750, Losses: 1613, Epsilon: 0.3025, Steps: 36754, Time: 167.88s\n",
      "Ações: Manter=10906, Comprar=15734, Vender=10114\n",
      "Ganhos Totais: 7457.75, Perdas Totais: -7791.50\n",
      "\n",
      "Episode 51/100, Total Reward: -249.25, Win Rate: 0.31, Wins: 742, Losses: 1659, Epsilon: 0.2995, Steps: 36754, Time: 169.94s\n",
      "Ações: Manter=10458, Comprar=14628, Vender=11668\n",
      "Ganhos Totais: 7871.00, Perdas Totais: -8120.25\n",
      "\n",
      "Episode 52/100, Total Reward: -54.75, Win Rate: 0.33, Wins: 768, Losses: 1538, Epsilon: 0.2965, Steps: 36754, Time: 170.82s\n",
      "Ações: Manter=10826, Comprar=14660, Vender=11268\n",
      "Ganhos Totais: 7416.50, Perdas Totais: -7471.25\n",
      "\n",
      "Episode 53/100, Total Reward: -1039.25, Win Rate: 0.33, Wins: 766, Losses: 1549, Epsilon: 0.2935, Steps: 36754, Time: 169.07s\n",
      "Ações: Manter=12425, Comprar=11190, Vender=13139\n",
      "Ganhos Totais: 7420.25, Perdas Totais: -8459.50\n",
      "\n",
      "Episode 54/100, Total Reward: -121.75, Win Rate: 0.35, Wins: 877, Losses: 1632, Epsilon: 0.2906, Steps: 36754, Time: 169.65s\n",
      "Ações: Manter=15346, Comprar=9638, Vender=11770\n",
      "Ganhos Totais: 8716.00, Perdas Totais: -8837.75\n",
      "\n",
      "Episode 55/100, Total Reward: -692.75, Win Rate: 0.33, Wins: 786, Losses: 1603, Epsilon: 0.2877, Steps: 36754, Time: 168.47s\n",
      "Ações: Manter=14053, Comprar=10828, Vender=11873\n",
      "Ganhos Totais: 7838.25, Perdas Totais: -8531.00\n",
      "\n",
      "Episode 56/100, Total Reward: -1096.25, Win Rate: 0.33, Wins: 769, Losses: 1584, Epsilon: 0.2848, Steps: 36754, Time: 146.13s\n",
      "Ações: Manter=12422, Comprar=12656, Vender=11676\n",
      "Ganhos Totais: 7339.50, Perdas Totais: -8435.75\n",
      "\n",
      "Episode 57/100, Total Reward: -209.75, Win Rate: 0.33, Wins: 756, Losses: 1513, Epsilon: 0.2820, Steps: 36754, Time: 127.07s\n",
      "Ações: Manter=10653, Comprar=15240, Vender=10861\n",
      "Ganhos Totais: 7466.00, Perdas Totais: -7675.75\n",
      "\n",
      "Episode 58/100, Total Reward: -120.75, Win Rate: 0.34, Wins: 784, Losses: 1516, Epsilon: 0.2791, Steps: 36754, Time: 118.56s\n",
      "Ações: Manter=11830, Comprar=12878, Vender=12046\n",
      "Ganhos Totais: 7697.00, Perdas Totais: -7817.75\n",
      "\n",
      "Episode 59/100, Total Reward: -205.00, Win Rate: 0.34, Wins: 775, Losses: 1531, Epsilon: 0.2763, Steps: 36754, Time: 114.41s\n",
      "Ações: Manter=13018, Comprar=12127, Vender=11609\n",
      "Ganhos Totais: 7685.25, Perdas Totais: -7890.25\n",
      "\n",
      "Episode 60/100, Total Reward: 73.00, Win Rate: 0.33, Wins: 765, Losses: 1521, Epsilon: 0.2736, Steps: 36754, Time: 108.64s\n",
      "Ações: Manter=11468, Comprar=11895, Vender=13391\n",
      "Ganhos Totais: 8175.75, Perdas Totais: -8102.75\n",
      "\n",
      "Modelo e log do episódio 60 salvos em: 4.5.1\\model_episode_60.pth e 4.5.1\\log_episode_60.csv\n",
      "Episode 61/100, Total Reward: -870.50, Win Rate: 0.32, Wins: 716, Losses: 1549, Epsilon: 0.2708, Steps: 36754, Time: 111.05s\n",
      "Ações: Manter=9681, Comprar=13794, Vender=13279\n",
      "Ganhos Totais: 6956.25, Perdas Totais: -7826.75\n",
      "\n",
      "Episode 62/100, Total Reward: -1003.00, Win Rate: 0.33, Wins: 746, Losses: 1514, Epsilon: 0.2681, Steps: 36754, Time: 108.84s\n",
      "Ações: Manter=11652, Comprar=14358, Vender=10744\n",
      "Ganhos Totais: 7435.25, Perdas Totais: -8438.25\n",
      "\n",
      "Episode 63/100, Total Reward: -182.25, Win Rate: 0.33, Wins: 716, Losses: 1485, Epsilon: 0.2655, Steps: 36754, Time: 108.82s\n",
      "Ações: Manter=11176, Comprar=12653, Vender=12925\n",
      "Ganhos Totais: 7026.00, Perdas Totais: -7208.25\n",
      "\n",
      "Episode 64/100, Total Reward: 241.75, Win Rate: 0.33, Wins: 768, Losses: 1560, Epsilon: 0.2628, Steps: 36754, Time: 111.69s\n",
      "Ações: Manter=12727, Comprar=11076, Vender=12951\n",
      "Ganhos Totais: 7840.75, Perdas Totais: -7599.00\n",
      "\n",
      "Modelo e log do episódio 64 salvos em: 4.5.1\\model_episode_64.pth e 4.5.1\\log_episode_64.csv\n",
      "Episode 65/100, Total Reward: -12.25, Win Rate: 0.35, Wins: 807, Losses: 1478, Epsilon: 0.2602, Steps: 36754, Time: 111.74s\n",
      "Ações: Manter=14004, Comprar=12885, Vender=9865\n",
      "Ganhos Totais: 7680.50, Perdas Totais: -7692.75\n",
      "\n",
      "Episode 66/100, Total Reward: -193.75, Win Rate: 0.31, Wins: 668, Losses: 1469, Epsilon: 0.2576, Steps: 36754, Time: 110.50s\n",
      "Ações: Manter=9563, Comprar=13551, Vender=13640\n",
      "Ganhos Totais: 6672.75, Perdas Totais: -6866.50\n",
      "\n",
      "Episode 67/100, Total Reward: 442.00, Win Rate: 0.33, Wins: 784, Losses: 1558, Epsilon: 0.2550, Steps: 36754, Time: 111.13s\n",
      "Ações: Manter=13593, Comprar=12349, Vender=10812\n",
      "Ganhos Totais: 8237.75, Perdas Totais: -7795.75\n",
      "\n",
      "Modelo e log do episódio 67 salvos em: 4.5.1\\model_episode_67.pth e 4.5.1\\log_episode_67.csv\n",
      "Episode 68/100, Total Reward: -319.25, Win Rate: 0.33, Wins: 794, Losses: 1585, Epsilon: 0.2524, Steps: 36754, Time: 110.85s\n",
      "Ações: Manter=12343, Comprar=15067, Vender=9344\n",
      "Ganhos Totais: 7920.75, Perdas Totais: -8240.00\n",
      "\n",
      "Episode 69/100, Total Reward: 196.75, Win Rate: 0.33, Wins: 715, Losses: 1464, Epsilon: 0.2499, Steps: 36754, Time: 111.10s\n",
      "Ações: Manter=10160, Comprar=14847, Vender=11747\n",
      "Ganhos Totais: 7117.25, Perdas Totais: -6920.50\n",
      "\n",
      "Modelo e log do episódio 69 salvos em: 4.5.1\\model_episode_69.pth e 4.5.1\\log_episode_69.csv\n",
      "Episode 70/100, Total Reward: -995.50, Win Rate: 0.33, Wins: 746, Losses: 1496, Epsilon: 0.2474, Steps: 36754, Time: 110.60s\n",
      "Ações: Manter=12638, Comprar=14542, Vender=9574\n",
      "Ganhos Totais: 6659.00, Perdas Totais: -7654.50\n",
      "\n",
      "Episode 71/100, Total Reward: -588.00, Win Rate: 0.33, Wins: 764, Losses: 1543, Epsilon: 0.2449, Steps: 36754, Time: 110.85s\n",
      "Ações: Manter=14768, Comprar=10088, Vender=11898\n",
      "Ganhos Totais: 7960.75, Perdas Totais: -8548.75\n",
      "\n",
      "Episode 72/100, Total Reward: -556.50, Win Rate: 0.34, Wins: 797, Losses: 1575, Epsilon: 0.2425, Steps: 36754, Time: 112.06s\n",
      "Ações: Manter=15037, Comprar=10927, Vender=10790\n",
      "Ganhos Totais: 7836.50, Perdas Totais: -8393.00\n",
      "\n",
      "Episode 73/100, Total Reward: -1002.50, Win Rate: 0.33, Wins: 730, Losses: 1460, Epsilon: 0.2401, Steps: 36754, Time: 110.84s\n",
      "Ações: Manter=12547, Comprar=11923, Vender=12284\n",
      "Ganhos Totais: 6914.75, Perdas Totais: -7917.25\n",
      "\n",
      "Episode 74/100, Total Reward: -1339.75, Win Rate: 0.30, Wins: 656, Losses: 1516, Epsilon: 0.2377, Steps: 36754, Time: 110.47s\n",
      "Ações: Manter=8798, Comprar=14065, Vender=13891\n",
      "Ganhos Totais: 6216.50, Perdas Totais: -7556.25\n",
      "\n",
      "Episode 75/100, Total Reward: -933.50, Win Rate: 0.35, Wins: 789, Losses: 1483, Epsilon: 0.2353, Steps: 36754, Time: 111.40s\n",
      "Ações: Manter=16106, Comprar=10405, Vender=10243\n",
      "Ganhos Totais: 7341.00, Perdas Totais: -8274.50\n",
      "\n",
      "Episode 76/100, Total Reward: -362.00, Win Rate: 0.33, Wins: 738, Losses: 1510, Epsilon: 0.2329, Steps: 36754, Time: 110.63s\n",
      "Ações: Manter=10986, Comprar=13278, Vender=12490\n",
      "Ganhos Totais: 7221.50, Perdas Totais: -7583.50\n",
      "\n",
      "Episode 77/100, Total Reward: -269.00, Win Rate: 0.33, Wins: 722, Losses: 1451, Epsilon: 0.2306, Steps: 36754, Time: 111.01s\n",
      "Ações: Manter=11784, Comprar=15260, Vender=9710\n",
      "Ganhos Totais: 6730.25, Perdas Totais: -6999.25\n",
      "\n",
      "Episode 78/100, Total Reward: -964.00, Win Rate: 0.32, Wins: 722, Losses: 1537, Epsilon: 0.2283, Steps: 36754, Time: 111.11s\n",
      "Ações: Manter=12983, Comprar=13667, Vender=10104\n",
      "Ganhos Totais: 7243.75, Perdas Totais: -8207.75\n",
      "\n",
      "Episode 79/100, Total Reward: -230.75, Win Rate: 0.34, Wins: 746, Losses: 1447, Epsilon: 0.2260, Steps: 36754, Time: 110.85s\n",
      "Ações: Manter=15076, Comprar=13585, Vender=8093\n",
      "Ganhos Totais: 7355.25, Perdas Totais: -7586.00\n",
      "\n",
      "Episode 80/100, Total Reward: 251.25, Win Rate: 0.32, Wins: 703, Losses: 1467, Epsilon: 0.2238, Steps: 36754, Time: 110.09s\n",
      "Ações: Manter=9800, Comprar=11088, Vender=15866\n",
      "Ganhos Totais: 6872.00, Perdas Totais: -6620.75\n",
      "\n",
      "Modelo e log do episódio 80 salvos em: 4.5.1\\model_episode_80.pth e 4.5.1\\log_episode_80.csv\n",
      "Episode 81/100, Total Reward: -652.25, Win Rate: 0.34, Wins: 793, Losses: 1513, Epsilon: 0.2215, Steps: 36754, Time: 110.25s\n",
      "Ações: Manter=14236, Comprar=11640, Vender=10878\n",
      "Ganhos Totais: 7727.00, Perdas Totais: -8379.25\n",
      "\n",
      "Episode 82/100, Total Reward: -1203.00, Win Rate: 0.33, Wins: 706, Losses: 1441, Epsilon: 0.2193, Steps: 36754, Time: 110.63s\n",
      "Ações: Manter=11675, Comprar=11778, Vender=13301\n",
      "Ganhos Totais: 6783.25, Perdas Totais: -7986.25\n",
      "\n",
      "Episode 83/100, Total Reward: 511.00, Win Rate: 0.34, Wins: 731, Losses: 1397, Epsilon: 0.2171, Steps: 36754, Time: 110.59s\n",
      "Ações: Manter=13207, Comprar=10352, Vender=13195\n",
      "Ganhos Totais: 7879.25, Perdas Totais: -7368.25\n",
      "\n",
      "Modelo e log do episódio 83 salvos em: 4.5.1\\model_episode_83.pth e 4.5.1\\log_episode_83.csv\n",
      "Episode 84/100, Total Reward: -918.50, Win Rate: 0.34, Wins: 739, Losses: 1425, Epsilon: 0.2149, Steps: 36754, Time: 109.75s\n",
      "Ações: Manter=15624, Comprar=9782, Vender=11348\n",
      "Ganhos Totais: 7321.25, Perdas Totais: -8239.75\n",
      "\n",
      "Episode 85/100, Total Reward: -47.00, Win Rate: 0.32, Wins: 683, Losses: 1444, Epsilon: 0.2128, Steps: 36754, Time: 110.48s\n",
      "Ações: Manter=10270, Comprar=13166, Vender=13318\n",
      "Ganhos Totais: 7451.00, Perdas Totais: -7498.00\n",
      "\n",
      "Episode 86/100, Total Reward: -1429.50, Win Rate: 0.33, Wins: 710, Losses: 1466, Epsilon: 0.2107, Steps: 36754, Time: 110.70s\n",
      "Ações: Manter=11582, Comprar=11813, Vender=13359\n",
      "Ganhos Totais: 6650.00, Perdas Totais: -8079.50\n",
      "\n",
      "Episode 87/100, Total Reward: 34.25, Win Rate: 0.34, Wins: 706, Losses: 1380, Epsilon: 0.2086, Steps: 36754, Time: 110.07s\n",
      "Ações: Manter=12473, Comprar=14144, Vender=10137\n",
      "Ganhos Totais: 6927.75, Perdas Totais: -6893.50\n",
      "\n",
      "Episode 88/100, Total Reward: -283.50, Win Rate: 0.32, Wins: 658, Losses: 1429, Epsilon: 0.2065, Steps: 36754, Time: 110.64s\n",
      "Ações: Manter=11608, Comprar=13176, Vender=11970\n",
      "Ganhos Totais: 6676.50, Perdas Totais: -6960.00\n",
      "\n",
      "Episode 89/100, Total Reward: -582.50, Win Rate: 0.34, Wins: 693, Losses: 1375, Epsilon: 0.2044, Steps: 36754, Time: 110.34s\n",
      "Ações: Manter=12314, Comprar=11387, Vender=13053\n",
      "Ganhos Totais: 7008.75, Perdas Totais: -7591.25\n",
      "\n",
      "Episode 90/100, Total Reward: -1308.75, Win Rate: 0.33, Wins: 719, Losses: 1449, Epsilon: 0.2024, Steps: 36754, Time: 110.66s\n",
      "Ações: Manter=12508, Comprar=9422, Vender=14824\n",
      "Ganhos Totais: 7410.50, Perdas Totais: -8719.25\n",
      "\n",
      "Episode 91/100, Total Reward: 183.75, Win Rate: 0.33, Wins: 677, Losses: 1401, Epsilon: 0.2003, Steps: 36754, Time: 110.11s\n",
      "Ações: Manter=9264, Comprar=13235, Vender=14255\n",
      "Ganhos Totais: 6783.00, Perdas Totais: -6599.25\n",
      "\n",
      "Episode 92/100, Total Reward: -1155.25, Win Rate: 0.32, Wins: 671, Losses: 1441, Epsilon: 0.1983, Steps: 36754, Time: 110.92s\n",
      "Ações: Manter=11900, Comprar=11765, Vender=13089\n",
      "Ganhos Totais: 6998.75, Perdas Totais: -8154.00\n",
      "\n",
      "Episode 93/100, Total Reward: -398.50, Win Rate: 0.33, Wins: 711, Losses: 1434, Epsilon: 0.1964, Steps: 36754, Time: 111.03s\n",
      "Ações: Manter=12479, Comprar=10424, Vender=13851\n",
      "Ganhos Totais: 7335.50, Perdas Totais: -7734.00\n",
      "\n",
      "Episode 94/100, Total Reward: -444.00, Win Rate: 0.34, Wins: 727, Losses: 1393, Epsilon: 0.1944, Steps: 36754, Time: 110.55s\n",
      "Ações: Manter=14103, Comprar=10054, Vender=12597\n",
      "Ganhos Totais: 6819.50, Perdas Totais: -7263.50\n",
      "\n",
      "Episode 95/100, Total Reward: 1012.00, Win Rate: 0.35, Wins: 738, Losses: 1400, Epsilon: 0.1924, Steps: 36754, Time: 110.96s\n",
      "Ações: Manter=10954, Comprar=11757, Vender=14043\n",
      "Ganhos Totais: 7892.50, Perdas Totais: -6880.50\n",
      "\n",
      "Modelo e log do episódio 95 salvos em: 4.5.1\\model_episode_95.pth e 4.5.1\\log_episode_95.csv\n",
      "Episode 96/100, Total Reward: -611.00, Win Rate: 0.35, Wins: 710, Losses: 1340, Epsilon: 0.1905, Steps: 36754, Time: 110.98s\n",
      "Ações: Manter=12575, Comprar=14035, Vender=10144\n",
      "Ganhos Totais: 6874.25, Perdas Totais: -7485.25\n",
      "\n",
      "Episode 97/100, Total Reward: -374.75, Win Rate: 0.32, Wins: 667, Losses: 1390, Epsilon: 0.1886, Steps: 36754, Time: 111.08s\n",
      "Ações: Manter=12809, Comprar=10740, Vender=13205\n",
      "Ganhos Totais: 6788.25, Perdas Totais: -7163.00\n",
      "\n",
      "Episode 98/100, Total Reward: -285.25, Win Rate: 0.32, Wins: 638, Losses: 1352, Epsilon: 0.1867, Steps: 36754, Time: 110.84s\n",
      "Ações: Manter=10138, Comprar=17197, Vender=9419\n",
      "Ganhos Totais: 6936.75, Perdas Totais: -7222.00\n",
      "\n",
      "Episode 99/100, Total Reward: 139.50, Win Rate: 0.34, Wins: 717, Losses: 1368, Epsilon: 0.1849, Steps: 36754, Time: 110.64s\n",
      "Ações: Manter=13413, Comprar=11629, Vender=11712\n",
      "Ganhos Totais: 7347.00, Perdas Totais: -7207.50\n",
      "\n",
      "Episode 100/100, Total Reward: -142.25, Win Rate: 0.34, Wins: 716, Losses: 1415, Epsilon: 0.1830, Steps: 36754, Time: 111.64s\n",
      "Ações: Manter=13859, Comprar=10045, Vender=12850\n",
      "Ganhos Totais: 7222.50, Perdas Totais: -7364.75\n",
      "\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 95, Total Reward: 1012.00, Win Rate: 0.35, Wins: 738, Losses: 1400, Ações: {0: 10954, 1: 11757, 2: 14043}, Steps: 36754, Time: 110.96s\n",
      "Rank 2: Episode 42, Total Reward: 807.25, Win Rate: 0.34, Wins: 839, Losses: 1640, Ações: {0: 12421, 1: 10694, 2: 13639}, Steps: 36754, Time: 166.75s\n",
      "Rank 3: Episode 2, Total Reward: 635.00, Win Rate: 0.32, Wins: 956, Losses: 2012, Ações: {0: 11516, 1: 12360, 2: 12878}, Steps: 36754, Time: 132.10s\n",
      "Rank 4: Episode 83, Total Reward: 511.00, Win Rate: 0.34, Wins: 731, Losses: 1397, Ações: {0: 13207, 1: 10352, 2: 13195}, Steps: 36754, Time: 110.59s\n",
      "Rank 5: Episode 67, Total Reward: 442.00, Win Rate: 0.33, Wins: 784, Losses: 1558, Ações: {0: 13593, 1: 12349, 2: 10812}, Steps: 36754, Time: 111.13s\n",
      "Rank 6: Episode 36, Total Reward: 420.50, Win Rate: 0.35, Wins: 866, Losses: 1628, Ações: {0: 12911, 1: 12665, 2: 11178}, Steps: 36754, Time: 134.53s\n",
      "Rank 7: Episode 46, Total Reward: 380.00, Win Rate: 0.34, Wins: 841, Losses: 1654, Ações: {0: 11087, 1: 12550, 2: 13117}, Steps: 36754, Time: 169.06s\n",
      "Rank 8: Episode 45, Total Reward: 259.00, Win Rate: 0.33, Wins: 769, Losses: 1566, Ações: {0: 10267, 1: 13941, 2: 12546}, Steps: 36754, Time: 172.67s\n",
      "Rank 9: Episode 80, Total Reward: 251.25, Win Rate: 0.32, Wins: 703, Losses: 1467, Ações: {0: 9800, 1: 11088, 2: 15866}, Steps: 36754, Time: 110.09s\n",
      "Rank 10: Episode 64, Total Reward: 241.75, Win Rate: 0.33, Wins: 768, Losses: 1560, Ações: {0: 12727, 1: 11076, 2: 12951}, Steps: 36754, Time: 111.69s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Carregar o dataset\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None  # Inicializar entry_step\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None  # Resetar entry_step\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step  # Registrar o passo de entrada\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.position = 0\n",
    "                    self.exit_price = current_price\n",
    "                    reward += -price_change - 0.25  # Ganho da posição vendida\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.current_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'profit': -price_change - 0.25\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'short',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'exit_step': self.current_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'profit': -price_change - 0.25\n",
    "                    })\n",
    "                    # Resetar entry_step após fechar a posição\n",
    "                    self.entry_step = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step  # Registrar o passo de entrada\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.position = 0\n",
    "                    self.exit_price = current_price\n",
    "                    reward += price_change - 0.25  # Ganho da posição comprada\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.current_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'profit': price_change - 0.25\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'long',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'exit_step': self.current_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'profit': price_change - 0.25\n",
    "                    })\n",
    "                    # Resetar entry_step após fechar a posição\n",
    "                    self.entry_step = None\n",
    "            else:  # Manter\n",
    "                if self.position == 1:\n",
    "                    reward += price_change  # Ganho da posição comprada\n",
    "                elif self.position == -1:\n",
    "                    reward += -price_change  # Ganho da posição vendida\n",
    "\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                reward += price_change - 0.25  # Ganho da posição comprada\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.current_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'profit': price_change - 0.25\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'long',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'exit_step': self.current_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'profit': price_change - 0.25\n",
    "                })\n",
    "                # Resetar entry_step após fechar a posição\n",
    "                self.entry_step = None\n",
    "                self.position = 0  # Fechar a posição\n",
    "\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                reward += -price_change - 0.25  # Ganho da posição vendida\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.current_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'profit': -price_change - 0.25\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'short',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'exit_step': self.current_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'profit': -price_change - 0.25\n",
    "                })\n",
    "                # Resetar entry_step após fechar a posição\n",
    "                self.entry_step = None\n",
    "                self.position = 0  # Fechar a posição\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5 # Valor inicial de epsilon, mais baixo para menos ações aleatórias no início\n",
    "epsilon_end = 0.05 # Valor final de epsilon, mais alto para mais exploração\n",
    "epsilon_decay = 0.99 # Decaimento mais rápido para o agente confiar mais nas ações aprendidas\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.5.1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []  # Lista para armazenar as operações do episódio atual\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "        total_reward += reward\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar ganhos e perdas\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "            win_total += reward\n",
    "        elif reward < 0:\n",
    "            losses += 1\n",
    "            lose_total += reward\n",
    "\n",
    "        # Registrar a operação se houver uma\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            else:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade)\n",
    "                current_trade = None  # Resetar a operação atual\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\\n\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
