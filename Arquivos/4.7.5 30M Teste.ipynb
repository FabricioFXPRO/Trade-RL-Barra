{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 1132.75, Win Rate: 0.52, Wins: 741, Losses: 684, Epsilon: 0.4950, Steps: 18292, Time: 61.10s\n",
      "Ações: Manter=6090, Comprar=6019, Vender=6183\n",
      "Ganhos Totais: 28489.00, Perdas Totais: -27356.25\n",
      "Modelo e log do episódio 1 salvos em: 4.7.5\\model_episode_1.pth e 4.7.5\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -3420.75, Win Rate: 0.47, Wins: 645, Losses: 730, Epsilon: 0.4900, Steps: 18292, Time: 63.87s\n",
      "Ações: Manter=6196, Comprar=5884, Vender=6212\n",
      "Ganhos Totais: 26373.00, Perdas Totais: -29793.75\n",
      "Modelo e log do episódio 2 salvos em: 4.7.5\\model_episode_2.pth e 4.7.5\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1987.25, Win Rate: 0.50, Wins: 681, Losses: 691, Epsilon: 0.4851, Steps: 18292, Time: 65.20s\n",
      "Ações: Manter=6149, Comprar=5415, Vender=6728\n",
      "Ganhos Totais: 26620.25, Perdas Totais: -28607.50\n",
      "Modelo e log do episódio 3 salvos em: 4.7.5\\model_episode_3.pth e 4.7.5\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -2845.75, Win Rate: 0.49, Wins: 661, Losses: 678, Epsilon: 0.4803, Steps: 18292, Time: 61.94s\n",
      "Ações: Manter=6120, Comprar=5467, Vender=6705\n",
      "Ganhos Totais: 26244.25, Perdas Totais: -29090.00\n",
      "Modelo e log do episódio 4 salvos em: 4.7.5\\model_episode_4.pth e 4.7.5\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -3412.00, Win Rate: 0.49, Wins: 652, Losses: 680, Epsilon: 0.4755, Steps: 18292, Time: 60.80s\n",
      "Ações: Manter=6314, Comprar=5929, Vender=6049\n",
      "Ganhos Totais: 25535.25, Perdas Totais: -28947.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.5\\model_episode_5.pth e 4.7.5\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -881.75, Win Rate: 0.51, Wins: 733, Losses: 694, Epsilon: 0.4707, Steps: 18292, Time: 61.05s\n",
      "Ações: Manter=5809, Comprar=6182, Vender=6301\n",
      "Ganhos Totais: 27852.50, Perdas Totais: -28734.25\n",
      "Modelo e log do episódio 6 salvos em: 4.7.5\\model_episode_6.pth e 4.7.5\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -1334.25, Win Rate: 0.50, Wins: 683, Losses: 687, Epsilon: 0.4660, Steps: 18292, Time: 61.56s\n",
      "Ações: Manter=6041, Comprar=6363, Vender=5888\n",
      "Ganhos Totais: 27214.25, Perdas Totais: -28548.50\n",
      "Modelo e log do episódio 7 salvos em: 4.7.5\\model_episode_7.pth e 4.7.5\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 34.75, Win Rate: 0.52, Wins: 646, Losses: 607, Epsilon: 0.4614, Steps: 18292, Time: 61.12s\n",
      "Ações: Manter=7201, Comprar=5378, Vender=5713\n",
      "Ganhos Totais: 25750.00, Perdas Totais: -25715.25\n",
      "Modelo e log do episódio 8 salvos em: 4.7.5\\model_episode_8.pth e 4.7.5\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -1757.25, Win Rate: 0.49, Wins: 670, Losses: 690, Epsilon: 0.4568, Steps: 18292, Time: 61.09s\n",
      "Ações: Manter=6041, Comprar=6562, Vender=5689\n",
      "Ganhos Totais: 27084.25, Perdas Totais: -28841.50\n",
      "Modelo e log do episódio 9 salvos em: 4.7.5\\model_episode_9.pth e 4.7.5\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -2282.50, Win Rate: 0.51, Wins: 678, Losses: 659, Epsilon: 0.4522, Steps: 18292, Time: 61.21s\n",
      "Ações: Manter=6433, Comprar=6206, Vender=5653\n",
      "Ganhos Totais: 26262.50, Perdas Totais: -28545.00\n",
      "Modelo e log do episódio 10 salvos em: 4.7.5\\model_episode_10.pth e 4.7.5\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 1532.00, Win Rate: 0.52, Wins: 708, Losses: 661, Epsilon: 0.4477, Steps: 18292, Time: 61.04s\n",
      "Ações: Manter=5793, Comprar=6299, Vender=6200\n",
      "Ganhos Totais: 28555.50, Perdas Totais: -27023.50\n",
      "Modelo e log do episódio 11 salvos em: 4.7.5\\model_episode_11.pth e 4.7.5\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 403.00, Win Rate: 0.51, Wins: 664, Losses: 644, Epsilon: 0.4432, Steps: 18292, Time: 61.62s\n",
      "Ações: Manter=6265, Comprar=6066, Vender=5961\n",
      "Ganhos Totais: 26803.25, Perdas Totais: -26400.25\n",
      "Modelo e log do episódio 12 salvos em: 4.7.5\\model_episode_12.pth e 4.7.5\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -2871.75, Win Rate: 0.46, Wins: 592, Losses: 693, Epsilon: 0.4388, Steps: 18292, Time: 61.27s\n",
      "Ações: Manter=6108, Comprar=6551, Vender=5633\n",
      "Ganhos Totais: 26065.75, Perdas Totais: -28937.50\n",
      "Episode 14/100, Total Reward: -545.75, Win Rate: 0.48, Wins: 632, Losses: 673, Epsilon: 0.4344, Steps: 18292, Time: 60.97s\n",
      "Ações: Manter=6524, Comprar=6019, Vender=5749\n",
      "Ganhos Totais: 26749.50, Perdas Totais: -27295.25\n",
      "Modelo e log do episódio 14 salvos em: 4.7.5\\model_episode_14.pth e 4.7.5\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -2593.75, Win Rate: 0.51, Wins: 623, Losses: 607, Epsilon: 0.4300, Steps: 18292, Time: 60.77s\n",
      "Ações: Manter=6998, Comprar=6125, Vender=5169\n",
      "Ganhos Totais: 25445.75, Perdas Totais: -28039.50\n",
      "Episode 16/100, Total Reward: -1816.00, Win Rate: 0.49, Wins: 640, Losses: 661, Epsilon: 0.4257, Steps: 18292, Time: 61.15s\n",
      "Ações: Manter=6532, Comprar=5543, Vender=6217\n",
      "Ganhos Totais: 25382.25, Perdas Totais: -27198.25\n",
      "Modelo e log do episódio 16 salvos em: 4.7.5\\model_episode_16.pth e 4.7.5\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -4566.00, Win Rate: 0.48, Wins: 578, Losses: 636, Epsilon: 0.4215, Steps: 18292, Time: 61.32s\n",
      "Ações: Manter=6875, Comprar=5649, Vender=5768\n",
      "Ganhos Totais: 23174.00, Perdas Totais: -27740.00\n",
      "Episode 18/100, Total Reward: 744.50, Win Rate: 0.51, Wins: 684, Losses: 655, Epsilon: 0.4173, Steps: 18292, Time: 60.91s\n",
      "Ações: Manter=5996, Comprar=5804, Vender=6492\n",
      "Ganhos Totais: 27637.00, Perdas Totais: -26892.50\n",
      "Modelo e log do episódio 18 salvos em: 4.7.5\\model_episode_18.pth e 4.7.5\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -4534.75, Win Rate: 0.49, Wins: 656, Losses: 683, Epsilon: 0.4131, Steps: 18292, Time: 60.81s\n",
      "Ações: Manter=5510, Comprar=6110, Vender=6672\n",
      "Ganhos Totais: 25922.00, Perdas Totais: -30456.75\n",
      "Episode 20/100, Total Reward: -1279.00, Win Rate: 0.53, Wins: 699, Losses: 619, Epsilon: 0.4090, Steps: 18292, Time: 61.61s\n",
      "Ações: Manter=6099, Comprar=5293, Vender=6900\n",
      "Ganhos Totais: 27897.00, Perdas Totais: -29176.00\n",
      "Modelo e log do episódio 20 salvos em: 4.7.5\\model_episode_20.pth e 4.7.5\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -3560.75, Win Rate: 0.48, Wins: 623, Losses: 665, Epsilon: 0.4049, Steps: 18292, Time: 60.72s\n",
      "Ações: Manter=6046, Comprar=5200, Vender=7046\n",
      "Ganhos Totais: 25906.50, Perdas Totais: -29467.25\n",
      "Episode 22/100, Total Reward: -1192.75, Win Rate: 0.52, Wins: 669, Losses: 607, Epsilon: 0.4008, Steps: 18292, Time: 61.34s\n",
      "Ações: Manter=7118, Comprar=5583, Vender=5591\n",
      "Ganhos Totais: 26471.50, Perdas Totais: -27664.25\n",
      "Modelo e log do episódio 22 salvos em: 4.7.5\\model_episode_22.pth e 4.7.5\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: -4092.75, Win Rate: 0.50, Wins: 669, Losses: 666, Epsilon: 0.3968, Steps: 18292, Time: 61.25s\n",
      "Ações: Manter=5933, Comprar=5997, Vender=6362\n",
      "Ganhos Totais: 25507.00, Perdas Totais: -29599.75\n",
      "Episode 24/100, Total Reward: 1016.25, Win Rate: 0.51, Wins: 709, Losses: 688, Epsilon: 0.3928, Steps: 18292, Time: 61.49s\n",
      "Ações: Manter=4958, Comprar=5396, Vender=7938\n",
      "Ganhos Totais: 29289.25, Perdas Totais: -28273.00\n",
      "Modelo e log do episódio 24 salvos em: 4.7.5\\model_episode_24.pth e 4.7.5\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 944.00, Win Rate: 0.52, Wins: 680, Losses: 632, Epsilon: 0.3889, Steps: 18292, Time: 61.33s\n",
      "Ações: Manter=6503, Comprar=6472, Vender=5317\n",
      "Ganhos Totais: 28235.50, Perdas Totais: -27291.50\n",
      "Modelo e log do episódio 25 salvos em: 4.7.5\\model_episode_25.pth e 4.7.5\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -3140.75, Win Rate: 0.52, Wins: 651, Losses: 607, Epsilon: 0.3850, Steps: 18292, Time: 61.38s\n",
      "Ações: Manter=6362, Comprar=6078, Vender=5852\n",
      "Ganhos Totais: 25498.25, Perdas Totais: -28639.00\n",
      "Episode 27/100, Total Reward: -389.50, Win Rate: 0.52, Wins: 659, Losses: 618, Epsilon: 0.3812, Steps: 18292, Time: 61.12s\n",
      "Ações: Manter=5364, Comprar=6388, Vender=6540\n",
      "Ganhos Totais: 27108.75, Perdas Totais: -27498.25\n",
      "Modelo e log do episódio 27 salvos em: 4.7.5\\model_episode_27.pth e 4.7.5\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: 1767.00, Win Rate: 0.52, Wins: 703, Losses: 643, Epsilon: 0.3774, Steps: 18292, Time: 61.41s\n",
      "Ações: Manter=6026, Comprar=5587, Vender=6679\n",
      "Ganhos Totais: 28158.75, Perdas Totais: -26391.75\n",
      "Modelo e log do episódio 28 salvos em: 4.7.5\\model_episode_28.pth e 4.7.5\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 2070.25, Win Rate: 0.52, Wins: 668, Losses: 626, Epsilon: 0.3736, Steps: 18292, Time: 61.07s\n",
      "Ações: Manter=6347, Comprar=5567, Vender=6378\n",
      "Ganhos Totais: 28038.75, Perdas Totais: -25968.50\n",
      "Modelo e log do episódio 29 salvos em: 4.7.5\\model_episode_29.pth e 4.7.5\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: 1682.25, Win Rate: 0.51, Wins: 663, Losses: 634, Epsilon: 0.3699, Steps: 18292, Time: 61.38s\n",
      "Ações: Manter=5987, Comprar=6020, Vender=6285\n",
      "Ganhos Totais: 28297.75, Perdas Totais: -26615.50\n",
      "Modelo e log do episódio 30 salvos em: 4.7.5\\model_episode_30.pth e 4.7.5\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -2573.25, Win Rate: 0.49, Wins: 666, Losses: 704, Epsilon: 0.3662, Steps: 18292, Time: 61.50s\n",
      "Ações: Manter=5269, Comprar=6627, Vender=6396\n",
      "Ganhos Totais: 27251.50, Perdas Totais: -29824.75\n",
      "Episode 32/100, Total Reward: 1863.50, Win Rate: 0.53, Wins: 728, Losses: 642, Epsilon: 0.3625, Steps: 18292, Time: 61.54s\n",
      "Ações: Manter=5868, Comprar=6237, Vender=6187\n",
      "Ganhos Totais: 28222.50, Perdas Totais: -26359.00\n",
      "Modelo e log do episódio 32 salvos em: 4.7.5\\model_episode_32.pth e 4.7.5\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 459.00, Win Rate: 0.53, Wins: 694, Losses: 615, Epsilon: 0.3589, Steps: 18292, Time: 61.18s\n",
      "Ações: Manter=5801, Comprar=5409, Vender=7082\n",
      "Ganhos Totais: 26936.75, Perdas Totais: -26477.75\n",
      "Modelo e log do episódio 33 salvos em: 4.7.5\\model_episode_33.pth e 4.7.5\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: -3301.25, Win Rate: 0.50, Wins: 672, Losses: 668, Epsilon: 0.3553, Steps: 18292, Time: 61.13s\n",
      "Ações: Manter=6117, Comprar=5582, Vender=6593\n",
      "Ganhos Totais: 26454.75, Perdas Totais: -29756.00\n",
      "Episode 35/100, Total Reward: -1834.25, Win Rate: 0.52, Wins: 666, Losses: 612, Epsilon: 0.3517, Steps: 18292, Time: 61.61s\n",
      "Ações: Manter=6741, Comprar=5797, Vender=5754\n",
      "Ganhos Totais: 26635.50, Perdas Totais: -28469.75\n",
      "Episode 36/100, Total Reward: -3862.50, Win Rate: 0.50, Wins: 626, Losses: 615, Epsilon: 0.3482, Steps: 18292, Time: 62.62s\n",
      "Ações: Manter=7654, Comprar=4968, Vender=5670\n",
      "Ganhos Totais: 24223.75, Perdas Totais: -28086.25\n",
      "Episode 37/100, Total Reward: 3619.75, Win Rate: 0.54, Wins: 732, Losses: 629, Epsilon: 0.3447, Steps: 18292, Time: 61.52s\n",
      "Ações: Manter=5864, Comprar=6697, Vender=5731\n",
      "Ganhos Totais: 29346.75, Perdas Totais: -25727.00\n",
      "Modelo e log do episódio 37 salvos em: 4.7.5\\model_episode_37.pth e 4.7.5\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: -875.00, Win Rate: 0.49, Wins: 668, Losses: 692, Epsilon: 0.3413, Steps: 18292, Time: 61.30s\n",
      "Ações: Manter=5194, Comprar=5989, Vender=7109\n",
      "Ganhos Totais: 27719.00, Perdas Totais: -28594.00\n",
      "Episode 39/100, Total Reward: 1800.00, Win Rate: 0.54, Wins: 689, Losses: 580, Epsilon: 0.3379, Steps: 18292, Time: 61.27s\n",
      "Ações: Manter=6491, Comprar=5834, Vender=5967\n",
      "Ganhos Totais: 27934.25, Perdas Totais: -26134.25\n",
      "Modelo e log do episódio 39 salvos em: 4.7.5\\model_episode_39.pth e 4.7.5\\log_episode_39.csv\n",
      "\n",
      "Episode 40/100, Total Reward: -3631.75, Win Rate: 0.50, Wins: 633, Losses: 640, Epsilon: 0.3345, Steps: 18292, Time: 61.65s\n",
      "Ações: Manter=5673, Comprar=6959, Vender=5660\n",
      "Ganhos Totais: 24808.00, Perdas Totais: -28439.75\n",
      "Episode 41/100, Total Reward: -819.50, Win Rate: 0.52, Wins: 682, Losses: 641, Epsilon: 0.3311, Steps: 18292, Time: 61.72s\n",
      "Ações: Manter=6111, Comprar=5715, Vender=6466\n",
      "Ganhos Totais: 27510.50, Perdas Totais: -28330.00\n",
      "Episode 42/100, Total Reward: -78.25, Win Rate: 0.52, Wins: 695, Losses: 651, Epsilon: 0.3278, Steps: 18292, Time: 63.24s\n",
      "Ações: Manter=5760, Comprar=6586, Vender=5946\n",
      "Ganhos Totais: 27888.75, Perdas Totais: -27967.00\n",
      "Episode 43/100, Total Reward: -1006.00, Win Rate: 0.51, Wins: 630, Losses: 610, Epsilon: 0.3246, Steps: 18292, Time: 61.13s\n",
      "Ações: Manter=6736, Comprar=5807, Vender=5749\n",
      "Ganhos Totais: 25259.25, Perdas Totais: -26265.25\n",
      "Episode 44/100, Total Reward: -225.50, Win Rate: 0.52, Wins: 684, Losses: 644, Epsilon: 0.3213, Steps: 18292, Time: 63.52s\n",
      "Ações: Manter=4979, Comprar=7317, Vender=5996\n",
      "Ganhos Totais: 27879.00, Perdas Totais: -28104.50\n",
      "Episode 45/100, Total Reward: -1299.25, Win Rate: 0.50, Wins: 600, Losses: 601, Epsilon: 0.3181, Steps: 18292, Time: 63.72s\n",
      "Ações: Manter=6420, Comprar=6235, Vender=5637\n",
      "Ganhos Totais: 26102.50, Perdas Totais: -27401.75\n",
      "Episode 46/100, Total Reward: -2020.50, Win Rate: 0.51, Wins: 695, Losses: 676, Epsilon: 0.3149, Steps: 18292, Time: 64.09s\n",
      "Ações: Manter=4653, Comprar=7813, Vender=5826\n",
      "Ganhos Totais: 27385.00, Perdas Totais: -29405.50\n",
      "Episode 47/100, Total Reward: 4979.25, Win Rate: 0.54, Wins: 679, Losses: 584, Epsilon: 0.3118, Steps: 18292, Time: 64.39s\n",
      "Ações: Manter=6361, Comprar=6339, Vender=5592\n",
      "Ganhos Totais: 29321.75, Perdas Totais: -24342.50\n",
      "Modelo e log do episódio 47 salvos em: 4.7.5\\model_episode_47.pth e 4.7.5\\log_episode_47.csv\n",
      "\n",
      "Episode 48/100, Total Reward: -1046.50, Win Rate: 0.55, Wins: 697, Losses: 578, Epsilon: 0.3086, Steps: 18292, Time: 64.83s\n",
      "Ações: Manter=5894, Comprar=6874, Vender=5524\n",
      "Ganhos Totais: 26411.75, Perdas Totais: -27458.25\n",
      "Episode 49/100, Total Reward: -3656.50, Win Rate: 0.49, Wins: 554, Losses: 586, Epsilon: 0.3056, Steps: 18292, Time: 64.09s\n",
      "Ações: Manter=6149, Comprar=6181, Vender=5962\n",
      "Ganhos Totais: 24956.50, Perdas Totais: -28613.00\n",
      "Episode 50/100, Total Reward: -549.75, Win Rate: 0.54, Wins: 705, Losses: 607, Epsilon: 0.3025, Steps: 18292, Time: 64.20s\n",
      "Ações: Manter=7001, Comprar=5279, Vender=6012\n",
      "Ganhos Totais: 27000.25, Perdas Totais: -27550.00\n",
      "Episode 51/100, Total Reward: -1848.75, Win Rate: 0.52, Wins: 671, Losses: 629, Epsilon: 0.2995, Steps: 18292, Time: 64.43s\n",
      "Ações: Manter=5755, Comprar=7439, Vender=5098\n",
      "Ganhos Totais: 25672.50, Perdas Totais: -27521.25\n",
      "Episode 52/100, Total Reward: -1378.75, Win Rate: 0.50, Wins: 628, Losses: 629, Epsilon: 0.2965, Steps: 18292, Time: 65.02s\n",
      "Ações: Manter=7094, Comprar=5734, Vender=5464\n",
      "Ganhos Totais: 26491.00, Perdas Totais: -27869.75\n",
      "Episode 53/100, Total Reward: -994.50, Win Rate: 0.54, Wins: 702, Losses: 588, Epsilon: 0.2935, Steps: 18292, Time: 63.25s\n",
      "Ações: Manter=5710, Comprar=6766, Vender=5816\n",
      "Ganhos Totais: 27429.50, Perdas Totais: -28424.00\n",
      "Episode 54/100, Total Reward: -4926.25, Win Rate: 0.49, Wins: 592, Losses: 605, Epsilon: 0.2906, Steps: 18292, Time: 63.62s\n",
      "Ações: Manter=5347, Comprar=7879, Vender=5066\n",
      "Ganhos Totais: 24487.75, Perdas Totais: -29414.00\n",
      "Episode 55/100, Total Reward: -87.25, Win Rate: 0.52, Wins: 669, Losses: 621, Epsilon: 0.2877, Steps: 18292, Time: 63.24s\n",
      "Ações: Manter=5929, Comprar=6802, Vender=5561\n",
      "Ganhos Totais: 27593.25, Perdas Totais: -27680.50\n",
      "Episode 56/100, Total Reward: -4047.75, Win Rate: 0.52, Wins: 713, Losses: 664, Epsilon: 0.2848, Steps: 18292, Time: 63.40s\n",
      "Ações: Manter=4894, Comprar=7502, Vender=5896\n",
      "Ganhos Totais: 26084.50, Perdas Totais: -30132.25\n",
      "Episode 57/100, Total Reward: -3757.75, Win Rate: 0.48, Wins: 587, Losses: 643, Epsilon: 0.2820, Steps: 18292, Time: 64.65s\n",
      "Ações: Manter=6438, Comprar=6119, Vender=5735\n",
      "Ganhos Totais: 24460.75, Perdas Totais: -28218.50\n",
      "Episode 58/100, Total Reward: 379.50, Win Rate: 0.52, Wins: 602, Losses: 562, Epsilon: 0.2791, Steps: 18292, Time: 64.43s\n",
      "Ações: Manter=5879, Comprar=8164, Vender=4249\n",
      "Ganhos Totais: 26823.00, Perdas Totais: -26443.50\n",
      "Episode 59/100, Total Reward: -2817.75, Win Rate: 0.49, Wins: 533, Losses: 550, Epsilon: 0.2763, Steps: 18292, Time: 64.15s\n",
      "Ações: Manter=7966, Comprar=5972, Vender=4354\n",
      "Ganhos Totais: 23777.50, Perdas Totais: -26595.25\n",
      "Episode 60/100, Total Reward: -2529.25, Win Rate: 0.50, Wins: 616, Losses: 607, Epsilon: 0.2736, Steps: 18292, Time: 63.47s\n",
      "Ações: Manter=6840, Comprar=5715, Vender=5737\n",
      "Ganhos Totais: 25353.00, Perdas Totais: -27882.25\n",
      "Episode 61/100, Total Reward: 2643.75, Win Rate: 0.54, Wins: 626, Losses: 531, Epsilon: 0.2708, Steps: 18292, Time: 63.35s\n",
      "Ações: Manter=7083, Comprar=6873, Vender=4336\n",
      "Ganhos Totais: 27436.50, Perdas Totais: -24792.75\n",
      "Modelo e log do episódio 61 salvos em: 4.7.5\\model_episode_61.pth e 4.7.5\\log_episode_61.csv\n",
      "\n",
      "Episode 62/100, Total Reward: -761.00, Win Rate: 0.52, Wins: 591, Losses: 535, Epsilon: 0.2681, Steps: 18292, Time: 63.49s\n",
      "Ações: Manter=7854, Comprar=6532, Vender=3906\n",
      "Ganhos Totais: 25742.25, Perdas Totais: -26503.25\n",
      "Episode 63/100, Total Reward: -2846.75, Win Rate: 0.53, Wins: 642, Losses: 576, Epsilon: 0.2655, Steps: 18292, Time: 64.75s\n",
      "Ações: Manter=5723, Comprar=8166, Vender=4403\n",
      "Ganhos Totais: 26026.75, Perdas Totais: -28873.50\n",
      "Episode 64/100, Total Reward: -1277.00, Win Rate: 0.52, Wins: 624, Losses: 584, Epsilon: 0.2628, Steps: 18292, Time: 64.78s\n",
      "Ações: Manter=7198, Comprar=6459, Vender=4635\n",
      "Ganhos Totais: 25817.00, Perdas Totais: -27094.00\n",
      "Episode 65/100, Total Reward: 408.75, Win Rate: 0.53, Wins: 682, Losses: 597, Epsilon: 0.2602, Steps: 18292, Time: 64.94s\n",
      "Ações: Manter=6706, Comprar=6867, Vender=4719\n",
      "Ganhos Totais: 26790.25, Perdas Totais: -26381.50\n",
      "Episode 66/100, Total Reward: 2219.75, Win Rate: 0.57, Wins: 747, Losses: 563, Epsilon: 0.2576, Steps: 18292, Time: 63.66s\n",
      "Ações: Manter=5006, Comprar=8167, Vender=5119\n",
      "Ganhos Totais: 28987.25, Perdas Totais: -26767.50\n",
      "Modelo e log do episódio 66 salvos em: 4.7.5\\model_episode_66.pth e 4.7.5\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: 912.00, Win Rate: 0.55, Wins: 708, Losses: 571, Epsilon: 0.2550, Steps: 18292, Time: 63.34s\n",
      "Ações: Manter=4369, Comprar=8628, Vender=5295\n",
      "Ganhos Totais: 27405.00, Perdas Totais: -26493.00\n",
      "Episode 68/100, Total Reward: -597.00, Win Rate: 0.53, Wins: 664, Losses: 590, Epsilon: 0.2524, Steps: 18292, Time: 64.92s\n",
      "Ações: Manter=4874, Comprar=7445, Vender=5973\n",
      "Ganhos Totais: 26184.75, Perdas Totais: -26781.75\n",
      "Episode 69/100, Total Reward: 427.50, Win Rate: 0.53, Wins: 691, Losses: 601, Epsilon: 0.2499, Steps: 18292, Time: 64.81s\n",
      "Ações: Manter=4704, Comprar=8698, Vender=4890\n",
      "Ganhos Totais: 27839.50, Perdas Totais: -27412.00\n",
      "Episode 70/100, Total Reward: 2507.25, Win Rate: 0.52, Wins: 655, Losses: 608, Epsilon: 0.2474, Steps: 18292, Time: 65.08s\n",
      "Ações: Manter=5344, Comprar=7653, Vender=5295\n",
      "Ganhos Totais: 28498.75, Perdas Totais: -25991.50\n",
      "Modelo e log do episódio 70 salvos em: 4.7.5\\model_episode_70.pth e 4.7.5\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: 2201.00, Win Rate: 0.54, Wins: 705, Losses: 608, Epsilon: 0.2449, Steps: 18292, Time: 63.73s\n",
      "Ações: Manter=5145, Comprar=7886, Vender=5261\n",
      "Ganhos Totais: 29784.25, Perdas Totais: -27583.25\n",
      "Modelo e log do episódio 71 salvos em: 4.7.5\\model_episode_71.pth e 4.7.5\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: -690.75, Win Rate: 0.52, Wins: 621, Losses: 568, Epsilon: 0.2425, Steps: 18292, Time: 63.22s\n",
      "Ações: Manter=7323, Comprar=5809, Vender=5160\n",
      "Ganhos Totais: 26005.50, Perdas Totais: -26696.25\n",
      "Episode 73/100, Total Reward: 2553.50, Win Rate: 0.54, Wins: 702, Losses: 603, Epsilon: 0.2401, Steps: 18292, Time: 64.00s\n",
      "Ações: Manter=5392, Comprar=7347, Vender=5553\n",
      "Ganhos Totais: 28971.00, Perdas Totais: -26417.50\n",
      "Modelo e log do episódio 73 salvos em: 4.7.5\\model_episode_73.pth e 4.7.5\\log_episode_73.csv\n",
      "\n",
      "Episode 74/100, Total Reward: -1274.00, Win Rate: 0.53, Wins: 678, Losses: 595, Epsilon: 0.2377, Steps: 18292, Time: 64.62s\n",
      "Ações: Manter=5226, Comprar=7913, Vender=5153\n",
      "Ganhos Totais: 27262.75, Perdas Totais: -28536.75\n",
      "Episode 75/100, Total Reward: 83.00, Win Rate: 0.52, Wins: 640, Losses: 580, Epsilon: 0.2353, Steps: 18292, Time: 64.78s\n",
      "Ações: Manter=6519, Comprar=7301, Vender=4472\n",
      "Ganhos Totais: 26754.25, Perdas Totais: -26671.25\n",
      "Episode 76/100, Total Reward: -1185.00, Win Rate: 0.52, Wins: 560, Losses: 514, Epsilon: 0.2329, Steps: 18292, Time: 64.76s\n",
      "Ações: Manter=7062, Comprar=6279, Vender=4951\n",
      "Ganhos Totais: 26202.25, Perdas Totais: -27387.25\n",
      "Episode 77/100, Total Reward: -1118.75, Win Rate: 0.52, Wins: 612, Losses: 555, Epsilon: 0.2306, Steps: 18292, Time: 63.70s\n",
      "Ações: Manter=7296, Comprar=6330, Vender=4666\n",
      "Ganhos Totais: 25072.50, Perdas Totais: -26191.25\n",
      "Episode 78/100, Total Reward: 866.50, Win Rate: 0.53, Wins: 624, Losses: 546, Epsilon: 0.2283, Steps: 18292, Time: 63.68s\n",
      "Ações: Manter=6802, Comprar=7057, Vender=4433\n",
      "Ganhos Totais: 26811.00, Perdas Totais: -25944.50\n",
      "Episode 79/100, Total Reward: -1501.50, Win Rate: 0.52, Wins: 654, Losses: 604, Epsilon: 0.2260, Steps: 18292, Time: 64.79s\n",
      "Ações: Manter=5486, Comprar=6592, Vender=6214\n",
      "Ganhos Totais: 26003.00, Perdas Totais: -27504.50\n",
      "Episode 80/100, Total Reward: 4642.25, Win Rate: 0.56, Wins: 715, Losses: 568, Epsilon: 0.2238, Steps: 18292, Time: 64.48s\n",
      "Ações: Manter=5186, Comprar=8325, Vender=4781\n",
      "Ganhos Totais: 28954.25, Perdas Totais: -24312.00\n",
      "Modelo e log do episódio 80 salvos em: 4.7.5\\model_episode_80.pth e 4.7.5\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: -1773.25, Win Rate: 0.53, Wins: 615, Losses: 550, Epsilon: 0.2215, Steps: 18292, Time: 63.89s\n",
      "Ações: Manter=5827, Comprar=6506, Vender=5959\n",
      "Ganhos Totais: 25954.75, Perdas Totais: -27728.00\n",
      "Episode 82/100, Total Reward: 168.50, Win Rate: 0.53, Wins: 589, Losses: 530, Epsilon: 0.2193, Steps: 18292, Time: 63.57s\n",
      "Ações: Manter=6055, Comprar=6068, Vender=6169\n",
      "Ganhos Totais: 26513.75, Perdas Totais: -26345.25\n",
      "Episode 83/100, Total Reward: 1062.00, Win Rate: 0.54, Wins: 618, Losses: 525, Epsilon: 0.2171, Steps: 18292, Time: 63.82s\n",
      "Ações: Manter=6164, Comprar=7639, Vender=4489\n",
      "Ganhos Totais: 26794.75, Perdas Totais: -25732.75\n",
      "Episode 84/100, Total Reward: -2372.75, Win Rate: 0.51, Wins: 581, Losses: 556, Epsilon: 0.2149, Steps: 18292, Time: 64.22s\n",
      "Ações: Manter=5451, Comprar=8817, Vender=4024\n",
      "Ganhos Totais: 24538.75, Perdas Totais: -26911.50\n",
      "Episode 85/100, Total Reward: 3027.25, Win Rate: 0.53, Wins: 676, Losses: 596, Epsilon: 0.2128, Steps: 18292, Time: 65.07s\n",
      "Ações: Manter=4947, Comprar=9472, Vender=3873\n",
      "Ganhos Totais: 28796.50, Perdas Totais: -25769.25\n",
      "Modelo e log do episódio 85 salvos em: 4.7.5\\model_episode_85.pth e 4.7.5\\log_episode_85.csv\n",
      "\n",
      "Episode 86/100, Total Reward: 2093.50, Win Rate: 0.54, Wins: 643, Losses: 545, Epsilon: 0.2107, Steps: 18292, Time: 65.25s\n",
      "Ações: Manter=5060, Comprar=9147, Vender=4085\n",
      "Ganhos Totais: 28003.00, Perdas Totais: -25909.50\n",
      "Modelo e log do episódio 86 salvos em: 4.7.5\\model_episode_86.pth e 4.7.5\\log_episode_86.csv\n",
      "\n",
      "Episode 87/100, Total Reward: 2628.25, Win Rate: 0.53, Wins: 678, Losses: 607, Epsilon: 0.2086, Steps: 18292, Time: 65.39s\n",
      "Ações: Manter=6198, Comprar=6904, Vender=5190\n",
      "Ganhos Totais: 28793.25, Perdas Totais: -26165.00\n",
      "Modelo e log do episódio 87 salvos em: 4.7.5\\model_episode_87.pth e 4.7.5\\log_episode_87.csv\n",
      "\n",
      "Episode 88/100, Total Reward: -240.50, Win Rate: 0.54, Wins: 608, Losses: 508, Epsilon: 0.2065, Steps: 18292, Time: 65.21s\n",
      "Ações: Manter=6755, Comprar=6835, Vender=4702\n",
      "Ganhos Totais: 25460.50, Perdas Totais: -25701.00\n",
      "Episode 89/100, Total Reward: 2760.50, Win Rate: 0.56, Wins: 641, Losses: 511, Epsilon: 0.2044, Steps: 18292, Time: 64.06s\n",
      "Ações: Manter=6529, Comprar=7471, Vender=4292\n",
      "Ganhos Totais: 27204.50, Perdas Totais: -24444.00\n",
      "Modelo e log do episódio 89 salvos em: 4.7.5\\model_episode_89.pth e 4.7.5\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: 3814.75, Win Rate: 0.55, Wins: 696, Losses: 577, Epsilon: 0.2024, Steps: 18292, Time: 64.13s\n",
      "Ações: Manter=4863, Comprar=8791, Vender=4638\n",
      "Ganhos Totais: 29161.00, Perdas Totais: -25346.25\n",
      "Modelo e log do episódio 90 salvos em: 4.7.5\\model_episode_90.pth e 4.7.5\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: 4206.25, Win Rate: 0.56, Wins: 703, Losses: 556, Epsilon: 0.2003, Steps: 18292, Time: 63.54s\n",
      "Ações: Manter=5169, Comprar=9013, Vender=4110\n",
      "Ganhos Totais: 30063.00, Perdas Totais: -25856.75\n",
      "Modelo e log do episódio 91 salvos em: 4.7.5\\model_episode_91.pth e 4.7.5\\log_episode_91.csv\n",
      "\n",
      "Episode 92/100, Total Reward: 1610.00, Win Rate: 0.53, Wins: 579, Losses: 510, Epsilon: 0.1983, Steps: 18292, Time: 64.45s\n",
      "Ações: Manter=6895, Comprar=7038, Vender=4359\n",
      "Ganhos Totais: 26850.25, Perdas Totais: -25240.25\n",
      "Episode 93/100, Total Reward: -827.25, Win Rate: 0.53, Wins: 608, Losses: 544, Epsilon: 0.1964, Steps: 18292, Time: 64.82s\n",
      "Ações: Manter=7350, Comprar=7140, Vender=3802\n",
      "Ganhos Totais: 26849.75, Perdas Totais: -27677.00\n",
      "Episode 94/100, Total Reward: 2916.00, Win Rate: 0.57, Wins: 715, Losses: 530, Epsilon: 0.1944, Steps: 18292, Time: 64.14s\n",
      "Ações: Manter=6043, Comprar=8228, Vender=4021\n",
      "Ganhos Totais: 28965.75, Perdas Totais: -26049.75\n",
      "Modelo e log do episódio 94 salvos em: 4.7.5\\model_episode_94.pth e 4.7.5\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: 4646.75, Win Rate: 0.55, Wins: 699, Losses: 583, Epsilon: 0.1924, Steps: 18292, Time: 64.39s\n",
      "Ações: Manter=4554, Comprar=8748, Vender=4990\n",
      "Ganhos Totais: 31204.00, Perdas Totais: -26557.25\n",
      "Modelo e log do episódio 95 salvos em: 4.7.5\\model_episode_95.pth e 4.7.5\\log_episode_95.csv\n",
      "\n",
      "Episode 96/100, Total Reward: 5125.50, Win Rate: 0.54, Wins: 635, Losses: 536, Epsilon: 0.1905, Steps: 18292, Time: 64.10s\n",
      "Ações: Manter=6863, Comprar=6913, Vender=4516\n",
      "Ganhos Totais: 28497.75, Perdas Totais: -23372.25\n",
      "Modelo e log do episódio 96 salvos em: 4.7.5\\model_episode_96.pth e 4.7.5\\log_episode_96.csv\n",
      "\n",
      "Episode 97/100, Total Reward: 1702.00, Win Rate: 0.54, Wins: 677, Losses: 579, Epsilon: 0.1886, Steps: 18292, Time: 63.94s\n",
      "Ações: Manter=4732, Comprar=9699, Vender=3861\n",
      "Ganhos Totais: 28971.75, Perdas Totais: -27269.75\n",
      "Episode 98/100, Total Reward: 3983.50, Win Rate: 0.54, Wins: 597, Losses: 510, Epsilon: 0.1867, Steps: 18292, Time: 64.81s\n",
      "Ações: Manter=7327, Comprar=6824, Vender=4141\n",
      "Ganhos Totais: 27704.00, Perdas Totais: -23720.50\n",
      "Modelo e log do episódio 98 salvos em: 4.7.5\\model_episode_98.pth e 4.7.5\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: 5411.75, Win Rate: 0.56, Wins: 602, Losses: 473, Epsilon: 0.1849, Steps: 18292, Time: 65.21s\n",
      "Ações: Manter=6764, Comprar=6917, Vender=4611\n",
      "Ganhos Totais: 28383.25, Perdas Totais: -22971.50\n",
      "Modelo e log do episódio 99 salvos em: 4.7.5\\model_episode_99.pth e 4.7.5\\log_episode_99.csv\n",
      "\n",
      "Episode 100/100, Total Reward: 1459.25, Win Rate: 0.55, Wins: 616, Losses: 510, Epsilon: 0.1830, Steps: 18292, Time: 65.01s\n",
      "Ações: Manter=6954, Comprar=7595, Vender=3743\n",
      "Ganhos Totais: 26810.00, Perdas Totais: -25350.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 99, Total Reward: 5411.75, Win Rate: 0.56, Wins: 602, Losses: 473, Ações: {0: 6764, 1: 6917, 2: 4611}, Steps: 18292, Time: 65.21s\n",
      "Rank 2: Episode 96, Total Reward: 5125.50, Win Rate: 0.54, Wins: 635, Losses: 536, Ações: {0: 6863, 1: 6913, 2: 4516}, Steps: 18292, Time: 64.10s\n",
      "Rank 3: Episode 47, Total Reward: 4979.25, Win Rate: 0.54, Wins: 679, Losses: 584, Ações: {0: 6361, 1: 6339, 2: 5592}, Steps: 18292, Time: 64.39s\n",
      "Rank 4: Episode 95, Total Reward: 4646.75, Win Rate: 0.55, Wins: 699, Losses: 583, Ações: {0: 4554, 1: 8748, 2: 4990}, Steps: 18292, Time: 64.39s\n",
      "Rank 5: Episode 80, Total Reward: 4642.25, Win Rate: 0.56, Wins: 715, Losses: 568, Ações: {0: 5186, 1: 8325, 2: 4781}, Steps: 18292, Time: 64.48s\n",
      "Rank 6: Episode 91, Total Reward: 4206.25, Win Rate: 0.56, Wins: 703, Losses: 556, Ações: {0: 5169, 1: 9013, 2: 4110}, Steps: 18292, Time: 63.54s\n",
      "Rank 7: Episode 98, Total Reward: 3983.50, Win Rate: 0.54, Wins: 597, Losses: 510, Ações: {0: 7327, 1: 6824, 2: 4141}, Steps: 18292, Time: 64.81s\n",
      "Rank 8: Episode 90, Total Reward: 3814.75, Win Rate: 0.55, Wins: 696, Losses: 577, Ações: {0: 4863, 1: 8791, 2: 4638}, Steps: 18292, Time: 64.13s\n",
      "Rank 9: Episode 37, Total Reward: 3619.75, Win Rate: 0.54, Wins: 732, Losses: 629, Ações: {0: 5864, 1: 6697, 2: 5731}, Steps: 18292, Time: 61.52s\n",
      "Rank 10: Episode 85, Total Reward: 3027.25, Win Rate: 0.53, Wins: 676, Losses: 596, Ações: {0: 4947, 1: 9472, 2: 3873}, Steps: 18292, Time: 65.07s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M30_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.5\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
