{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -571.75, Win Rate: 0.51, Wins: 1414, Losses: 1376, Epsilon: 0.4950, Steps: 36754, Time: 136.79s\n",
      "Ações: Manter=11977, Comprar=12320, Vender=12457\n",
      "Ganhos Totais: 37151.50, Perdas Totais: -37723.25\n",
      "Modelo e log do episódio 1 salvos em: 4.7.1\\model_episode_1.pth e 4.7.1\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -4133.25, Win Rate: 0.50, Wins: 1335, Losses: 1347, Epsilon: 0.4900, Steps: 36754, Time: 138.50s\n",
      "Ações: Manter=12322, Comprar=12055, Vender=12377\n",
      "Ganhos Totais: 35494.75, Perdas Totais: -39628.00\n",
      "Modelo e log do episódio 2 salvos em: 4.7.1\\model_episode_2.pth e 4.7.1\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -863.00, Win Rate: 0.52, Wins: 1384, Losses: 1295, Epsilon: 0.4851, Steps: 36754, Time: 131.72s\n",
      "Ações: Manter=12087, Comprar=11967, Vender=12700\n",
      "Ganhos Totais: 37502.75, Perdas Totais: -38365.75\n",
      "Modelo e log do episódio 3 salvos em: 4.7.1\\model_episode_3.pth e 4.7.1\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -2827.00, Win Rate: 0.51, Wins: 1329, Losses: 1280, Epsilon: 0.4803, Steps: 36754, Time: 131.67s\n",
      "Ações: Manter=11887, Comprar=11970, Vender=12897\n",
      "Ganhos Totais: 35673.00, Perdas Totais: -38500.00\n",
      "Modelo e log do episódio 4 salvos em: 4.7.1\\model_episode_4.pth e 4.7.1\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: 621.25, Win Rate: 0.53, Wins: 1375, Losses: 1242, Epsilon: 0.4755, Steps: 36754, Time: 170.14s\n",
      "Ações: Manter=11694, Comprar=12245, Vender=12815\n",
      "Ganhos Totais: 38026.50, Perdas Totais: -37405.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.1\\model_episode_5.pth e 4.7.1\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -727.50, Win Rate: 0.53, Wins: 1379, Losses: 1235, Epsilon: 0.4707, Steps: 36754, Time: 154.36s\n",
      "Ações: Manter=11703, Comprar=11679, Vender=13372\n",
      "Ganhos Totais: 37357.50, Perdas Totais: -38085.00\n",
      "Modelo e log do episódio 6 salvos em: 4.7.1\\model_episode_6.pth e 4.7.1\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -893.25, Win Rate: 0.53, Wins: 1370, Losses: 1204, Epsilon: 0.4660, Steps: 36754, Time: 166.70s\n",
      "Ações: Manter=11904, Comprar=12210, Vender=12640\n",
      "Ganhos Totais: 36538.75, Perdas Totais: -37432.00\n",
      "Modelo e log do episódio 7 salvos em: 4.7.1\\model_episode_7.pth e 4.7.1\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 629.00, Win Rate: 0.53, Wins: 1341, Losses: 1182, Epsilon: 0.4614, Steps: 36754, Time: 164.28s\n",
      "Ações: Manter=12146, Comprar=11917, Vender=12691\n",
      "Ganhos Totais: 37514.75, Perdas Totais: -36885.75\n",
      "Modelo e log do episódio 8 salvos em: 4.7.1\\model_episode_8.pth e 4.7.1\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -3568.00, Win Rate: 0.52, Wins: 1321, Losses: 1214, Epsilon: 0.4568, Steps: 36754, Time: 162.96s\n",
      "Ações: Manter=11621, Comprar=12578, Vender=12555\n",
      "Ganhos Totais: 35009.50, Perdas Totais: -38577.50\n",
      "Modelo e log do episódio 9 salvos em: 4.7.1\\model_episode_9.pth e 4.7.1\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -1347.75, Win Rate: 0.52, Wins: 1352, Losses: 1265, Epsilon: 0.4522, Steps: 36754, Time: 166.59s\n",
      "Ações: Manter=11507, Comprar=12304, Vender=12943\n",
      "Ganhos Totais: 36969.25, Perdas Totais: -38317.00\n",
      "Modelo e log do episódio 10 salvos em: 4.7.1\\model_episode_10.pth e 4.7.1\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -934.25, Win Rate: 0.53, Wins: 1365, Losses: 1232, Epsilon: 0.4477, Steps: 36754, Time: 158.58s\n",
      "Ações: Manter=11515, Comprar=12476, Vender=12763\n",
      "Ganhos Totais: 37139.50, Perdas Totais: -38073.75\n",
      "Modelo e log do episódio 11 salvos em: 4.7.1\\model_episode_11.pth e 4.7.1\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 3746.50, Win Rate: 0.54, Wins: 1377, Losses: 1191, Epsilon: 0.4432, Steps: 36754, Time: 156.32s\n",
      "Ações: Manter=11571, Comprar=12277, Vender=12906\n",
      "Ganhos Totais: 40182.75, Perdas Totais: -36436.25\n",
      "Modelo e log do episódio 12 salvos em: 4.7.1\\model_episode_12.pth e 4.7.1\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -1006.50, Win Rate: 0.55, Wins: 1440, Losses: 1202, Epsilon: 0.4388, Steps: 36754, Time: 157.52s\n",
      "Ações: Manter=11465, Comprar=12192, Vender=13097\n",
      "Ganhos Totais: 35495.50, Perdas Totais: -36502.00\n",
      "Modelo e log do episódio 13 salvos em: 4.7.1\\model_episode_13.pth e 4.7.1\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 808.50, Win Rate: 0.54, Wins: 1394, Losses: 1194, Epsilon: 0.4344, Steps: 36754, Time: 159.12s\n",
      "Ações: Manter=11639, Comprar=12527, Vender=12588\n",
      "Ganhos Totais: 38045.00, Perdas Totais: -37236.50\n",
      "Modelo e log do episódio 14 salvos em: 4.7.1\\model_episode_14.pth e 4.7.1\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -2721.50, Win Rate: 0.52, Wins: 1336, Losses: 1211, Epsilon: 0.4300, Steps: 36754, Time: 157.63s\n",
      "Ações: Manter=11831, Comprar=12071, Vender=12852\n",
      "Ganhos Totais: 36048.50, Perdas Totais: -38770.00\n",
      "Episode 16/100, Total Reward: -411.75, Win Rate: 0.53, Wins: 1327, Losses: 1187, Epsilon: 0.4257, Steps: 36754, Time: 159.29s\n",
      "Ações: Manter=11682, Comprar=12613, Vender=12459\n",
      "Ganhos Totais: 35642.50, Perdas Totais: -36054.25\n",
      "Modelo e log do episódio 16 salvos em: 4.7.1\\model_episode_16.pth e 4.7.1\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -1286.25, Win Rate: 0.53, Wins: 1356, Losses: 1192, Epsilon: 0.4215, Steps: 36754, Time: 158.89s\n",
      "Ações: Manter=11428, Comprar=12158, Vender=13168\n",
      "Ganhos Totais: 35926.75, Perdas Totais: -37213.00\n",
      "Episode 18/100, Total Reward: 416.00, Win Rate: 0.54, Wins: 1415, Losses: 1211, Epsilon: 0.4173, Steps: 36754, Time: 159.64s\n",
      "Ações: Manter=10695, Comprar=12195, Vender=13864\n",
      "Ganhos Totais: 37806.00, Perdas Totais: -37390.00\n",
      "Modelo e log do episódio 18 salvos em: 4.7.1\\model_episode_18.pth e 4.7.1\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -3141.50, Win Rate: 0.53, Wins: 1368, Losses: 1221, Epsilon: 0.4131, Steps: 36754, Time: 160.92s\n",
      "Ações: Manter=11457, Comprar=11782, Vender=13515\n",
      "Ganhos Totais: 34987.50, Perdas Totais: -38129.00\n",
      "Episode 20/100, Total Reward: -2898.75, Win Rate: 0.54, Wins: 1377, Losses: 1183, Epsilon: 0.4090, Steps: 36754, Time: 131.78s\n",
      "Ações: Manter=11451, Comprar=11999, Vender=13304\n",
      "Ganhos Totais: 35742.25, Perdas Totais: -38641.00\n",
      "Episode 21/100, Total Reward: -907.75, Win Rate: 0.54, Wins: 1343, Losses: 1156, Epsilon: 0.4049, Steps: 36754, Time: 117.56s\n",
      "Ações: Manter=11498, Comprar=12461, Vender=12795\n",
      "Ganhos Totais: 36930.00, Perdas Totais: -37837.75\n",
      "Episode 22/100, Total Reward: -1029.50, Win Rate: 0.55, Wins: 1444, Losses: 1205, Epsilon: 0.4008, Steps: 36754, Time: 117.79s\n",
      "Ações: Manter=10799, Comprar=12215, Vender=13740\n",
      "Ganhos Totais: 37296.00, Perdas Totais: -38325.50\n",
      "Episode 23/100, Total Reward: -2367.25, Win Rate: 0.56, Wins: 1418, Losses: 1132, Epsilon: 0.3968, Steps: 36754, Time: 114.95s\n",
      "Ações: Manter=10906, Comprar=12558, Vender=13290\n",
      "Ganhos Totais: 36457.00, Perdas Totais: -38824.25\n",
      "Episode 24/100, Total Reward: -348.25, Win Rate: 0.54, Wins: 1424, Losses: 1213, Epsilon: 0.3928, Steps: 36754, Time: 119.03s\n",
      "Ações: Manter=10592, Comprar=12035, Vender=14127\n",
      "Ganhos Totais: 37187.25, Perdas Totais: -37535.50\n",
      "Modelo e log do episódio 24 salvos em: 4.7.1\\model_episode_24.pth e 4.7.1\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 807.00, Win Rate: 0.54, Wins: 1414, Losses: 1213, Epsilon: 0.3889, Steps: 36754, Time: 120.48s\n",
      "Ações: Manter=10194, Comprar=12228, Vender=14332\n",
      "Ganhos Totais: 38864.00, Perdas Totais: -38057.00\n",
      "Modelo e log do episódio 25 salvos em: 4.7.1\\model_episode_25.pth e 4.7.1\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -3195.00, Win Rate: 0.51, Wins: 1266, Losses: 1210, Epsilon: 0.3850, Steps: 36754, Time: 131.32s\n",
      "Ações: Manter=11383, Comprar=12144, Vender=13227\n",
      "Ganhos Totais: 35641.75, Perdas Totais: -38836.75\n",
      "Episode 27/100, Total Reward: -3427.75, Win Rate: 0.53, Wins: 1355, Losses: 1182, Epsilon: 0.3812, Steps: 36754, Time: 121.76s\n",
      "Ações: Manter=10682, Comprar=11494, Vender=14578\n",
      "Ganhos Totais: 35641.50, Perdas Totais: -39069.25\n",
      "Episode 28/100, Total Reward: 344.75, Win Rate: 0.54, Wins: 1441, Losses: 1204, Epsilon: 0.3774, Steps: 36754, Time: 135.35s\n",
      "Ações: Manter=10738, Comprar=11865, Vender=14151\n",
      "Ganhos Totais: 37657.00, Perdas Totais: -37312.25\n",
      "Modelo e log do episódio 28 salvos em: 4.7.1\\model_episode_28.pth e 4.7.1\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 2194.50, Win Rate: 0.54, Wins: 1421, Losses: 1215, Epsilon: 0.3736, Steps: 36754, Time: 144.73s\n",
      "Ações: Manter=11038, Comprar=12262, Vender=13454\n",
      "Ganhos Totais: 38205.00, Perdas Totais: -36010.50\n",
      "Modelo e log do episódio 29 salvos em: 4.7.1\\model_episode_29.pth e 4.7.1\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: -821.25, Win Rate: 0.54, Wins: 1421, Losses: 1206, Epsilon: 0.3699, Steps: 36754, Time: 142.97s\n",
      "Ações: Manter=11064, Comprar=11644, Vender=14046\n",
      "Ganhos Totais: 37424.00, Perdas Totais: -38245.25\n",
      "Episode 31/100, Total Reward: -1253.25, Win Rate: 0.56, Wins: 1373, Losses: 1096, Epsilon: 0.3662, Steps: 36754, Time: 143.16s\n",
      "Ações: Manter=12213, Comprar=11023, Vender=13518\n",
      "Ganhos Totais: 35795.00, Perdas Totais: -37048.25\n",
      "Episode 32/100, Total Reward: 773.00, Win Rate: 0.55, Wins: 1540, Losses: 1250, Epsilon: 0.3625, Steps: 36754, Time: 138.16s\n",
      "Ações: Manter=11443, Comprar=12126, Vender=13185\n",
      "Ganhos Totais: 38674.00, Perdas Totais: -37901.00\n",
      "Modelo e log do episódio 32 salvos em: 4.7.1\\model_episode_32.pth e 4.7.1\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 660.25, Win Rate: 0.56, Wins: 1468, Losses: 1164, Epsilon: 0.3589, Steps: 36754, Time: 137.80s\n",
      "Ações: Manter=12236, Comprar=11432, Vender=13086\n",
      "Ganhos Totais: 38487.75, Perdas Totais: -37827.50\n",
      "Modelo e log do episódio 33 salvos em: 4.7.1\\model_episode_33.pth e 4.7.1\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: 1359.25, Win Rate: 0.54, Wins: 1456, Losses: 1226, Epsilon: 0.3553, Steps: 36754, Time: 137.00s\n",
      "Ações: Manter=12021, Comprar=11822, Vender=12911\n",
      "Ganhos Totais: 38336.50, Perdas Totais: -36977.25\n",
      "Modelo e log do episódio 34 salvos em: 4.7.1\\model_episode_34.pth e 4.7.1\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: 187.75, Win Rate: 0.55, Wins: 1478, Losses: 1221, Epsilon: 0.3517, Steps: 36754, Time: 118.28s\n",
      "Ações: Manter=12837, Comprar=11354, Vender=12563\n",
      "Ganhos Totais: 37604.50, Perdas Totais: -37416.75\n",
      "Episode 36/100, Total Reward: -2559.75, Win Rate: 0.53, Wins: 1413, Losses: 1256, Epsilon: 0.3482, Steps: 36754, Time: 115.96s\n",
      "Ações: Manter=13293, Comprar=11378, Vender=12083\n",
      "Ganhos Totais: 35249.00, Perdas Totais: -37808.75\n",
      "Episode 37/100, Total Reward: -4232.75, Win Rate: 0.54, Wins: 1384, Losses: 1172, Epsilon: 0.3447, Steps: 36754, Time: 115.52s\n",
      "Ações: Manter=13174, Comprar=11410, Vender=12170\n",
      "Ganhos Totais: 34567.50, Perdas Totais: -38800.25\n",
      "Episode 38/100, Total Reward: -93.25, Win Rate: 0.55, Wins: 1490, Losses: 1223, Epsilon: 0.3413, Steps: 36754, Time: 115.65s\n",
      "Ações: Manter=13327, Comprar=10675, Vender=12752\n",
      "Ganhos Totais: 36373.25, Perdas Totais: -36466.50\n",
      "Episode 39/100, Total Reward: -99.25, Win Rate: 0.54, Wins: 1465, Losses: 1238, Epsilon: 0.3379, Steps: 36754, Time: 116.08s\n",
      "Ações: Manter=13249, Comprar=11547, Vender=11958\n",
      "Ganhos Totais: 37142.25, Perdas Totais: -37241.50\n",
      "Episode 40/100, Total Reward: -3083.50, Win Rate: 0.55, Wins: 1435, Losses: 1197, Epsilon: 0.3345, Steps: 36754, Time: 115.70s\n",
      "Ações: Manter=13296, Comprar=11032, Vender=12426\n",
      "Ganhos Totais: 34141.25, Perdas Totais: -37224.75\n",
      "Episode 41/100, Total Reward: -2995.50, Win Rate: 0.55, Wins: 1460, Losses: 1194, Epsilon: 0.3311, Steps: 36754, Time: 115.70s\n",
      "Ações: Manter=12856, Comprar=11237, Vender=12661\n",
      "Ganhos Totais: 34987.25, Perdas Totais: -37982.75\n",
      "Episode 42/100, Total Reward: -1615.00, Win Rate: 0.53, Wins: 1448, Losses: 1287, Epsilon: 0.3278, Steps: 36754, Time: 116.40s\n",
      "Ações: Manter=11674, Comprar=14999, Vender=10081\n",
      "Ganhos Totais: 35699.75, Perdas Totais: -37314.75\n",
      "Episode 43/100, Total Reward: 1238.25, Win Rate: 0.53, Wins: 1455, Losses: 1268, Epsilon: 0.3246, Steps: 36754, Time: 115.40s\n",
      "Ações: Manter=10419, Comprar=15636, Vender=10699\n",
      "Ganhos Totais: 38322.25, Perdas Totais: -37084.00\n",
      "Modelo e log do episódio 43 salvos em: 4.7.1\\model_episode_43.pth e 4.7.1\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -3093.50, Win Rate: 0.53, Wins: 1266, Losses: 1143, Epsilon: 0.3213, Steps: 36754, Time: 115.31s\n",
      "Ações: Manter=11239, Comprar=14720, Vender=10795\n",
      "Ganhos Totais: 34063.75, Perdas Totais: -37157.25\n",
      "Episode 45/100, Total Reward: 1824.25, Win Rate: 0.55, Wins: 1331, Losses: 1107, Epsilon: 0.3181, Steps: 36754, Time: 116.08s\n",
      "Ações: Manter=10185, Comprar=16890, Vender=9679\n",
      "Ganhos Totais: 36877.00, Perdas Totais: -35052.75\n",
      "Modelo e log do episódio 45 salvos em: 4.7.1\\model_episode_45.pth e 4.7.1\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: 333.75, Win Rate: 0.52, Wins: 1192, Losses: 1089, Epsilon: 0.3149, Steps: 36754, Time: 115.76s\n",
      "Ações: Manter=10894, Comprar=15535, Vender=10325\n",
      "Ganhos Totais: 35463.75, Perdas Totais: -35130.00\n",
      "Episode 47/100, Total Reward: -420.25, Win Rate: 0.55, Wins: 1248, Losses: 1018, Epsilon: 0.3118, Steps: 36754, Time: 115.25s\n",
      "Ações: Manter=12086, Comprar=13437, Vender=11231\n",
      "Ganhos Totais: 34714.00, Perdas Totais: -35134.25\n",
      "Episode 48/100, Total Reward: 1324.75, Win Rate: 0.54, Wins: 1179, Losses: 1018, Epsilon: 0.3086, Steps: 36754, Time: 115.50s\n",
      "Ações: Manter=12490, Comprar=13391, Vender=10873\n",
      "Ganhos Totais: 33648.00, Perdas Totais: -32323.25\n",
      "Modelo e log do episódio 48 salvos em: 4.7.1\\model_episode_48.pth e 4.7.1\\log_episode_48.csv\n",
      "\n",
      "Episode 49/100, Total Reward: 2254.00, Win Rate: 0.55, Wins: 1226, Losses: 989, Epsilon: 0.3056, Steps: 36754, Time: 116.11s\n",
      "Ações: Manter=11885, Comprar=14730, Vender=10139\n",
      "Ganhos Totais: 36556.00, Perdas Totais: -34302.00\n",
      "Modelo e log do episódio 49 salvos em: 4.7.1\\model_episode_49.pth e 4.7.1\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: 971.50, Win Rate: 0.55, Wins: 1249, Losses: 1005, Epsilon: 0.3025, Steps: 36754, Time: 115.09s\n",
      "Ações: Manter=10632, Comprar=16476, Vender=9646\n",
      "Ganhos Totais: 36758.25, Perdas Totais: -35786.75\n",
      "Modelo e log do episódio 50 salvos em: 4.7.1\\model_episode_50.pth e 4.7.1\\log_episode_50.csv\n",
      "\n",
      "Episode 51/100, Total Reward: 191.00, Win Rate: 0.53, Wins: 1051, Losses: 948, Epsilon: 0.2995, Steps: 36754, Time: 115.74s\n",
      "Ações: Manter=12098, Comprar=12472, Vender=12184\n",
      "Ganhos Totais: 32727.75, Perdas Totais: -32536.75\n",
      "Episode 52/100, Total Reward: 707.75, Win Rate: 0.54, Wins: 1052, Losses: 897, Epsilon: 0.2965, Steps: 36754, Time: 115.94s\n",
      "Ações: Manter=10706, Comprar=17302, Vender=8746\n",
      "Ganhos Totais: 33792.00, Perdas Totais: -33084.25\n",
      "Episode 53/100, Total Reward: -632.25, Win Rate: 0.53, Wins: 1006, Losses: 889, Epsilon: 0.2935, Steps: 36754, Time: 115.70s\n",
      "Ações: Manter=10631, Comprar=16763, Vender=9360\n",
      "Ganhos Totais: 32554.25, Perdas Totais: -33186.50\n",
      "Episode 54/100, Total Reward: 3627.00, Win Rate: 0.56, Wins: 1150, Losses: 906, Epsilon: 0.2906, Steps: 36754, Time: 115.75s\n",
      "Ações: Manter=10787, Comprar=17472, Vender=8495\n",
      "Ganhos Totais: 37018.25, Perdas Totais: -33391.25\n",
      "Modelo e log do episódio 54 salvos em: 4.7.1\\model_episode_54.pth e 4.7.1\\log_episode_54.csv\n",
      "\n",
      "Episode 55/100, Total Reward: 391.75, Win Rate: 0.53, Wins: 1080, Losses: 947, Epsilon: 0.2877, Steps: 36754, Time: 115.90s\n",
      "Ações: Manter=10035, Comprar=18053, Vender=8666\n",
      "Ganhos Totais: 35208.75, Perdas Totais: -34817.00\n",
      "Episode 56/100, Total Reward: 5066.75, Win Rate: 0.54, Wins: 1093, Losses: 922, Epsilon: 0.2848, Steps: 36754, Time: 115.52s\n",
      "Ações: Manter=9480, Comprar=17750, Vender=9524\n",
      "Ganhos Totais: 38003.25, Perdas Totais: -32936.50\n",
      "Modelo e log do episódio 56 salvos em: 4.7.1\\model_episode_56.pth e 4.7.1\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: 373.00, Win Rate: 0.54, Wins: 1056, Losses: 888, Epsilon: 0.2820, Steps: 36754, Time: 115.44s\n",
      "Ações: Manter=10113, Comprar=17181, Vender=9460\n",
      "Ganhos Totais: 34525.25, Perdas Totais: -34152.25\n",
      "Episode 58/100, Total Reward: 2083.75, Win Rate: 0.55, Wins: 1065, Losses: 882, Epsilon: 0.2791, Steps: 36754, Time: 116.05s\n",
      "Ações: Manter=10669, Comprar=17199, Vender=8886\n",
      "Ganhos Totais: 35802.25, Perdas Totais: -33718.50\n",
      "Modelo e log do episódio 58 salvos em: 4.7.1\\model_episode_58.pth e 4.7.1\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: -2693.00, Win Rate: 0.55, Wins: 1076, Losses: 892, Epsilon: 0.2763, Steps: 36754, Time: 116.13s\n",
      "Ações: Manter=9387, Comprar=17824, Vender=9543\n",
      "Ganhos Totais: 34023.00, Perdas Totais: -36716.00\n",
      "Episode 60/100, Total Reward: -1361.00, Win Rate: 0.54, Wins: 987, Losses: 845, Epsilon: 0.2736, Steps: 36754, Time: 115.65s\n",
      "Ações: Manter=9810, Comprar=17844, Vender=9100\n",
      "Ganhos Totais: 33247.25, Perdas Totais: -34608.25\n",
      "Episode 61/100, Total Reward: -140.50, Win Rate: 0.54, Wins: 1007, Losses: 858, Epsilon: 0.2708, Steps: 36754, Time: 116.25s\n",
      "Ações: Manter=10686, Comprar=16795, Vender=9273\n",
      "Ganhos Totais: 34807.75, Perdas Totais: -34948.25\n",
      "Episode 62/100, Total Reward: -210.25, Win Rate: 0.53, Wins: 1109, Losses: 980, Epsilon: 0.2681, Steps: 36754, Time: 116.02s\n",
      "Ações: Manter=11013, Comprar=15851, Vender=9890\n",
      "Ganhos Totais: 34978.50, Perdas Totais: -35188.75\n",
      "Episode 63/100, Total Reward: 4366.75, Win Rate: 0.56, Wins: 1269, Losses: 982, Epsilon: 0.2655, Steps: 36754, Time: 115.86s\n",
      "Ações: Manter=11163, Comprar=15881, Vender=9710\n",
      "Ganhos Totais: 37654.25, Perdas Totais: -33287.50\n",
      "Modelo e log do episódio 63 salvos em: 4.7.1\\model_episode_63.pth e 4.7.1\\log_episode_63.csv\n",
      "\n",
      "Episode 64/100, Total Reward: -21.50, Win Rate: 0.54, Wins: 1253, Losses: 1058, Epsilon: 0.2628, Steps: 36754, Time: 116.32s\n",
      "Ações: Manter=10473, Comprar=16448, Vender=9833\n",
      "Ganhos Totais: 35551.00, Perdas Totais: -35572.50\n",
      "Episode 65/100, Total Reward: 370.75, Win Rate: 0.54, Wins: 1268, Losses: 1076, Epsilon: 0.2602, Steps: 36754, Time: 116.42s\n",
      "Ações: Manter=10246, Comprar=16345, Vender=10163\n",
      "Ganhos Totais: 34901.00, Perdas Totais: -34530.25\n",
      "Episode 66/100, Total Reward: 1307.75, Win Rate: 0.55, Wins: 1240, Losses: 1014, Epsilon: 0.2576, Steps: 36754, Time: 116.60s\n",
      "Ações: Manter=10229, Comprar=16377, Vender=10148\n",
      "Ganhos Totais: 35069.75, Perdas Totais: -33762.00\n",
      "Episode 67/100, Total Reward: 568.50, Win Rate: 0.55, Wins: 1312, Losses: 1057, Epsilon: 0.2550, Steps: 36754, Time: 116.48s\n",
      "Ações: Manter=9803, Comprar=16809, Vender=10142\n",
      "Ganhos Totais: 35849.25, Perdas Totais: -35280.75\n",
      "Episode 68/100, Total Reward: 1821.50, Win Rate: 0.56, Wins: 1316, Losses: 1028, Epsilon: 0.2524, Steps: 36754, Time: 116.56s\n",
      "Ações: Manter=9819, Comprar=16414, Vender=10521\n",
      "Ganhos Totais: 38162.25, Perdas Totais: -36340.75\n",
      "Modelo e log do episódio 68 salvos em: 4.7.1\\model_episode_68.pth e 4.7.1\\log_episode_68.csv\n",
      "\n",
      "Episode 69/100, Total Reward: 4270.25, Win Rate: 0.56, Wins: 1321, Losses: 1043, Epsilon: 0.2499, Steps: 36754, Time: 116.71s\n",
      "Ações: Manter=10172, Comprar=15462, Vender=11120\n",
      "Ganhos Totais: 36970.00, Perdas Totais: -32699.75\n",
      "Modelo e log do episódio 69 salvos em: 4.7.1\\model_episode_69.pth e 4.7.1\\log_episode_69.csv\n",
      "\n",
      "Episode 70/100, Total Reward: 4327.25, Win Rate: 0.56, Wins: 1367, Losses: 1065, Epsilon: 0.2474, Steps: 36754, Time: 116.90s\n",
      "Ações: Manter=9380, Comprar=17299, Vender=10075\n",
      "Ganhos Totais: 38912.00, Perdas Totais: -34584.75\n",
      "Modelo e log do episódio 70 salvos em: 4.7.1\\model_episode_70.pth e 4.7.1\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: 2306.00, Win Rate: 0.56, Wins: 1185, Losses: 948, Epsilon: 0.2449, Steps: 36754, Time: 116.67s\n",
      "Ações: Manter=10964, Comprar=15124, Vender=10666\n",
      "Ganhos Totais: 33994.50, Perdas Totais: -31688.50\n",
      "Modelo e log do episódio 71 salvos em: 4.7.1\\model_episode_71.pth e 4.7.1\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: 5424.25, Win Rate: 0.55, Wins: 1242, Losses: 1013, Epsilon: 0.2425, Steps: 36754, Time: 117.12s\n",
      "Ações: Manter=9788, Comprar=14208, Vender=12758\n",
      "Ganhos Totais: 39265.75, Perdas Totais: -33841.50\n",
      "Modelo e log do episódio 72 salvos em: 4.7.1\\model_episode_72.pth e 4.7.1\\log_episode_72.csv\n",
      "\n",
      "Episode 73/100, Total Reward: 3367.00, Win Rate: 0.55, Wins: 1178, Losses: 960, Epsilon: 0.2401, Steps: 36754, Time: 116.96s\n",
      "Ações: Manter=10835, Comprar=12942, Vender=12977\n",
      "Ganhos Totais: 36014.75, Perdas Totais: -32647.75\n",
      "Modelo e log do episódio 73 salvos em: 4.7.1\\model_episode_73.pth e 4.7.1\\log_episode_73.csv\n",
      "\n",
      "Episode 74/100, Total Reward: 1298.75, Win Rate: 0.54, Wins: 1118, Losses: 956, Epsilon: 0.2377, Steps: 36754, Time: 117.23s\n",
      "Ações: Manter=11700, Comprar=12649, Vender=12405\n",
      "Ganhos Totais: 34206.75, Perdas Totais: -32908.00\n",
      "Episode 75/100, Total Reward: -3767.75, Win Rate: 0.52, Wins: 967, Losses: 899, Epsilon: 0.2353, Steps: 36754, Time: 117.02s\n",
      "Ações: Manter=12716, Comprar=12088, Vender=11950\n",
      "Ganhos Totais: 30406.75, Perdas Totais: -34174.50\n",
      "Episode 76/100, Total Reward: -98.50, Win Rate: 0.54, Wins: 1079, Losses: 909, Epsilon: 0.2329, Steps: 36754, Time: 116.70s\n",
      "Ações: Manter=12346, Comprar=12020, Vender=12388\n",
      "Ganhos Totais: 32827.50, Perdas Totais: -32926.00\n",
      "Episode 77/100, Total Reward: 827.00, Win Rate: 0.54, Wins: 1083, Losses: 921, Epsilon: 0.2306, Steps: 36754, Time: 117.13s\n",
      "Ações: Manter=12349, Comprar=12221, Vender=12184\n",
      "Ganhos Totais: 34354.75, Perdas Totais: -33527.75\n",
      "Episode 78/100, Total Reward: 1056.50, Win Rate: 0.52, Wins: 1053, Losses: 963, Epsilon: 0.2283, Steps: 36754, Time: 117.14s\n",
      "Ações: Manter=12435, Comprar=11155, Vender=13164\n",
      "Ganhos Totais: 33891.75, Perdas Totais: -32835.25\n",
      "Episode 79/100, Total Reward: 539.25, Win Rate: 0.54, Wins: 1030, Losses: 893, Epsilon: 0.2260, Steps: 36754, Time: 119.89s\n",
      "Ações: Manter=12475, Comprar=12188, Vender=12091\n",
      "Ganhos Totais: 32601.25, Perdas Totais: -32062.00\n",
      "Episode 80/100, Total Reward: 452.00, Win Rate: 0.55, Wins: 1026, Losses: 854, Epsilon: 0.2238, Steps: 36754, Time: 119.69s\n",
      "Ações: Manter=12366, Comprar=12072, Vender=12316\n",
      "Ganhos Totais: 33479.75, Perdas Totais: -33027.75\n",
      "Episode 81/100, Total Reward: 2974.75, Win Rate: 0.54, Wins: 1076, Losses: 905, Epsilon: 0.2215, Steps: 36754, Time: 120.04s\n",
      "Ações: Manter=11345, Comprar=12581, Vender=12828\n",
      "Ganhos Totais: 35731.00, Perdas Totais: -32756.25\n",
      "Modelo e log do episódio 81 salvos em: 4.7.1\\model_episode_81.pth e 4.7.1\\log_episode_81.csv\n",
      "\n",
      "Episode 82/100, Total Reward: -559.50, Win Rate: 0.53, Wins: 997, Losses: 870, Epsilon: 0.2193, Steps: 36754, Time: 135.37s\n",
      "Ações: Manter=11322, Comprar=11148, Vender=14284\n",
      "Ganhos Totais: 34379.00, Perdas Totais: -34938.50\n",
      "Episode 83/100, Total Reward: -771.25, Win Rate: 0.53, Wins: 991, Losses: 887, Epsilon: 0.2171, Steps: 36754, Time: 152.84s\n",
      "Ações: Manter=12784, Comprar=10097, Vender=13873\n",
      "Ganhos Totais: 32360.00, Perdas Totais: -33131.25\n",
      "Episode 84/100, Total Reward: -4146.75, Win Rate: 0.52, Wins: 999, Losses: 919, Epsilon: 0.2149, Steps: 36754, Time: 139.82s\n",
      "Ações: Manter=11921, Comprar=11804, Vender=13029\n",
      "Ganhos Totais: 29751.75, Perdas Totais: -33898.50\n",
      "Episode 85/100, Total Reward: -622.00, Win Rate: 0.54, Wins: 982, Losses: 837, Epsilon: 0.2128, Steps: 36754, Time: 134.69s\n",
      "Ações: Manter=11161, Comprar=12480, Vender=13113\n",
      "Ganhos Totais: 32233.25, Perdas Totais: -32855.25\n",
      "Episode 86/100, Total Reward: -2797.75, Win Rate: 0.52, Wins: 1027, Losses: 944, Epsilon: 0.2107, Steps: 36754, Time: 138.74s\n",
      "Ações: Manter=11426, Comprar=11655, Vender=13673\n",
      "Ganhos Totais: 32437.25, Perdas Totais: -35235.00\n",
      "Episode 87/100, Total Reward: 963.50, Win Rate: 0.54, Wins: 1066, Losses: 898, Epsilon: 0.2086, Steps: 36754, Time: 119.62s\n",
      "Ações: Manter=12079, Comprar=11757, Vender=12918\n",
      "Ganhos Totais: 34217.75, Perdas Totais: -33254.25\n",
      "Episode 88/100, Total Reward: 322.75, Win Rate: 0.52, Wins: 1010, Losses: 939, Epsilon: 0.2065, Steps: 36754, Time: 117.80s\n",
      "Ações: Manter=11353, Comprar=11971, Vender=13430\n",
      "Ganhos Totais: 34302.25, Perdas Totais: -33979.50\n",
      "Episode 89/100, Total Reward: 2471.50, Win Rate: 0.54, Wins: 992, Losses: 855, Epsilon: 0.2044, Steps: 36754, Time: 123.77s\n",
      "Ações: Manter=11548, Comprar=11585, Vender=13621\n",
      "Ganhos Totais: 34716.75, Perdas Totais: -32245.25\n",
      "Modelo e log do episódio 89 salvos em: 4.7.1\\model_episode_89.pth e 4.7.1\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: -897.75, Win Rate: 0.53, Wins: 933, Losses: 832, Epsilon: 0.2024, Steps: 36754, Time: 136.91s\n",
      "Ações: Manter=11951, Comprar=11871, Vender=12932\n",
      "Ganhos Totais: 32149.00, Perdas Totais: -33046.75\n",
      "Episode 91/100, Total Reward: -326.25, Win Rate: 0.53, Wins: 1040, Losses: 916, Epsilon: 0.2003, Steps: 36754, Time: 121.13s\n",
      "Ações: Manter=11950, Comprar=10888, Vender=13916\n",
      "Ganhos Totais: 34411.50, Perdas Totais: -34737.75\n",
      "Episode 92/100, Total Reward: 3162.75, Win Rate: 0.54, Wins: 941, Losses: 811, Epsilon: 0.1983, Steps: 36754, Time: 120.57s\n",
      "Ações: Manter=9255, Comprar=17160, Vender=10339\n",
      "Ganhos Totais: 34307.00, Perdas Totais: -31144.25\n",
      "Modelo e log do episódio 92 salvos em: 4.7.1\\model_episode_92.pth e 4.7.1\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: 5387.50, Win Rate: 0.54, Wins: 926, Losses: 791, Epsilon: 0.1964, Steps: 36754, Time: 133.22s\n",
      "Ações: Manter=10342, Comprar=15628, Vender=10784\n",
      "Ganhos Totais: 35256.00, Perdas Totais: -29868.50\n",
      "Modelo e log do episódio 93 salvos em: 4.7.1\\model_episode_93.pth e 4.7.1\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: -1537.25, Win Rate: 0.53, Wins: 864, Losses: 780, Epsilon: 0.1944, Steps: 36754, Time: 136.46s\n",
      "Ações: Manter=9338, Comprar=15572, Vender=11844\n",
      "Ganhos Totais: 32373.50, Perdas Totais: -33910.75\n",
      "Episode 95/100, Total Reward: 473.25, Win Rate: 0.53, Wins: 797, Losses: 700, Epsilon: 0.1924, Steps: 36754, Time: 134.26s\n",
      "Ações: Manter=10762, Comprar=15290, Vender=10702\n",
      "Ganhos Totais: 29327.50, Perdas Totais: -28854.25\n",
      "Episode 96/100, Total Reward: 2027.00, Win Rate: 0.53, Wins: 831, Losses: 737, Epsilon: 0.1905, Steps: 36754, Time: 132.88s\n",
      "Ações: Manter=10642, Comprar=15535, Vender=10577\n",
      "Ganhos Totais: 32551.75, Perdas Totais: -30524.75\n",
      "Episode 97/100, Total Reward: -624.75, Win Rate: 0.53, Wins: 859, Losses: 757, Epsilon: 0.1886, Steps: 36754, Time: 133.25s\n",
      "Ações: Manter=8305, Comprar=16060, Vender=12389\n",
      "Ganhos Totais: 31735.75, Perdas Totais: -32360.50\n",
      "Episode 98/100, Total Reward: -260.75, Win Rate: 0.54, Wins: 839, Losses: 716, Epsilon: 0.1867, Steps: 36754, Time: 117.82s\n",
      "Ações: Manter=9712, Comprar=16031, Vender=11011\n",
      "Ganhos Totais: 31212.25, Perdas Totais: -31473.00\n",
      "Episode 99/100, Total Reward: -504.00, Win Rate: 0.54, Wins: 881, Losses: 747, Epsilon: 0.1849, Steps: 36754, Time: 119.64s\n",
      "Ações: Manter=9373, Comprar=16827, Vender=10554\n",
      "Ganhos Totais: 31651.75, Perdas Totais: -32155.75\n",
      "Episode 100/100, Total Reward: 4502.25, Win Rate: 0.56, Wins: 933, Losses: 747, Epsilon: 0.1830, Steps: 36754, Time: 122.75s\n",
      "Ações: Manter=9371, Comprar=16970, Vender=10413\n",
      "Ganhos Totais: 35011.00, Perdas Totais: -30508.75\n",
      "Modelo e log do episódio 100 salvos em: 4.7.1\\model_episode_100.pth e 4.7.1\\log_episode_100.csv\n",
      "\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 72, Total Reward: 5424.25, Win Rate: 0.55, Wins: 1242, Losses: 1013, Ações: {0: 9788, 1: 14208, 2: 12758}, Steps: 36754, Time: 117.12s\n",
      "Rank 2: Episode 93, Total Reward: 5387.50, Win Rate: 0.54, Wins: 926, Losses: 791, Ações: {0: 10342, 1: 15628, 2: 10784}, Steps: 36754, Time: 133.22s\n",
      "Rank 3: Episode 56, Total Reward: 5066.75, Win Rate: 0.54, Wins: 1093, Losses: 922, Ações: {0: 9480, 1: 17750, 2: 9524}, Steps: 36754, Time: 115.52s\n",
      "Rank 4: Episode 100, Total Reward: 4502.25, Win Rate: 0.56, Wins: 933, Losses: 747, Ações: {0: 9371, 1: 16970, 2: 10413}, Steps: 36754, Time: 122.75s\n",
      "Rank 5: Episode 63, Total Reward: 4366.75, Win Rate: 0.56, Wins: 1269, Losses: 982, Ações: {0: 11163, 1: 15881, 2: 9710}, Steps: 36754, Time: 115.86s\n",
      "Rank 6: Episode 70, Total Reward: 4327.25, Win Rate: 0.56, Wins: 1367, Losses: 1065, Ações: {0: 9380, 1: 17299, 2: 10075}, Steps: 36754, Time: 116.90s\n",
      "Rank 7: Episode 69, Total Reward: 4270.25, Win Rate: 0.56, Wins: 1321, Losses: 1043, Ações: {0: 10172, 1: 15462, 2: 11120}, Steps: 36754, Time: 116.71s\n",
      "Rank 8: Episode 12, Total Reward: 3746.50, Win Rate: 0.54, Wins: 1377, Losses: 1191, Ações: {0: 11571, 1: 12277, 2: 12906}, Steps: 36754, Time: 156.32s\n",
      "Rank 9: Episode 54, Total Reward: 3627.00, Win Rate: 0.56, Wins: 1150, Losses: 906, Ações: {0: 10787, 1: 17472, 2: 8495}, Steps: 36754, Time: 115.75s\n",
      "Rank 10: Episode 73, Total Reward: 3367.00, Win Rate: 0.55, Wins: 1178, Losses: 960, Ações: {0: 10835, 1: 12942, 2: 12977}, Steps: 36754, Time: 116.96s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA8','SMA20', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
