{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -387.75, Win Rate: 0.51, Wins: 1435, Losses: 1392, Epsilon: 0.4950, Steps: 36754, Time: 151.38s\n",
      "Ações: Manter=11892, Comprar=12146, Vender=12716\n",
      "Ganhos Totais: 37574.75, Perdas Totais: -37962.50\n",
      "Modelo e log do episódio 1 salvos em: 4.8.3\\model_episode_1.pth e 4.8.3\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: 16.00, Win Rate: 0.52, Wins: 1466, Losses: 1345, Epsilon: 0.4900, Steps: 36754, Time: 147.74s\n",
      "Ações: Manter=11444, Comprar=13137, Vender=12173\n",
      "Ganhos Totais: 38268.50, Perdas Totais: -38252.50\n",
      "Modelo e log do episódio 2 salvos em: 4.8.3\\model_episode_2.pth e 4.8.3\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -2521.00, Win Rate: 0.51, Wins: 1482, Losses: 1437, Epsilon: 0.4851, Steps: 36754, Time: 149.86s\n",
      "Ações: Manter=11895, Comprar=12492, Vender=12367\n",
      "Ganhos Totais: 36490.25, Perdas Totais: -39011.25\n",
      "Modelo e log do episódio 3 salvos em: 4.8.3\\model_episode_3.pth e 4.8.3\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -3404.00, Win Rate: 0.52, Wins: 1485, Losses: 1371, Epsilon: 0.4803, Steps: 36754, Time: 154.78s\n",
      "Ações: Manter=11753, Comprar=13167, Vender=11834\n",
      "Ganhos Totais: 35622.75, Perdas Totais: -39026.75\n",
      "Modelo e log do episódio 4 salvos em: 4.8.3\\model_episode_4.pth e 4.8.3\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -2872.00, Win Rate: 0.50, Wins: 1484, Losses: 1478, Epsilon: 0.4755, Steps: 36754, Time: 154.25s\n",
      "Ações: Manter=11649, Comprar=13030, Vender=12075\n",
      "Ganhos Totais: 36413.25, Perdas Totais: -39285.25\n",
      "Modelo e log do episódio 5 salvos em: 4.8.3\\model_episode_5.pth e 4.8.3\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 2472.00, Win Rate: 0.54, Wins: 1585, Losses: 1367, Epsilon: 0.4707, Steps: 36754, Time: 156.83s\n",
      "Ações: Manter=11389, Comprar=13114, Vender=12251\n",
      "Ganhos Totais: 38843.25, Perdas Totais: -36371.25\n",
      "Modelo e log do episódio 6 salvos em: 4.8.3\\model_episode_6.pth e 4.8.3\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -718.50, Win Rate: 0.55, Wins: 1605, Losses: 1329, Epsilon: 0.4660, Steps: 36754, Time: 148.98s\n",
      "Ações: Manter=11962, Comprar=12486, Vender=12306\n",
      "Ganhos Totais: 39526.25, Perdas Totais: -40244.75\n",
      "Modelo e log do episódio 7 salvos em: 4.8.3\\model_episode_7.pth e 4.8.3\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -1668.00, Win Rate: 0.52, Wins: 1469, Losses: 1369, Epsilon: 0.4614, Steps: 36754, Time: 156.65s\n",
      "Ações: Manter=11871, Comprar=12534, Vender=12349\n",
      "Ganhos Totais: 37776.50, Perdas Totais: -39444.50\n",
      "Modelo e log do episódio 8 salvos em: 4.8.3\\model_episode_8.pth e 4.8.3\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -474.25, Win Rate: 0.52, Wins: 1485, Losses: 1386, Epsilon: 0.4568, Steps: 36754, Time: 159.53s\n",
      "Ações: Manter=12145, Comprar=12210, Vender=12399\n",
      "Ganhos Totais: 38404.00, Perdas Totais: -38878.25\n",
      "Modelo e log do episódio 9 salvos em: 4.8.3\\model_episode_9.pth e 4.8.3\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -684.50, Win Rate: 0.53, Wins: 1451, Losses: 1308, Epsilon: 0.4522, Steps: 36754, Time: 147.51s\n",
      "Ações: Manter=12195, Comprar=12179, Vender=12380\n",
      "Ganhos Totais: 37764.75, Perdas Totais: -38449.25\n",
      "Modelo e log do episódio 10 salvos em: 4.8.3\\model_episode_10.pth e 4.8.3\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 1950.00, Win Rate: 0.53, Wins: 1503, Losses: 1312, Epsilon: 0.4477, Steps: 36754, Time: 155.14s\n",
      "Ações: Manter=11961, Comprar=12589, Vender=12204\n",
      "Ganhos Totais: 37789.75, Perdas Totais: -35839.75\n",
      "Modelo e log do episódio 11 salvos em: 4.8.3\\model_episode_11.pth e 4.8.3\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 1669.75, Win Rate: 0.53, Wins: 1481, Losses: 1291, Epsilon: 0.4432, Steps: 36754, Time: 157.62s\n",
      "Ações: Manter=11823, Comprar=12665, Vender=12266\n",
      "Ganhos Totais: 38078.25, Perdas Totais: -36408.50\n",
      "Modelo e log do episódio 12 salvos em: 4.8.3\\model_episode_12.pth e 4.8.3\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -2259.25, Win Rate: 0.52, Wins: 1453, Losses: 1358, Epsilon: 0.4388, Steps: 36754, Time: 148.36s\n",
      "Ações: Manter=12278, Comprar=11956, Vender=12520\n",
      "Ganhos Totais: 36788.25, Perdas Totais: -39047.50\n",
      "Modelo e log do episódio 13 salvos em: 4.8.3\\model_episode_13.pth e 4.8.3\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -1185.50, Win Rate: 0.52, Wins: 1439, Losses: 1321, Epsilon: 0.4344, Steps: 36754, Time: 147.43s\n",
      "Ações: Manter=11751, Comprar=12114, Vender=12889\n",
      "Ganhos Totais: 37817.75, Perdas Totais: -39003.25\n",
      "Modelo e log do episódio 14 salvos em: 4.8.3\\model_episode_14.pth e 4.8.3\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -246.50, Win Rate: 0.52, Wins: 1432, Losses: 1313, Epsilon: 0.4300, Steps: 36754, Time: 147.11s\n",
      "Ações: Manter=12547, Comprar=11529, Vender=12678\n",
      "Ganhos Totais: 36855.25, Perdas Totais: -37101.75\n",
      "Modelo e log do episódio 15 salvos em: 4.8.3\\model_episode_15.pth e 4.8.3\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -1161.50, Win Rate: 0.51, Wins: 1372, Losses: 1313, Epsilon: 0.4257, Steps: 36754, Time: 147.86s\n",
      "Ações: Manter=13183, Comprar=11988, Vender=11583\n",
      "Ganhos Totais: 36763.00, Perdas Totais: -37924.50\n",
      "Modelo e log do episódio 16 salvos em: 4.8.3\\model_episode_16.pth e 4.8.3\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -2079.75, Win Rate: 0.53, Wins: 1363, Losses: 1228, Epsilon: 0.4215, Steps: 36754, Time: 147.53s\n",
      "Ações: Manter=13437, Comprar=11404, Vender=11913\n",
      "Ganhos Totais: 35718.25, Perdas Totais: -37798.00\n",
      "Episode 18/100, Total Reward: -575.75, Win Rate: 0.53, Wins: 1454, Losses: 1315, Epsilon: 0.4173, Steps: 36754, Time: 147.60s\n",
      "Ações: Manter=12603, Comprar=11434, Vender=12717\n",
      "Ganhos Totais: 36536.50, Perdas Totais: -37112.25\n",
      "Modelo e log do episódio 18 salvos em: 4.8.3\\model_episode_18.pth e 4.8.3\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -1426.25, Win Rate: 0.53, Wins: 1469, Losses: 1299, Epsilon: 0.4131, Steps: 36754, Time: 148.38s\n",
      "Ações: Manter=13166, Comprar=11614, Vender=11974\n",
      "Ganhos Totais: 36796.75, Perdas Totais: -38223.00\n",
      "Episode 20/100, Total Reward: 1946.00, Win Rate: 0.52, Wins: 1440, Losses: 1312, Epsilon: 0.4090, Steps: 36754, Time: 147.58s\n",
      "Ações: Manter=12362, Comprar=11730, Vender=12662\n",
      "Ganhos Totais: 39700.75, Perdas Totais: -37754.75\n",
      "Modelo e log do episódio 20 salvos em: 4.8.3\\model_episode_20.pth e 4.8.3\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -2464.00, Win Rate: 0.52, Wins: 1365, Losses: 1270, Epsilon: 0.4049, Steps: 36754, Time: 148.32s\n",
      "Ações: Manter=13694, Comprar=10647, Vender=12413\n",
      "Ganhos Totais: 36607.25, Perdas Totais: -39071.25\n",
      "Episode 22/100, Total Reward: -1308.25, Win Rate: 0.53, Wins: 1344, Losses: 1172, Epsilon: 0.4008, Steps: 36754, Time: 147.92s\n",
      "Ações: Manter=12765, Comprar=10898, Vender=13091\n",
      "Ganhos Totais: 36241.25, Perdas Totais: -37549.50\n",
      "Episode 23/100, Total Reward: 1344.50, Win Rate: 0.54, Wins: 1438, Losses: 1221, Epsilon: 0.3968, Steps: 36754, Time: 148.15s\n",
      "Ações: Manter=12703, Comprar=10630, Vender=13421\n",
      "Ganhos Totais: 38650.75, Perdas Totais: -37306.25\n",
      "Modelo e log do episódio 23 salvos em: 4.8.3\\model_episode_23.pth e 4.8.3\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: -3181.75, Win Rate: 0.52, Wins: 1442, Losses: 1351, Epsilon: 0.3928, Steps: 36754, Time: 148.75s\n",
      "Ações: Manter=11645, Comprar=11155, Vender=13954\n",
      "Ganhos Totais: 35046.75, Perdas Totais: -38228.50\n",
      "Episode 25/100, Total Reward: -5612.25, Win Rate: 0.51, Wins: 1301, Losses: 1232, Epsilon: 0.3889, Steps: 36754, Time: 147.78s\n",
      "Ações: Manter=13881, Comprar=11103, Vender=11770\n",
      "Ganhos Totais: 33896.50, Perdas Totais: -39508.75\n",
      "Episode 26/100, Total Reward: -1463.25, Win Rate: 0.52, Wins: 1304, Losses: 1181, Epsilon: 0.3850, Steps: 36754, Time: 148.54s\n",
      "Ações: Manter=12339, Comprar=10649, Vender=13766\n",
      "Ganhos Totais: 35524.25, Perdas Totais: -36987.50\n",
      "Episode 27/100, Total Reward: -3158.00, Win Rate: 0.52, Wins: 1299, Losses: 1180, Epsilon: 0.3812, Steps: 36754, Time: 149.16s\n",
      "Ações: Manter=12807, Comprar=11067, Vender=12880\n",
      "Ganhos Totais: 34958.00, Perdas Totais: -38116.00\n",
      "Episode 28/100, Total Reward: -5417.50, Win Rate: 0.51, Wins: 1286, Losses: 1229, Epsilon: 0.3774, Steps: 36754, Time: 148.73s\n",
      "Ações: Manter=13269, Comprar=9887, Vender=13598\n",
      "Ganhos Totais: 32971.00, Perdas Totais: -38388.50\n",
      "Episode 29/100, Total Reward: -3106.25, Win Rate: 0.53, Wins: 1364, Losses: 1204, Epsilon: 0.3736, Steps: 36754, Time: 149.04s\n",
      "Ações: Manter=13805, Comprar=10492, Vender=12457\n",
      "Ganhos Totais: 35537.50, Perdas Totais: -38643.75\n",
      "Episode 30/100, Total Reward: -5286.25, Win Rate: 0.51, Wins: 1337, Losses: 1290, Epsilon: 0.3699, Steps: 36754, Time: 148.22s\n",
      "Ações: Manter=14211, Comprar=9379, Vender=13164\n",
      "Ganhos Totais: 34296.25, Perdas Totais: -39582.50\n",
      "Episode 31/100, Total Reward: -1919.00, Win Rate: 0.53, Wins: 1351, Losses: 1201, Epsilon: 0.3662, Steps: 36754, Time: 149.57s\n",
      "Ações: Manter=14393, Comprar=9409, Vender=12952\n",
      "Ganhos Totais: 36196.75, Perdas Totais: -38115.75\n",
      "Episode 32/100, Total Reward: -603.50, Win Rate: 0.53, Wins: 1413, Losses: 1230, Epsilon: 0.3625, Steps: 36754, Time: 154.14s\n",
      "Ações: Manter=14146, Comprar=11327, Vender=11281\n",
      "Ganhos Totais: 36413.50, Perdas Totais: -37017.00\n",
      "Episode 33/100, Total Reward: -3120.00, Win Rate: 0.53, Wins: 1297, Losses: 1166, Epsilon: 0.3589, Steps: 36754, Time: 147.61s\n",
      "Ações: Manter=13351, Comprar=13138, Vender=10265\n",
      "Ganhos Totais: 34382.75, Perdas Totais: -37502.75\n",
      "Episode 34/100, Total Reward: -1207.75, Win Rate: 0.53, Wins: 1243, Losses: 1091, Epsilon: 0.3553, Steps: 36754, Time: 136.72s\n",
      "Ações: Manter=14596, Comprar=11121, Vender=11037\n",
      "Ganhos Totais: 35373.00, Perdas Totais: -36580.75\n",
      "Episode 35/100, Total Reward: 1312.50, Win Rate: 0.52, Wins: 1367, Losses: 1244, Epsilon: 0.3517, Steps: 36754, Time: 130.24s\n",
      "Ações: Manter=12140, Comprar=11647, Vender=12967\n",
      "Ganhos Totais: 38419.50, Perdas Totais: -37107.00\n",
      "Modelo e log do episódio 35 salvos em: 4.8.3\\model_episode_35.pth e 4.8.3\\log_episode_35.csv\n",
      "\n",
      "Episode 36/100, Total Reward: -1368.75, Win Rate: 0.53, Wins: 1183, Losses: 1050, Epsilon: 0.3482, Steps: 36754, Time: 130.30s\n",
      "Ações: Manter=15182, Comprar=10416, Vender=11156\n",
      "Ganhos Totais: 35072.50, Perdas Totais: -36441.25\n",
      "Episode 37/100, Total Reward: 2732.00, Win Rate: 0.54, Wins: 1409, Losses: 1184, Epsilon: 0.3447, Steps: 36754, Time: 129.97s\n",
      "Ações: Manter=13000, Comprar=10879, Vender=12875\n",
      "Ganhos Totais: 37816.50, Perdas Totais: -35084.50\n",
      "Modelo e log do episódio 37 salvos em: 4.8.3\\model_episode_37.pth e 4.8.3\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: 1210.25, Win Rate: 0.52, Wins: 1197, Losses: 1099, Epsilon: 0.3413, Steps: 36754, Time: 129.73s\n",
      "Ações: Manter=14242, Comprar=10633, Vender=11879\n",
      "Ganhos Totais: 35177.75, Perdas Totais: -33967.50\n",
      "Modelo e log do episódio 38 salvos em: 4.8.3\\model_episode_38.pth e 4.8.3\\log_episode_38.csv\n",
      "\n",
      "Episode 39/100, Total Reward: -433.00, Win Rate: 0.53, Wins: 1284, Losses: 1131, Epsilon: 0.3379, Steps: 36754, Time: 129.81s\n",
      "Ações: Manter=14572, Comprar=10289, Vender=11893\n",
      "Ganhos Totais: 35632.50, Perdas Totais: -36065.50\n",
      "Episode 40/100, Total Reward: -3263.25, Win Rate: 0.51, Wins: 1171, Losses: 1105, Epsilon: 0.3345, Steps: 36754, Time: 130.15s\n",
      "Ações: Manter=15056, Comprar=10469, Vender=11229\n",
      "Ganhos Totais: 33460.25, Perdas Totais: -36723.50\n",
      "Episode 41/100, Total Reward: -3313.00, Win Rate: 0.50, Wins: 1171, Losses: 1153, Epsilon: 0.3311, Steps: 36754, Time: 129.88s\n",
      "Ações: Manter=12967, Comprar=11365, Vender=12422\n",
      "Ganhos Totais: 33864.50, Perdas Totais: -37177.50\n",
      "Episode 42/100, Total Reward: 859.75, Win Rate: 0.53, Wins: 1301, Losses: 1131, Epsilon: 0.3278, Steps: 36754, Time: 130.26s\n",
      "Ações: Manter=12778, Comprar=10864, Vender=13112\n",
      "Ganhos Totais: 36406.50, Perdas Totais: -35546.75\n",
      "Modelo e log do episódio 42 salvos em: 4.8.3\\model_episode_42.pth e 4.8.3\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: -1753.25, Win Rate: 0.53, Wins: 1352, Losses: 1210, Epsilon: 0.3246, Steps: 36754, Time: 130.31s\n",
      "Ações: Manter=13061, Comprar=10423, Vender=13270\n",
      "Ganhos Totais: 35999.75, Perdas Totais: -37753.00\n",
      "Episode 44/100, Total Reward: -2154.25, Win Rate: 0.53, Wins: 1297, Losses: 1164, Epsilon: 0.3213, Steps: 36754, Time: 129.93s\n",
      "Ações: Manter=12843, Comprar=9996, Vender=13915\n",
      "Ganhos Totais: 35093.25, Perdas Totais: -37247.50\n",
      "Episode 45/100, Total Reward: -1171.25, Win Rate: 0.52, Wins: 1174, Losses: 1090, Epsilon: 0.3181, Steps: 36754, Time: 129.87s\n",
      "Ações: Manter=16666, Comprar=8199, Vender=11889\n",
      "Ganhos Totais: 33921.50, Perdas Totais: -35092.75\n",
      "Episode 46/100, Total Reward: 318.75, Win Rate: 0.53, Wins: 1301, Losses: 1140, Epsilon: 0.3149, Steps: 36754, Time: 130.25s\n",
      "Ações: Manter=14299, Comprar=8701, Vender=13754\n",
      "Ganhos Totais: 36168.50, Perdas Totais: -35849.75\n",
      "Modelo e log do episódio 46 salvos em: 4.8.3\\model_episode_46.pth e 4.8.3\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: 654.25, Win Rate: 0.53, Wins: 1281, Losses: 1115, Epsilon: 0.3118, Steps: 36754, Time: 129.89s\n",
      "Ações: Manter=15025, Comprar=9897, Vender=11832\n",
      "Ganhos Totais: 36423.75, Perdas Totais: -35769.50\n",
      "Modelo e log do episódio 47 salvos em: 4.8.3\\model_episode_47.pth e 4.8.3\\log_episode_47.csv\n",
      "\n",
      "Episode 48/100, Total Reward: 1713.00, Win Rate: 0.54, Wins: 1296, Losses: 1112, Epsilon: 0.3086, Steps: 36754, Time: 130.07s\n",
      "Ações: Manter=14005, Comprar=10720, Vender=12029\n",
      "Ganhos Totais: 34928.00, Perdas Totais: -33215.00\n",
      "Modelo e log do episódio 48 salvos em: 4.8.3\\model_episode_48.pth e 4.8.3\\log_episode_48.csv\n",
      "\n",
      "Episode 49/100, Total Reward: 1956.00, Win Rate: 0.52, Wins: 1257, Losses: 1169, Epsilon: 0.3056, Steps: 36754, Time: 130.22s\n",
      "Ações: Manter=14505, Comprar=8156, Vender=14093\n",
      "Ganhos Totais: 37081.25, Perdas Totais: -35125.25\n",
      "Modelo e log do episódio 49 salvos em: 4.8.3\\model_episode_49.pth e 4.8.3\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: -1388.75, Win Rate: 0.53, Wins: 1245, Losses: 1089, Epsilon: 0.3025, Steps: 36754, Time: 130.15s\n",
      "Ações: Manter=13534, Comprar=8823, Vender=14397\n",
      "Ganhos Totais: 35372.00, Perdas Totais: -36760.75\n",
      "Episode 51/100, Total Reward: 2776.00, Win Rate: 0.55, Wins: 1208, Losses: 999, Epsilon: 0.2995, Steps: 36754, Time: 130.42s\n",
      "Ações: Manter=17034, Comprar=8248, Vender=11472\n",
      "Ganhos Totais: 35353.75, Perdas Totais: -32577.75\n",
      "Modelo e log do episódio 51 salvos em: 4.8.3\\model_episode_51.pth e 4.8.3\\log_episode_51.csv\n",
      "\n",
      "Episode 52/100, Total Reward: 2789.50, Win Rate: 0.54, Wins: 1179, Losses: 1012, Epsilon: 0.2965, Steps: 36754, Time: 130.28s\n",
      "Ações: Manter=14514, Comprar=10386, Vender=11854\n",
      "Ganhos Totais: 36635.25, Perdas Totais: -33845.75\n",
      "Modelo e log do episódio 52 salvos em: 4.8.3\\model_episode_52.pth e 4.8.3\\log_episode_52.csv\n",
      "\n",
      "Episode 53/100, Total Reward: -1263.50, Win Rate: 0.53, Wins: 1168, Losses: 1056, Epsilon: 0.2935, Steps: 36754, Time: 130.66s\n",
      "Ações: Manter=14731, Comprar=8486, Vender=13537\n",
      "Ganhos Totais: 34644.50, Perdas Totais: -35908.00\n",
      "Episode 54/100, Total Reward: -446.75, Win Rate: 0.52, Wins: 1122, Losses: 1034, Epsilon: 0.2906, Steps: 36754, Time: 130.64s\n",
      "Ações: Manter=13133, Comprar=13465, Vender=10156\n",
      "Ganhos Totais: 33911.00, Perdas Totais: -34357.75\n",
      "Episode 55/100, Total Reward: 1339.25, Win Rate: 0.54, Wins: 1230, Losses: 1044, Epsilon: 0.2877, Steps: 36754, Time: 130.77s\n",
      "Ações: Manter=10721, Comprar=11441, Vender=14592\n",
      "Ganhos Totais: 35847.25, Perdas Totais: -34508.00\n",
      "Episode 56/100, Total Reward: 300.75, Win Rate: 0.52, Wins: 1206, Losses: 1113, Epsilon: 0.2848, Steps: 36754, Time: 130.21s\n",
      "Ações: Manter=12712, Comprar=12832, Vender=11210\n",
      "Ganhos Totais: 35383.75, Perdas Totais: -35083.00\n",
      "Episode 57/100, Total Reward: 1080.00, Win Rate: 0.53, Wins: 1176, Losses: 1035, Epsilon: 0.2820, Steps: 36754, Time: 130.52s\n",
      "Ações: Manter=12901, Comprar=14303, Vender=9550\n",
      "Ganhos Totais: 35750.25, Perdas Totais: -34670.25\n",
      "Episode 58/100, Total Reward: -2043.25, Win Rate: 0.52, Wins: 1214, Losses: 1117, Epsilon: 0.2791, Steps: 36754, Time: 130.55s\n",
      "Ações: Manter=12078, Comprar=9805, Vender=14871\n",
      "Ganhos Totais: 33898.00, Perdas Totais: -35941.25\n",
      "Episode 59/100, Total Reward: -525.75, Win Rate: 0.53, Wins: 1134, Losses: 996, Epsilon: 0.2763, Steps: 36754, Time: 130.62s\n",
      "Ações: Manter=11111, Comprar=10835, Vender=14808\n",
      "Ganhos Totais: 33890.00, Perdas Totais: -34415.75\n",
      "Episode 60/100, Total Reward: 4704.75, Win Rate: 0.54, Wins: 1222, Losses: 1026, Epsilon: 0.2736, Steps: 36754, Time: 131.44s\n",
      "Ações: Manter=14581, Comprar=10194, Vender=11979\n",
      "Ganhos Totais: 36491.25, Perdas Totais: -31786.50\n",
      "Modelo e log do episódio 60 salvos em: 4.8.3\\model_episode_60.pth e 4.8.3\\log_episode_60.csv\n",
      "\n",
      "Episode 61/100, Total Reward: -828.50, Win Rate: 0.54, Wins: 1126, Losses: 956, Epsilon: 0.2708, Steps: 36754, Time: 130.51s\n",
      "Ações: Manter=18384, Comprar=10043, Vender=8327\n",
      "Ganhos Totais: 32192.50, Perdas Totais: -33021.00\n",
      "Episode 62/100, Total Reward: 1459.25, Win Rate: 0.51, Wins: 1068, Losses: 1034, Epsilon: 0.2681, Steps: 36754, Time: 130.56s\n",
      "Ações: Manter=10329, Comprar=13718, Vender=12707\n",
      "Ganhos Totais: 34227.25, Perdas Totais: -32768.00\n",
      "Episode 63/100, Total Reward: 218.25, Win Rate: 0.52, Wins: 1123, Losses: 1057, Epsilon: 0.2655, Steps: 36754, Time: 130.94s\n",
      "Ações: Manter=10327, Comprar=12910, Vender=13517\n",
      "Ganhos Totais: 35730.00, Perdas Totais: -35511.75\n",
      "Episode 64/100, Total Reward: 936.50, Win Rate: 0.52, Wins: 1083, Losses: 1010, Epsilon: 0.2628, Steps: 36754, Time: 130.96s\n",
      "Ações: Manter=9933, Comprar=13612, Vender=13209\n",
      "Ganhos Totais: 34648.00, Perdas Totais: -33711.50\n",
      "Episode 65/100, Total Reward: 3015.75, Win Rate: 0.51, Wins: 1013, Losses: 965, Epsilon: 0.2602, Steps: 36754, Time: 130.72s\n",
      "Ações: Manter=13695, Comprar=12582, Vender=10477\n",
      "Ganhos Totais: 34833.75, Perdas Totais: -31818.00\n",
      "Modelo e log do episódio 65 salvos em: 4.8.3\\model_episode_65.pth e 4.8.3\\log_episode_65.csv\n",
      "\n",
      "Episode 66/100, Total Reward: 180.00, Win Rate: 0.51, Wins: 886, Losses: 851, Epsilon: 0.2576, Steps: 36754, Time: 130.66s\n",
      "Ações: Manter=13769, Comprar=10843, Vender=12142\n",
      "Ganhos Totais: 32596.00, Perdas Totais: -32416.00\n",
      "Episode 67/100, Total Reward: -64.00, Win Rate: 0.51, Wins: 971, Losses: 946, Epsilon: 0.2550, Steps: 36754, Time: 130.71s\n",
      "Ações: Manter=12528, Comprar=10468, Vender=13758\n",
      "Ganhos Totais: 34678.75, Perdas Totais: -34742.75\n",
      "Episode 68/100, Total Reward: -1447.25, Win Rate: 0.51, Wins: 1009, Losses: 964, Epsilon: 0.2524, Steps: 36754, Time: 131.12s\n",
      "Ações: Manter=9153, Comprar=16157, Vender=11444\n",
      "Ganhos Totais: 33327.50, Perdas Totais: -34774.75\n",
      "Episode 69/100, Total Reward: 339.25, Win Rate: 0.49, Wins: 899, Losses: 928, Epsilon: 0.2499, Steps: 36754, Time: 131.39s\n",
      "Ações: Manter=14345, Comprar=9650, Vender=12759\n",
      "Ganhos Totais: 33447.75, Perdas Totais: -33108.50\n",
      "Episode 70/100, Total Reward: 4195.75, Win Rate: 0.52, Wins: 957, Losses: 893, Epsilon: 0.2474, Steps: 36754, Time: 130.82s\n",
      "Ações: Manter=13009, Comprar=12290, Vender=11455\n",
      "Ganhos Totais: 35071.00, Perdas Totais: -30875.25\n",
      "Modelo e log do episódio 70 salvos em: 4.8.3\\model_episode_70.pth e 4.8.3\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: 5114.75, Win Rate: 0.52, Wins: 1075, Losses: 993, Epsilon: 0.2449, Steps: 36754, Time: 131.12s\n",
      "Ações: Manter=9772, Comprar=16463, Vender=10519\n",
      "Ganhos Totais: 36532.50, Perdas Totais: -31417.75\n",
      "Modelo e log do episódio 71 salvos em: 4.8.3\\model_episode_71.pth e 4.8.3\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: 908.25, Win Rate: 0.52, Wins: 1007, Losses: 912, Epsilon: 0.2425, Steps: 36754, Time: 131.60s\n",
      "Ações: Manter=10203, Comprar=16208, Vender=10343\n",
      "Ganhos Totais: 33175.00, Perdas Totais: -32266.75\n",
      "Episode 73/100, Total Reward: -15.25, Win Rate: 0.52, Wins: 1074, Losses: 1000, Epsilon: 0.2401, Steps: 36754, Time: 130.82s\n",
      "Ações: Manter=9815, Comprar=17564, Vender=9375\n",
      "Ganhos Totais: 33045.50, Perdas Totais: -33060.75\n",
      "Episode 74/100, Total Reward: 1515.75, Win Rate: 0.51, Wins: 1002, Losses: 955, Epsilon: 0.2377, Steps: 36754, Time: 131.20s\n",
      "Ações: Manter=14657, Comprar=10885, Vender=11212\n",
      "Ganhos Totais: 35256.25, Perdas Totais: -33740.50\n",
      "Episode 75/100, Total Reward: 2516.00, Win Rate: 0.53, Wins: 980, Losses: 879, Epsilon: 0.2353, Steps: 36754, Time: 131.31s\n",
      "Ações: Manter=14802, Comprar=13193, Vender=8759\n",
      "Ganhos Totais: 33446.25, Perdas Totais: -30930.25\n",
      "Modelo e log do episódio 75 salvos em: 4.8.3\\model_episode_75.pth e 4.8.3\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: 6558.75, Win Rate: 0.53, Wins: 917, Losses: 807, Epsilon: 0.2329, Steps: 36754, Time: 131.22s\n",
      "Ações: Manter=17488, Comprar=13926, Vender=5340\n",
      "Ganhos Totais: 34021.75, Perdas Totais: -27463.00\n",
      "Modelo e log do episódio 76 salvos em: 4.8.3\\model_episode_76.pth e 4.8.3\\log_episode_76.csv\n",
      "\n",
      "Episode 77/100, Total Reward: 611.50, Win Rate: 0.50, Wins: 780, Losses: 767, Epsilon: 0.2306, Steps: 36754, Time: 131.03s\n",
      "Ações: Manter=19514, Comprar=6425, Vender=10815\n",
      "Ganhos Totais: 29803.25, Perdas Totais: -29191.75\n",
      "Episode 78/100, Total Reward: 6038.75, Win Rate: 0.53, Wins: 992, Losses: 894, Epsilon: 0.2283, Steps: 36754, Time: 131.71s\n",
      "Ações: Manter=11834, Comprar=14337, Vender=10583\n",
      "Ganhos Totais: 36637.75, Perdas Totais: -30599.00\n",
      "Modelo e log do episódio 78 salvos em: 4.8.3\\model_episode_78.pth e 4.8.3\\log_episode_78.csv\n",
      "\n",
      "Episode 79/100, Total Reward: 4215.00, Win Rate: 0.52, Wins: 814, Losses: 746, Epsilon: 0.2260, Steps: 36754, Time: 131.95s\n",
      "Ações: Manter=17835, Comprar=9696, Vender=9223\n",
      "Ganhos Totais: 32318.00, Perdas Totais: -28103.00\n",
      "Modelo e log do episódio 79 salvos em: 4.8.3\\model_episode_79.pth e 4.8.3\\log_episode_79.csv\n",
      "\n",
      "Episode 80/100, Total Reward: 4533.00, Win Rate: 0.53, Wins: 828, Losses: 722, Epsilon: 0.2238, Steps: 36754, Time: 131.28s\n",
      "Ações: Manter=18377, Comprar=6726, Vender=11651\n",
      "Ganhos Totais: 32039.50, Perdas Totais: -27506.50\n",
      "Modelo e log do episódio 80 salvos em: 4.8.3\\model_episode_80.pth e 4.8.3\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: -481.25, Win Rate: 0.49, Wins: 703, Losses: 734, Epsilon: 0.2215, Steps: 36754, Time: 132.38s\n",
      "Ações: Manter=16752, Comprar=7897, Vender=12105\n",
      "Ganhos Totais: 29074.25, Perdas Totais: -29555.50\n",
      "Episode 82/100, Total Reward: -786.00, Win Rate: 0.50, Wins: 786, Losses: 787, Epsilon: 0.2193, Steps: 36754, Time: 131.00s\n",
      "Ações: Manter=18065, Comprar=6496, Vender=12193\n",
      "Ganhos Totais: 29180.50, Perdas Totais: -29966.50\n",
      "Episode 83/100, Total Reward: 1049.50, Win Rate: 0.52, Wins: 807, Losses: 734, Epsilon: 0.2171, Steps: 36754, Time: 131.61s\n",
      "Ações: Manter=17612, Comprar=7118, Vender=12024\n",
      "Ganhos Totais: 30410.75, Perdas Totais: -29361.25\n",
      "Episode 84/100, Total Reward: 1079.75, Win Rate: 0.50, Wins: 737, Losses: 738, Epsilon: 0.2149, Steps: 36754, Time: 130.98s\n",
      "Ações: Manter=12300, Comprar=6644, Vender=17810\n",
      "Ganhos Totais: 31488.50, Perdas Totais: -30408.75\n",
      "Episode 85/100, Total Reward: -674.75, Win Rate: 0.50, Wins: 824, Losses: 814, Epsilon: 0.2128, Steps: 36754, Time: 131.24s\n",
      "Ações: Manter=11744, Comprar=6434, Vender=18576\n",
      "Ganhos Totais: 30865.75, Perdas Totais: -31540.50\n",
      "Episode 86/100, Total Reward: -1088.50, Win Rate: 0.50, Wins: 894, Losses: 899, Epsilon: 0.2107, Steps: 36754, Time: 131.68s\n",
      "Ações: Manter=10876, Comprar=6514, Vender=19364\n",
      "Ganhos Totais: 31540.50, Perdas Totais: -32629.00\n",
      "Episode 87/100, Total Reward: -1645.50, Win Rate: 0.49, Wins: 574, Losses: 594, Epsilon: 0.2086, Steps: 36754, Time: 133.73s\n",
      "Ações: Manter=21122, Comprar=4847, Vender=10785\n",
      "Ganhos Totais: 25042.75, Perdas Totais: -26688.25\n",
      "Episode 88/100, Total Reward: 1972.25, Win Rate: 0.52, Wins: 781, Losses: 715, Epsilon: 0.2065, Steps: 36754, Time: 130.98s\n",
      "Ações: Manter=14367, Comprar=5734, Vender=16653\n",
      "Ganhos Totais: 31284.00, Perdas Totais: -29311.75\n",
      "Episode 89/100, Total Reward: 2829.75, Win Rate: 0.50, Wins: 719, Losses: 733, Epsilon: 0.2044, Steps: 36754, Time: 132.02s\n",
      "Ações: Manter=16008, Comprar=5059, Vender=15687\n",
      "Ganhos Totais: 30544.75, Perdas Totais: -27715.00\n",
      "Modelo e log do episódio 89 salvos em: 4.8.3\\model_episode_89.pth e 4.8.3\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: -1501.00, Win Rate: 0.50, Wins: 745, Losses: 746, Epsilon: 0.2024, Steps: 36754, Time: 131.47s\n",
      "Ações: Manter=16677, Comprar=5921, Vender=14156\n",
      "Ganhos Totais: 29272.00, Perdas Totais: -30773.00\n",
      "Episode 91/100, Total Reward: -2017.50, Win Rate: 0.51, Wins: 741, Losses: 706, Epsilon: 0.2003, Steps: 36754, Time: 132.63s\n",
      "Ações: Manter=17135, Comprar=4848, Vender=14771\n",
      "Ganhos Totais: 27478.25, Perdas Totais: -29495.75\n",
      "Episode 92/100, Total Reward: 556.25, Win Rate: 0.52, Wins: 695, Losses: 649, Epsilon: 0.1983, Steps: 36754, Time: 131.92s\n",
      "Ações: Manter=12136, Comprar=5099, Vender=19519\n",
      "Ganhos Totais: 28859.75, Perdas Totais: -28303.50\n",
      "Episode 93/100, Total Reward: -548.25, Win Rate: 0.51, Wins: 748, Losses: 721, Epsilon: 0.1964, Steps: 36754, Time: 131.79s\n",
      "Ações: Manter=9815, Comprar=5433, Vender=21506\n",
      "Ganhos Totais: 30081.75, Perdas Totais: -30630.00\n",
      "Episode 94/100, Total Reward: -531.50, Win Rate: 0.50, Wins: 702, Losses: 702, Epsilon: 0.1944, Steps: 36754, Time: 132.29s\n",
      "Ações: Manter=6894, Comprar=5113, Vender=24747\n",
      "Ganhos Totais: 29136.25, Perdas Totais: -29667.75\n",
      "Episode 95/100, Total Reward: -2033.00, Win Rate: 0.50, Wins: 871, Losses: 863, Epsilon: 0.1924, Steps: 36754, Time: 132.12s\n",
      "Ações: Manter=6120, Comprar=4774, Vender=25860\n",
      "Ganhos Totais: 30675.25, Perdas Totais: -32708.25\n",
      "Episode 96/100, Total Reward: -2760.25, Win Rate: 0.49, Wins: 837, Losses: 886, Epsilon: 0.1905, Steps: 36754, Time: 132.04s\n",
      "Ações: Manter=5686, Comprar=4904, Vender=26164\n",
      "Ganhos Totais: 31329.50, Perdas Totais: -34089.75\n",
      "Episode 97/100, Total Reward: -3172.50, Win Rate: 0.50, Wins: 851, Losses: 841, Epsilon: 0.1886, Steps: 36754, Time: 131.14s\n",
      "Ações: Manter=13307, Comprar=6131, Vender=17316\n",
      "Ganhos Totais: 29635.25, Perdas Totais: -32807.75\n",
      "Episode 98/100, Total Reward: -5622.75, Win Rate: 0.49, Wins: 763, Losses: 796, Epsilon: 0.1867, Steps: 36754, Time: 126.69s\n",
      "Ações: Manter=10475, Comprar=4946, Vender=21333\n",
      "Ganhos Totais: 27922.75, Perdas Totais: -33545.50\n",
      "Episode 99/100, Total Reward: -3150.25, Win Rate: 0.49, Wins: 755, Losses: 787, Epsilon: 0.1849, Steps: 36754, Time: 126.00s\n",
      "Ações: Manter=16996, Comprar=5343, Vender=14415\n",
      "Ganhos Totais: 29009.75, Perdas Totais: -32160.00\n",
      "Episode 100/100, Total Reward: -3607.00, Win Rate: 0.50, Wins: 763, Losses: 756, Epsilon: 0.1830, Steps: 36754, Time: 126.29s\n",
      "Ações: Manter=13197, Comprar=5214, Vender=18343\n",
      "Ganhos Totais: 29111.75, Perdas Totais: -32718.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 76, Total Reward: 6558.75, Win Rate: 0.53, Wins: 917, Losses: 807, Ações: {0: 17488, 1: 13926, 2: 5340}, Steps: 36754, Time: 131.22s\n",
      "Rank 2: Episode 78, Total Reward: 6038.75, Win Rate: 0.53, Wins: 992, Losses: 894, Ações: {0: 11834, 1: 14337, 2: 10583}, Steps: 36754, Time: 131.71s\n",
      "Rank 3: Episode 71, Total Reward: 5114.75, Win Rate: 0.52, Wins: 1075, Losses: 993, Ações: {0: 9772, 1: 16463, 2: 10519}, Steps: 36754, Time: 131.12s\n",
      "Rank 4: Episode 60, Total Reward: 4704.75, Win Rate: 0.54, Wins: 1222, Losses: 1026, Ações: {0: 14581, 1: 10194, 2: 11979}, Steps: 36754, Time: 131.44s\n",
      "Rank 5: Episode 80, Total Reward: 4533.00, Win Rate: 0.53, Wins: 828, Losses: 722, Ações: {0: 18377, 1: 6726, 2: 11651}, Steps: 36754, Time: 131.28s\n",
      "Rank 6: Episode 79, Total Reward: 4215.00, Win Rate: 0.52, Wins: 814, Losses: 746, Ações: {0: 17835, 1: 9696, 2: 9223}, Steps: 36754, Time: 131.95s\n",
      "Rank 7: Episode 70, Total Reward: 4195.75, Win Rate: 0.52, Wins: 957, Losses: 893, Ações: {0: 13009, 1: 12290, 2: 11455}, Steps: 36754, Time: 130.82s\n",
      "Rank 8: Episode 65, Total Reward: 3015.75, Win Rate: 0.51, Wins: 1013, Losses: 965, Ações: {0: 13695, 1: 12582, 2: 10477}, Steps: 36754, Time: 130.72s\n",
      "Rank 9: Episode 89, Total Reward: 2829.75, Win Rate: 0.50, Wins: 719, Losses: 733, Ações: {0: 16008, 1: 5059, 2: 15687}, Steps: 36754, Time: 132.02s\n",
      "Rank 10: Episode 52, Total Reward: 2789.50, Win Rate: 0.54, Wins: 1179, Losses: 1012, Ações: {0: 14514, 1: 10386, 2: 11854}, Steps: 36754, Time: 130.28s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.8.3\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
