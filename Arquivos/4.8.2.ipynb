{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 2251.75, Win Rate: 0.51, Wins: 1577, Losses: 1517, Epsilon: 0.4950, Steps: 36754, Time: 144.24s\n",
      "Ações: Manter=11478, Comprar=12056, Vender=13220\n",
      "Ganhos Totais: 39915.00, Perdas Totais: -37663.25\n",
      "Modelo e log do episódio 1 salvos em: 4.8.2\\model_episode_1.pth e 4.8.2\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -3965.75, Win Rate: 0.52, Wins: 1501, Losses: 1375, Epsilon: 0.4900, Steps: 36754, Time: 127.35s\n",
      "Ações: Manter=11601, Comprar=12101, Vender=13052\n",
      "Ganhos Totais: 36398.75, Perdas Totais: -40364.50\n",
      "Modelo e log do episódio 2 salvos em: 4.8.2\\model_episode_2.pth e 4.8.2\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -6080.75, Win Rate: 0.50, Wins: 1389, Losses: 1370, Epsilon: 0.4851, Steps: 36754, Time: 128.07s\n",
      "Ações: Manter=11700, Comprar=12104, Vender=12950\n",
      "Ganhos Totais: 34697.75, Perdas Totais: -40778.50\n",
      "Modelo e log do episódio 3 salvos em: 4.8.2\\model_episode_3.pth e 4.8.2\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -883.50, Win Rate: 0.51, Wins: 1386, Losses: 1319, Epsilon: 0.4803, Steps: 36754, Time: 151.00s\n",
      "Ações: Manter=11934, Comprar=11954, Vender=12866\n",
      "Ganhos Totais: 36422.75, Perdas Totais: -37306.25\n",
      "Modelo e log do episódio 4 salvos em: 4.8.2\\model_episode_4.pth e 4.8.2\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: 3068.25, Win Rate: 0.53, Wins: 1495, Losses: 1307, Epsilon: 0.4755, Steps: 36754, Time: 148.08s\n",
      "Ações: Manter=11662, Comprar=12334, Vender=12758\n",
      "Ganhos Totais: 39820.25, Perdas Totais: -36752.00\n",
      "Modelo e log do episódio 5 salvos em: 4.8.2\\model_episode_5.pth e 4.8.2\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -5316.00, Win Rate: 0.51, Wins: 1426, Losses: 1345, Epsilon: 0.4707, Steps: 36754, Time: 148.08s\n",
      "Ações: Manter=12737, Comprar=11684, Vender=12333\n",
      "Ganhos Totais: 34834.25, Perdas Totais: -40150.25\n",
      "Modelo e log do episódio 6 salvos em: 4.8.2\\model_episode_6.pth e 4.8.2\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -4054.25, Win Rate: 0.53, Wins: 1380, Losses: 1246, Epsilon: 0.4660, Steps: 36754, Time: 154.71s\n",
      "Ações: Manter=12736, Comprar=12192, Vender=11826\n",
      "Ganhos Totais: 35199.25, Perdas Totais: -39253.50\n",
      "Modelo e log do episódio 7 salvos em: 4.8.2\\model_episode_7.pth e 4.8.2\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -4382.50, Win Rate: 0.52, Wins: 1375, Losses: 1279, Epsilon: 0.4614, Steps: 36754, Time: 152.63s\n",
      "Ações: Manter=12804, Comprar=11868, Vender=12082\n",
      "Ganhos Totais: 34897.25, Perdas Totais: -39279.75\n",
      "Modelo e log do episódio 8 salvos em: 4.8.2\\model_episode_8.pth e 4.8.2\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -1532.25, Win Rate: 0.52, Wins: 1506, Losses: 1368, Epsilon: 0.4568, Steps: 36754, Time: 156.30s\n",
      "Ações: Manter=12299, Comprar=11699, Vender=12756\n",
      "Ganhos Totais: 37693.75, Perdas Totais: -39226.00\n",
      "Modelo e log do episódio 9 salvos em: 4.8.2\\model_episode_9.pth e 4.8.2\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -5180.00, Win Rate: 0.51, Wins: 1406, Losses: 1331, Epsilon: 0.4522, Steps: 36754, Time: 151.40s\n",
      "Ações: Manter=12721, Comprar=11813, Vender=12220\n",
      "Ganhos Totais: 35490.75, Perdas Totais: -40670.75\n",
      "Modelo e log do episódio 10 salvos em: 4.8.2\\model_episode_10.pth e 4.8.2\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 3000.25, Win Rate: 0.55, Wins: 1505, Losses: 1221, Epsilon: 0.4477, Steps: 36754, Time: 154.30s\n",
      "Ações: Manter=12882, Comprar=11597, Vender=12275\n",
      "Ganhos Totais: 40421.25, Perdas Totais: -37421.00\n",
      "Modelo e log do episódio 11 salvos em: 4.8.2\\model_episode_11.pth e 4.8.2\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -3779.75, Win Rate: 0.53, Wins: 1428, Losses: 1283, Epsilon: 0.4432, Steps: 36754, Time: 160.73s\n",
      "Ações: Manter=12276, Comprar=11871, Vender=12607\n",
      "Ganhos Totais: 34873.75, Perdas Totais: -38653.50\n",
      "Modelo e log do episódio 12 salvos em: 4.8.2\\model_episode_12.pth e 4.8.2\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 399.75, Win Rate: 0.54, Wins: 1444, Losses: 1255, Epsilon: 0.4388, Steps: 36754, Time: 148.88s\n",
      "Ações: Manter=12004, Comprar=12680, Vender=12070\n",
      "Ganhos Totais: 37076.50, Perdas Totais: -36676.75\n",
      "Modelo e log do episódio 13 salvos em: 4.8.2\\model_episode_13.pth e 4.8.2\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -2760.50, Win Rate: 0.52, Wins: 1397, Losses: 1310, Epsilon: 0.4344, Steps: 36754, Time: 150.84s\n",
      "Ações: Manter=13018, Comprar=12030, Vender=11706\n",
      "Ganhos Totais: 36554.75, Perdas Totais: -39315.25\n",
      "Modelo e log do episódio 14 salvos em: 4.8.2\\model_episode_14.pth e 4.8.2\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -3907.25, Win Rate: 0.52, Wins: 1371, Losses: 1248, Epsilon: 0.4300, Steps: 36754, Time: 159.21s\n",
      "Ações: Manter=13843, Comprar=11636, Vender=11275\n",
      "Ganhos Totais: 35733.50, Perdas Totais: -39640.75\n",
      "Modelo e log do episódio 15 salvos em: 4.8.2\\model_episode_15.pth e 4.8.2\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: 889.75, Win Rate: 0.54, Wins: 1436, Losses: 1225, Epsilon: 0.4257, Steps: 36754, Time: 150.33s\n",
      "Ações: Manter=13612, Comprar=12201, Vender=10941\n",
      "Ganhos Totais: 37812.75, Perdas Totais: -36923.00\n",
      "Modelo e log do episódio 16 salvos em: 4.8.2\\model_episode_16.pth e 4.8.2\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: 2348.25, Win Rate: 0.54, Wins: 1543, Losses: 1336, Epsilon: 0.4215, Steps: 36754, Time: 147.56s\n",
      "Ações: Manter=13181, Comprar=12249, Vender=11324\n",
      "Ganhos Totais: 39848.00, Perdas Totais: -37499.75\n",
      "Modelo e log do episódio 17 salvos em: 4.8.2\\model_episode_17.pth e 4.8.2\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -579.50, Win Rate: 0.54, Wins: 1469, Losses: 1257, Epsilon: 0.4173, Steps: 36754, Time: 147.17s\n",
      "Ações: Manter=13166, Comprar=11819, Vender=11769\n",
      "Ganhos Totais: 37582.25, Perdas Totais: -38161.75\n",
      "Modelo e log do episódio 18 salvos em: 4.8.2\\model_episode_18.pth e 4.8.2\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -784.75, Win Rate: 0.54, Wins: 1433, Losses: 1223, Epsilon: 0.4131, Steps: 36754, Time: 147.55s\n",
      "Ações: Manter=13441, Comprar=12377, Vender=10936\n",
      "Ganhos Totais: 35737.25, Perdas Totais: -36522.00\n",
      "Modelo e log do episódio 19 salvos em: 4.8.2\\model_episode_19.pth e 4.8.2\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: -3233.50, Win Rate: 0.53, Wins: 1403, Losses: 1246, Epsilon: 0.4090, Steps: 36754, Time: 147.86s\n",
      "Ações: Manter=13808, Comprar=11401, Vender=11545\n",
      "Ganhos Totais: 35611.00, Perdas Totais: -38844.50\n",
      "Episode 21/100, Total Reward: 1411.75, Win Rate: 0.55, Wins: 1468, Losses: 1215, Epsilon: 0.4049, Steps: 36754, Time: 147.22s\n",
      "Ações: Manter=14005, Comprar=11886, Vender=10863\n",
      "Ganhos Totais: 38669.00, Perdas Totais: -37257.25\n",
      "Modelo e log do episódio 21 salvos em: 4.8.2\\model_episode_21.pth e 4.8.2\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: 604.50, Win Rate: 0.53, Wins: 1320, Losses: 1181, Epsilon: 0.4008, Steps: 36754, Time: 147.83s\n",
      "Ações: Manter=13845, Comprar=11838, Vender=11071\n",
      "Ganhos Totais: 37292.75, Perdas Totais: -36688.25\n",
      "Modelo e log do episódio 22 salvos em: 4.8.2\\model_episode_22.pth e 4.8.2\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: -1548.50, Win Rate: 0.52, Wins: 1204, Losses: 1106, Epsilon: 0.3968, Steps: 36754, Time: 147.40s\n",
      "Ações: Manter=14904, Comprar=11225, Vender=10625\n",
      "Ganhos Totais: 33977.50, Perdas Totais: -35526.00\n",
      "Episode 24/100, Total Reward: -4347.75, Win Rate: 0.52, Wins: 1214, Losses: 1123, Epsilon: 0.3928, Steps: 36754, Time: 147.66s\n",
      "Ações: Manter=13309, Comprar=11926, Vender=11519\n",
      "Ganhos Totais: 33380.75, Perdas Totais: -37728.50\n",
      "Episode 25/100, Total Reward: -173.75, Win Rate: 0.53, Wins: 1220, Losses: 1080, Epsilon: 0.3889, Steps: 36754, Time: 147.98s\n",
      "Ações: Manter=13779, Comprar=11828, Vender=11147\n",
      "Ganhos Totais: 34351.00, Perdas Totais: -34524.75\n",
      "Modelo e log do episódio 25 salvos em: 4.8.2\\model_episode_25.pth e 4.8.2\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -1794.25, Win Rate: 0.54, Wins: 1262, Losses: 1072, Epsilon: 0.3850, Steps: 36754, Time: 147.66s\n",
      "Ações: Manter=12734, Comprar=12060, Vender=11960\n",
      "Ganhos Totais: 34803.50, Perdas Totais: -36597.75\n",
      "Episode 27/100, Total Reward: -416.75, Win Rate: 0.54, Wins: 1321, Losses: 1120, Epsilon: 0.3812, Steps: 36754, Time: 148.21s\n",
      "Ações: Manter=13471, Comprar=11975, Vender=11308\n",
      "Ganhos Totais: 35545.75, Perdas Totais: -35962.50\n",
      "Modelo e log do episódio 27 salvos em: 4.8.2\\model_episode_27.pth e 4.8.2\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: 252.25, Win Rate: 0.54, Wins: 1324, Losses: 1121, Epsilon: 0.3774, Steps: 36754, Time: 147.94s\n",
      "Ações: Manter=13304, Comprar=12834, Vender=10616\n",
      "Ganhos Totais: 36054.25, Perdas Totais: -35802.00\n",
      "Modelo e log do episódio 28 salvos em: 4.8.2\\model_episode_28.pth e 4.8.2\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 2698.50, Win Rate: 0.56, Wins: 1363, Losses: 1072, Epsilon: 0.3736, Steps: 36754, Time: 148.64s\n",
      "Ações: Manter=14620, Comprar=10788, Vender=11346\n",
      "Ganhos Totais: 37245.25, Perdas Totais: -34546.75\n",
      "Modelo e log do episódio 29 salvos em: 4.8.2\\model_episode_29.pth e 4.8.2\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: -979.50, Win Rate: 0.55, Wins: 1358, Losses: 1125, Epsilon: 0.3699, Steps: 36754, Time: 149.50s\n",
      "Ações: Manter=12710, Comprar=12048, Vender=11996\n",
      "Ganhos Totais: 36365.25, Perdas Totais: -37344.75\n",
      "Episode 31/100, Total Reward: -904.75, Win Rate: 0.55, Wins: 1352, Losses: 1091, Epsilon: 0.3662, Steps: 36754, Time: 148.49s\n",
      "Ações: Manter=12252, Comprar=12386, Vender=12116\n",
      "Ganhos Totais: 36109.75, Perdas Totais: -37014.50\n",
      "Episode 32/100, Total Reward: 580.75, Win Rate: 0.52, Wins: 1243, Losses: 1128, Epsilon: 0.3625, Steps: 36754, Time: 149.01s\n",
      "Ações: Manter=13086, Comprar=11196, Vender=12472\n",
      "Ganhos Totais: 35889.75, Perdas Totais: -35309.00\n",
      "Modelo e log do episódio 32 salvos em: 4.8.2\\model_episode_32.pth e 4.8.2\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: -3674.50, Win Rate: 0.53, Wins: 1208, Losses: 1078, Epsilon: 0.3589, Steps: 36754, Time: 148.72s\n",
      "Ações: Manter=11759, Comprar=10727, Vender=14268\n",
      "Ganhos Totais: 33860.75, Perdas Totais: -37535.25\n",
      "Episode 34/100, Total Reward: 476.75, Win Rate: 0.55, Wins: 1234, Losses: 1024, Epsilon: 0.3553, Steps: 36754, Time: 148.64s\n",
      "Ações: Manter=10105, Comprar=11868, Vender=14781\n",
      "Ganhos Totais: 36964.00, Perdas Totais: -36487.25\n",
      "Modelo e log do episódio 34 salvos em: 4.8.2\\model_episode_34.pth e 4.8.2\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: 200.75, Win Rate: 0.55, Wins: 1161, Losses: 962, Epsilon: 0.3517, Steps: 36754, Time: 151.92s\n",
      "Ações: Manter=10736, Comprar=10836, Vender=15182\n",
      "Ganhos Totais: 36080.50, Perdas Totais: -35879.75\n",
      "Episode 36/100, Total Reward: -2481.00, Win Rate: 0.52, Wins: 1155, Losses: 1084, Epsilon: 0.3482, Steps: 36754, Time: 154.59s\n",
      "Ações: Manter=9909, Comprar=13007, Vender=13838\n",
      "Ganhos Totais: 35128.25, Perdas Totais: -37609.25\n",
      "Episode 37/100, Total Reward: -3025.75, Win Rate: 0.53, Wins: 1205, Losses: 1080, Epsilon: 0.3447, Steps: 36754, Time: 137.74s\n",
      "Ações: Manter=9852, Comprar=13120, Vender=13782\n",
      "Ganhos Totais: 34630.50, Perdas Totais: -37656.25\n",
      "Episode 38/100, Total Reward: -4286.50, Win Rate: 0.54, Wins: 1229, Losses: 1057, Epsilon: 0.3413, Steps: 36754, Time: 130.59s\n",
      "Ações: Manter=11404, Comprar=11962, Vender=13388\n",
      "Ganhos Totais: 34148.75, Perdas Totais: -38435.25\n",
      "Episode 39/100, Total Reward: -512.25, Win Rate: 0.54, Wins: 1225, Losses: 1030, Epsilon: 0.3379, Steps: 36754, Time: 130.41s\n",
      "Ações: Manter=10323, Comprar=12464, Vender=13967\n",
      "Ganhos Totais: 35249.25, Perdas Totais: -35761.50\n",
      "Episode 40/100, Total Reward: -4827.25, Win Rate: 0.53, Wins: 1157, Losses: 1024, Epsilon: 0.3345, Steps: 36754, Time: 130.25s\n",
      "Ações: Manter=11127, Comprar=12876, Vender=12751\n",
      "Ganhos Totais: 33477.25, Perdas Totais: -38304.50\n",
      "Episode 41/100, Total Reward: -4529.25, Win Rate: 0.52, Wins: 1104, Losses: 1030, Epsilon: 0.3311, Steps: 36754, Time: 129.97s\n",
      "Ações: Manter=10851, Comprar=11455, Vender=14448\n",
      "Ganhos Totais: 33730.25, Perdas Totais: -38259.50\n",
      "Episode 42/100, Total Reward: -3464.25, Win Rate: 0.52, Wins: 1086, Losses: 1018, Epsilon: 0.3278, Steps: 36754, Time: 130.11s\n",
      "Ações: Manter=9727, Comprar=11889, Vender=15138\n",
      "Ganhos Totais: 33710.75, Perdas Totais: -37175.00\n",
      "Episode 43/100, Total Reward: -1698.75, Win Rate: 0.51, Wins: 1093, Losses: 1061, Epsilon: 0.3246, Steps: 36754, Time: 130.24s\n",
      "Ações: Manter=9347, Comprar=11874, Vender=15533\n",
      "Ganhos Totais: 34499.25, Perdas Totais: -36198.00\n",
      "Episode 44/100, Total Reward: -667.00, Win Rate: 0.52, Wins: 1101, Losses: 1015, Epsilon: 0.3213, Steps: 36754, Time: 130.32s\n",
      "Ações: Manter=7959, Comprar=13026, Vender=15769\n",
      "Ganhos Totais: 35751.00, Perdas Totais: -36418.00\n",
      "Episode 45/100, Total Reward: -1731.75, Win Rate: 0.52, Wins: 1066, Losses: 979, Epsilon: 0.3181, Steps: 36754, Time: 130.12s\n",
      "Ações: Manter=10162, Comprar=13388, Vender=13204\n",
      "Ganhos Totais: 35484.50, Perdas Totais: -37216.25\n",
      "Episode 46/100, Total Reward: -3450.00, Win Rate: 0.51, Wins: 1000, Losses: 969, Epsilon: 0.3149, Steps: 36754, Time: 130.22s\n",
      "Ações: Manter=10209, Comprar=13238, Vender=13307\n",
      "Ganhos Totais: 33028.25, Perdas Totais: -36478.25\n",
      "Episode 47/100, Total Reward: -345.00, Win Rate: 0.54, Wins: 1118, Losses: 946, Epsilon: 0.3118, Steps: 36754, Time: 130.24s\n",
      "Ações: Manter=8757, Comprar=14158, Vender=13839\n",
      "Ganhos Totais: 35024.50, Perdas Totais: -35369.50\n",
      "Episode 48/100, Total Reward: -1890.50, Win Rate: 0.51, Wins: 1007, Losses: 951, Epsilon: 0.3086, Steps: 36754, Time: 130.04s\n",
      "Ações: Manter=9855, Comprar=13127, Vender=13772\n",
      "Ganhos Totais: 33901.00, Perdas Totais: -35791.50\n",
      "Episode 49/100, Total Reward: -5270.00, Win Rate: 0.51, Wins: 962, Losses: 916, Epsilon: 0.3056, Steps: 36754, Time: 130.63s\n",
      "Ações: Manter=10487, Comprar=12619, Vender=13648\n",
      "Ganhos Totais: 31881.50, Perdas Totais: -37151.50\n",
      "Episode 50/100, Total Reward: -6193.50, Win Rate: 0.52, Wins: 955, Losses: 883, Epsilon: 0.3025, Steps: 36754, Time: 130.38s\n",
      "Ações: Manter=11264, Comprar=12556, Vender=12934\n",
      "Ganhos Totais: 30898.75, Perdas Totais: -37092.25\n",
      "Episode 51/100, Total Reward: -5146.00, Win Rate: 0.52, Wins: 1009, Losses: 937, Epsilon: 0.2995, Steps: 36754, Time: 130.53s\n",
      "Ações: Manter=10238, Comprar=13274, Vender=13242\n",
      "Ganhos Totais: 32364.75, Perdas Totais: -37510.75\n",
      "Episode 52/100, Total Reward: -692.25, Win Rate: 0.53, Wins: 1083, Losses: 955, Epsilon: 0.2965, Steps: 36754, Time: 130.65s\n",
      "Ações: Manter=9932, Comprar=12627, Vender=14195\n",
      "Ganhos Totais: 35440.50, Perdas Totais: -36132.75\n",
      "Episode 53/100, Total Reward: -1892.75, Win Rate: 0.53, Wins: 1091, Losses: 960, Epsilon: 0.2935, Steps: 36754, Time: 130.41s\n",
      "Ações: Manter=9884, Comprar=11165, Vender=15705\n",
      "Ganhos Totais: 34938.25, Perdas Totais: -36831.00\n",
      "Episode 54/100, Total Reward: -197.00, Win Rate: 0.52, Wins: 1115, Losses: 1033, Epsilon: 0.2906, Steps: 36754, Time: 130.77s\n",
      "Ações: Manter=8442, Comprar=11373, Vender=16939\n",
      "Ganhos Totais: 36622.75, Perdas Totais: -36819.75\n",
      "Episode 55/100, Total Reward: -2851.25, Win Rate: 0.51, Wins: 947, Losses: 925, Epsilon: 0.2877, Steps: 36754, Time: 130.91s\n",
      "Ações: Manter=11374, Comprar=10278, Vender=15102\n",
      "Ganhos Totais: 33122.25, Perdas Totais: -35973.50\n",
      "Episode 56/100, Total Reward: -1909.25, Win Rate: 0.52, Wins: 1015, Losses: 950, Epsilon: 0.2848, Steps: 36754, Time: 130.75s\n",
      "Ações: Manter=11247, Comprar=10225, Vender=15282\n",
      "Ganhos Totais: 34216.75, Perdas Totais: -36126.00\n",
      "Episode 57/100, Total Reward: 1336.25, Win Rate: 0.53, Wins: 1100, Losses: 990, Epsilon: 0.2820, Steps: 36754, Time: 131.87s\n",
      "Ações: Manter=10845, Comprar=9624, Vender=16285\n",
      "Ganhos Totais: 36165.75, Perdas Totais: -34829.50\n",
      "Modelo e log do episódio 57 salvos em: 4.8.2\\model_episode_57.pth e 4.8.2\\log_episode_57.csv\n",
      "\n",
      "Episode 58/100, Total Reward: -455.50, Win Rate: 0.52, Wins: 1121, Losses: 1025, Epsilon: 0.2791, Steps: 36754, Time: 131.16s\n",
      "Ações: Manter=10111, Comprar=9765, Vender=16878\n",
      "Ganhos Totais: 36215.75, Perdas Totais: -36671.25\n",
      "Episode 59/100, Total Reward: 1268.75, Win Rate: 0.53, Wins: 1134, Losses: 988, Epsilon: 0.2763, Steps: 36754, Time: 131.02s\n",
      "Ações: Manter=11152, Comprar=9177, Vender=16425\n",
      "Ganhos Totais: 37633.50, Perdas Totais: -36364.75\n",
      "Modelo e log do episódio 59 salvos em: 4.8.2\\model_episode_59.pth e 4.8.2\\log_episode_59.csv\n",
      "\n",
      "Episode 60/100, Total Reward: -1383.50, Win Rate: 0.52, Wins: 1097, Losses: 996, Epsilon: 0.2736, Steps: 36754, Time: 130.81s\n",
      "Ações: Manter=12011, Comprar=9700, Vender=15043\n",
      "Ganhos Totais: 35846.00, Perdas Totais: -37229.50\n",
      "Episode 61/100, Total Reward: 141.25, Win Rate: 0.54, Wins: 1243, Losses: 1067, Epsilon: 0.2708, Steps: 36754, Time: 131.09s\n",
      "Ações: Manter=11353, Comprar=10353, Vender=15048\n",
      "Ganhos Totais: 36686.75, Perdas Totais: -36545.50\n",
      "Episode 62/100, Total Reward: -4786.75, Win Rate: 0.52, Wins: 1163, Losses: 1066, Epsilon: 0.2681, Steps: 36754, Time: 131.09s\n",
      "Ações: Manter=10302, Comprar=10851, Vender=15601\n",
      "Ganhos Totais: 34547.25, Perdas Totais: -39334.00\n",
      "Episode 63/100, Total Reward: -7144.00, Win Rate: 0.52, Wins: 1150, Losses: 1047, Epsilon: 0.2655, Steps: 36754, Time: 131.85s\n",
      "Ações: Manter=9245, Comprar=10960, Vender=16549\n",
      "Ganhos Totais: 33451.50, Perdas Totais: -40595.50\n",
      "Episode 64/100, Total Reward: -6006.00, Win Rate: 0.52, Wins: 1139, Losses: 1038, Epsilon: 0.2628, Steps: 36754, Time: 130.81s\n",
      "Ações: Manter=9619, Comprar=10322, Vender=16813\n",
      "Ganhos Totais: 34239.25, Perdas Totais: -40245.25\n",
      "Episode 65/100, Total Reward: -3374.75, Win Rate: 0.53, Wins: 1141, Losses: 1020, Epsilon: 0.2602, Steps: 36754, Time: 131.05s\n",
      "Ações: Manter=10141, Comprar=10669, Vender=15944\n",
      "Ganhos Totais: 34561.00, Perdas Totais: -37935.75\n",
      "Episode 66/100, Total Reward: -2895.25, Win Rate: 0.54, Wins: 1213, Losses: 1019, Epsilon: 0.2576, Steps: 36754, Time: 131.12s\n",
      "Ações: Manter=9121, Comprar=10306, Vender=17327\n",
      "Ganhos Totais: 35847.00, Perdas Totais: -38742.25\n",
      "Episode 67/100, Total Reward: -2595.75, Win Rate: 0.55, Wins: 1250, Losses: 1022, Epsilon: 0.2550, Steps: 36754, Time: 131.42s\n",
      "Ações: Manter=7967, Comprar=13220, Vender=15567\n",
      "Ganhos Totais: 36817.00, Perdas Totais: -39412.75\n",
      "Episode 68/100, Total Reward: -3074.00, Win Rate: 0.55, Wins: 1235, Losses: 1009, Epsilon: 0.2524, Steps: 36754, Time: 130.79s\n",
      "Ações: Manter=9653, Comprar=12288, Vender=14813\n",
      "Ganhos Totais: 35539.25, Perdas Totais: -38613.25\n",
      "Episode 69/100, Total Reward: -4671.00, Win Rate: 0.52, Wins: 1188, Losses: 1077, Epsilon: 0.2499, Steps: 36754, Time: 131.57s\n",
      "Ações: Manter=6967, Comprar=11813, Vender=17974\n",
      "Ganhos Totais: 34854.75, Perdas Totais: -39525.75\n",
      "Episode 70/100, Total Reward: -1589.00, Win Rate: 0.53, Wins: 1185, Losses: 1033, Epsilon: 0.2474, Steps: 36754, Time: 131.24s\n",
      "Ações: Manter=9726, Comprar=12023, Vender=15005\n",
      "Ganhos Totais: 36592.75, Perdas Totais: -38181.75\n",
      "Episode 71/100, Total Reward: -819.25, Win Rate: 0.55, Wins: 1253, Losses: 1032, Epsilon: 0.2449, Steps: 36754, Time: 131.28s\n",
      "Ações: Manter=8655, Comprar=12769, Vender=15330\n",
      "Ganhos Totais: 36785.50, Perdas Totais: -37604.75\n",
      "Episode 72/100, Total Reward: -1567.25, Win Rate: 0.54, Wins: 1139, Losses: 951, Epsilon: 0.2425, Steps: 36754, Time: 131.56s\n",
      "Ações: Manter=9781, Comprar=14576, Vender=12397\n",
      "Ganhos Totais: 35922.00, Perdas Totais: -37489.25\n",
      "Episode 73/100, Total Reward: 1479.75, Win Rate: 0.57, Wins: 1217, Losses: 923, Epsilon: 0.2401, Steps: 36754, Time: 131.20s\n",
      "Ações: Manter=10013, Comprar=13730, Vender=13011\n",
      "Ganhos Totais: 37938.75, Perdas Totais: -36459.00\n",
      "Modelo e log do episódio 73 salvos em: 4.8.2\\model_episode_73.pth e 4.8.2\\log_episode_73.csv\n",
      "\n",
      "Episode 74/100, Total Reward: -1680.00, Win Rate: 0.55, Wins: 1139, Losses: 937, Epsilon: 0.2377, Steps: 36754, Time: 131.15s\n",
      "Ações: Manter=10600, Comprar=13541, Vender=12613\n",
      "Ganhos Totais: 35699.00, Perdas Totais: -37379.00\n",
      "Episode 75/100, Total Reward: -3715.25, Win Rate: 0.54, Wins: 1130, Losses: 949, Epsilon: 0.2353, Steps: 36754, Time: 131.51s\n",
      "Ações: Manter=9805, Comprar=13972, Vender=12977\n",
      "Ganhos Totais: 35895.75, Perdas Totais: -39611.00\n",
      "Episode 76/100, Total Reward: 438.25, Win Rate: 0.57, Wins: 1202, Losses: 911, Epsilon: 0.2329, Steps: 36754, Time: 131.51s\n",
      "Ações: Manter=10068, Comprar=13355, Vender=13331\n",
      "Ganhos Totais: 37753.25, Perdas Totais: -37315.00\n",
      "Episode 77/100, Total Reward: -1643.50, Win Rate: 0.56, Wins: 1228, Losses: 976, Epsilon: 0.2306, Steps: 36754, Time: 131.24s\n",
      "Ações: Manter=10503, Comprar=15255, Vender=10996\n",
      "Ganhos Totais: 35694.25, Perdas Totais: -37337.75\n",
      "Episode 78/100, Total Reward: -1086.00, Win Rate: 0.55, Wins: 1107, Losses: 891, Epsilon: 0.2283, Steps: 36754, Time: 131.28s\n",
      "Ações: Manter=11701, Comprar=13373, Vender=11680\n",
      "Ganhos Totais: 35206.75, Perdas Totais: -36292.75\n",
      "Episode 79/100, Total Reward: -726.25, Win Rate: 0.56, Wins: 1119, Losses: 881, Epsilon: 0.2260, Steps: 36754, Time: 131.33s\n",
      "Ações: Manter=12675, Comprar=14209, Vender=9870\n",
      "Ganhos Totais: 34587.00, Perdas Totais: -35313.25\n",
      "Episode 80/100, Total Reward: -236.25, Win Rate: 0.56, Wins: 1137, Losses: 907, Epsilon: 0.2238, Steps: 36754, Time: 131.41s\n",
      "Ações: Manter=10634, Comprar=13300, Vender=12820\n",
      "Ganhos Totais: 35171.75, Perdas Totais: -35408.00\n",
      "Episode 81/100, Total Reward: 1871.50, Win Rate: 0.57, Wins: 1186, Losses: 905, Epsilon: 0.2215, Steps: 36754, Time: 131.61s\n",
      "Ações: Manter=10563, Comprar=14549, Vender=11642\n",
      "Ganhos Totais: 37710.00, Perdas Totais: -35838.50\n",
      "Modelo e log do episódio 81 salvos em: 4.8.2\\model_episode_81.pth e 4.8.2\\log_episode_81.csv\n",
      "\n",
      "Episode 82/100, Total Reward: -1120.25, Win Rate: 0.54, Wins: 1023, Losses: 878, Epsilon: 0.2193, Steps: 36754, Time: 131.33s\n",
      "Ações: Manter=12413, Comprar=13817, Vender=10524\n",
      "Ganhos Totais: 35617.50, Perdas Totais: -36737.75\n",
      "Episode 83/100, Total Reward: -3856.75, Win Rate: 0.55, Wins: 1053, Losses: 854, Epsilon: 0.2171, Steps: 36754, Time: 131.57s\n",
      "Ações: Manter=12712, Comprar=13506, Vender=10536\n",
      "Ganhos Totais: 33652.00, Perdas Totais: -37508.75\n",
      "Episode 84/100, Total Reward: 3608.75, Win Rate: 0.58, Wins: 1142, Losses: 827, Epsilon: 0.2149, Steps: 36754, Time: 132.45s\n",
      "Ações: Manter=12153, Comprar=13556, Vender=11045\n",
      "Ganhos Totais: 36461.00, Perdas Totais: -32852.25\n",
      "Modelo e log do episódio 84 salvos em: 4.8.2\\model_episode_84.pth e 4.8.2\\log_episode_84.csv\n",
      "\n",
      "Episode 85/100, Total Reward: -564.25, Win Rate: 0.58, Wins: 1137, Losses: 831, Epsilon: 0.2128, Steps: 36754, Time: 131.26s\n",
      "Ações: Manter=12233, Comprar=13488, Vender=11033\n",
      "Ganhos Totais: 35234.75, Perdas Totais: -35799.00\n",
      "Episode 86/100, Total Reward: -300.50, Win Rate: 0.56, Wins: 1127, Losses: 877, Epsilon: 0.2107, Steps: 36754, Time: 131.85s\n",
      "Ações: Manter=12266, Comprar=12622, Vender=11866\n",
      "Ganhos Totais: 36508.25, Perdas Totais: -36808.75\n",
      "Episode 87/100, Total Reward: 4544.50, Win Rate: 0.58, Wins: 1109, Losses: 803, Epsilon: 0.2086, Steps: 36754, Time: 131.71s\n",
      "Ações: Manter=12448, Comprar=12810, Vender=11496\n",
      "Ganhos Totais: 37739.25, Perdas Totais: -33194.75\n",
      "Modelo e log do episódio 87 salvos em: 4.8.2\\model_episode_87.pth e 4.8.2\\log_episode_87.csv\n",
      "\n",
      "Episode 88/100, Total Reward: -1583.75, Win Rate: 0.58, Wins: 1118, Losses: 826, Epsilon: 0.2065, Steps: 36754, Time: 131.21s\n",
      "Ações: Manter=12016, Comprar=13135, Vender=11603\n",
      "Ganhos Totais: 35067.00, Perdas Totais: -36650.75\n",
      "Episode 89/100, Total Reward: -66.50, Win Rate: 0.58, Wins: 1133, Losses: 820, Epsilon: 0.2044, Steps: 36754, Time: 132.31s\n",
      "Ações: Manter=11374, Comprar=14149, Vender=11231\n",
      "Ganhos Totais: 35325.50, Perdas Totais: -35392.00\n",
      "Episode 90/100, Total Reward: 5435.00, Win Rate: 0.59, Wins: 1146, Losses: 810, Epsilon: 0.2024, Steps: 36754, Time: 134.17s\n",
      "Ações: Manter=11721, Comprar=13552, Vender=11481\n",
      "Ganhos Totais: 37799.75, Perdas Totais: -32364.75\n",
      "Modelo e log do episódio 90 salvos em: 4.8.2\\model_episode_90.pth e 4.8.2\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: -186.75, Win Rate: 0.57, Wins: 1164, Losses: 865, Epsilon: 0.2003, Steps: 36754, Time: 131.52s\n",
      "Ações: Manter=10813, Comprar=14273, Vender=11668\n",
      "Ganhos Totais: 35511.50, Perdas Totais: -35698.25\n",
      "Episode 92/100, Total Reward: 2668.25, Win Rate: 0.60, Wins: 1266, Losses: 855, Epsilon: 0.1983, Steps: 36754, Time: 132.16s\n",
      "Ações: Manter=9926, Comprar=16617, Vender=10211\n",
      "Ganhos Totais: 36590.75, Perdas Totais: -33922.50\n",
      "Modelo e log do episódio 92 salvos em: 4.8.2\\model_episode_92.pth e 4.8.2\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: -295.75, Win Rate: 0.60, Wins: 1284, Losses: 862, Epsilon: 0.1964, Steps: 36754, Time: 131.63s\n",
      "Ações: Manter=10010, Comprar=16003, Vender=10741\n",
      "Ganhos Totais: 35646.50, Perdas Totais: -35942.25\n",
      "Episode 94/100, Total Reward: -307.25, Win Rate: 0.60, Wins: 1315, Losses: 864, Epsilon: 0.1944, Steps: 36754, Time: 132.15s\n",
      "Ações: Manter=8995, Comprar=15894, Vender=11865\n",
      "Ganhos Totais: 36126.50, Perdas Totais: -36433.75\n",
      "Episode 95/100, Total Reward: 985.50, Win Rate: 0.59, Wins: 1276, Losses: 891, Epsilon: 0.1924, Steps: 36754, Time: 132.48s\n",
      "Ações: Manter=8483, Comprar=16661, Vender=11610\n",
      "Ganhos Totais: 36911.00, Perdas Totais: -35925.50\n",
      "Episode 96/100, Total Reward: 4052.50, Win Rate: 0.60, Wins: 1279, Losses: 845, Epsilon: 0.1905, Steps: 36754, Time: 132.13s\n",
      "Ações: Manter=8390, Comprar=16520, Vender=11844\n",
      "Ganhos Totais: 38151.25, Perdas Totais: -34098.75\n",
      "Modelo e log do episódio 96 salvos em: 4.8.2\\model_episode_96.pth e 4.8.2\\log_episode_96.csv\n",
      "\n",
      "Episode 97/100, Total Reward: -1167.75, Win Rate: 0.59, Wins: 1213, Losses: 840, Epsilon: 0.1886, Steps: 36754, Time: 132.10s\n",
      "Ações: Manter=7556, Comprar=17142, Vender=12056\n",
      "Ganhos Totais: 34384.25, Perdas Totais: -35552.00\n",
      "Episode 98/100, Total Reward: 365.50, Win Rate: 0.60, Wins: 1194, Losses: 786, Epsilon: 0.1867, Steps: 36754, Time: 132.28s\n",
      "Ações: Manter=9729, Comprar=14986, Vender=12039\n",
      "Ganhos Totais: 34472.50, Perdas Totais: -34107.00\n",
      "Episode 99/100, Total Reward: 2680.00, Win Rate: 0.60, Wins: 1246, Losses: 836, Epsilon: 0.1849, Steps: 36754, Time: 132.13s\n",
      "Ações: Manter=8348, Comprar=16868, Vender=11538\n",
      "Ganhos Totais: 36388.00, Perdas Totais: -33708.00\n",
      "Modelo e log do episódio 99 salvos em: 4.8.2\\model_episode_99.pth e 4.8.2\\log_episode_99.csv\n",
      "\n",
      "Episode 100/100, Total Reward: 3286.00, Win Rate: 0.60, Wins: 1268, Losses: 854, Epsilon: 0.1830, Steps: 36754, Time: 131.72s\n",
      "Ações: Manter=7643, Comprar=16610, Vender=12501\n",
      "Ganhos Totais: 36283.50, Perdas Totais: -32997.50\n",
      "Modelo e log do episódio 100 salvos em: 4.8.2\\model_episode_100.pth e 4.8.2\\log_episode_100.csv\n",
      "\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 90, Total Reward: 5435.00, Win Rate: 0.59, Wins: 1146, Losses: 810, Ações: {0: 11721, 1: 13552, 2: 11481}, Steps: 36754, Time: 134.17s\n",
      "Rank 2: Episode 87, Total Reward: 4544.50, Win Rate: 0.58, Wins: 1109, Losses: 803, Ações: {0: 12448, 1: 12810, 2: 11496}, Steps: 36754, Time: 131.71s\n",
      "Rank 3: Episode 96, Total Reward: 4052.50, Win Rate: 0.60, Wins: 1279, Losses: 845, Ações: {0: 8390, 1: 16520, 2: 11844}, Steps: 36754, Time: 132.13s\n",
      "Rank 4: Episode 84, Total Reward: 3608.75, Win Rate: 0.58, Wins: 1142, Losses: 827, Ações: {0: 12153, 1: 13556, 2: 11045}, Steps: 36754, Time: 132.45s\n",
      "Rank 5: Episode 100, Total Reward: 3286.00, Win Rate: 0.60, Wins: 1268, Losses: 854, Ações: {0: 7643, 1: 16610, 2: 12501}, Steps: 36754, Time: 131.72s\n",
      "Rank 6: Episode 5, Total Reward: 3068.25, Win Rate: 0.53, Wins: 1495, Losses: 1307, Ações: {0: 11662, 1: 12334, 2: 12758}, Steps: 36754, Time: 148.08s\n",
      "Rank 7: Episode 11, Total Reward: 3000.25, Win Rate: 0.55, Wins: 1505, Losses: 1221, Ações: {0: 12882, 1: 11597, 2: 12275}, Steps: 36754, Time: 154.30s\n",
      "Rank 8: Episode 29, Total Reward: 2698.50, Win Rate: 0.56, Wins: 1363, Losses: 1072, Ações: {0: 14620, 1: 10788, 2: 11346}, Steps: 36754, Time: 148.64s\n",
      "Rank 9: Episode 99, Total Reward: 2680.00, Win Rate: 0.60, Wins: 1246, Losses: 836, Ações: {0: 8348, 1: 16868, 2: 11538}, Steps: 36754, Time: 132.13s\n",
      "Rank 10: Episode 92, Total Reward: 2668.25, Win Rate: 0.60, Wins: 1266, Losses: 855, Ações: {0: 9926, 1: 16617, 2: 10211}, Steps: 36754, Time: 132.16s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.8.2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
