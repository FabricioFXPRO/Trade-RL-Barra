{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass\n",
    "        else:\n",
    "            # Fechar posição se o gatilho não estiver ativo\n",
    "            if self.position == 1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 3: Criar o Agente Rainbow DQN usando PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Implementação da Camada Noisy Linear\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma_init = sigma_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.zeros(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.zeros(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return nn.functional.linear(x, weight, bias)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt())\n",
    "\n",
    "# Implementação da Rede Rainbow DQN\n",
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(obs_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Advantage stream\n",
    "        self.advantage = nn.Sequential(\n",
    "            NoisyLinear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(256, n_actions)\n",
    "        )\n",
    "        # Value stream\n",
    "        self.value = nn.Sequential(\n",
    "            NoisyLinear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        # Dueling Q-values\n",
    "        q_values = value + advantage - advantage.mean()\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()\n",
    "\n",
    "# Implementação do Prioritized Replay Buffer\n",
    "class PrioritizedReplayBuffer(object):\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def push(self, *args):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((*args,))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (*args,)\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        batch = list(zip(*samples))\n",
    "\n",
    "        states = torch.cat(batch[0]).to(device)\n",
    "        actions = torch.tensor(batch[1], dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(batch[2], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.cat(batch[3]).to(device)\n",
    "        dones = torch.tensor(batch[4], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 4: Treinamento do Agente Rainbow DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -2993.00, Win Rate: 0.57, Wins: 518, Losses: 397, Steps: 36754, Time: 389.32s\n",
      "Ações: Manter=8035, Comprar=20421, Vender=8298\n",
      "Ganhos Totais: 25735.75, Perdas Totais: -28499.25\n",
      "Modelo e log do episódio 1 salvos em: 4.18.4\\model_episode_1.pth e 4.18.4\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -1565.50, Win Rate: 0.55, Wins: 784, Losses: 630, Steps: 36754, Time: 444.46s\n",
      "Ações: Manter=4617, Comprar=21001, Vender=11136\n",
      "Ganhos Totais: 28669.25, Perdas Totais: -29879.25\n",
      "Modelo e log do episódio 2 salvos em: 4.18.4\\model_episode_2.pth e 4.18.4\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1734.25, Win Rate: 0.55, Wins: 753, Losses: 608, Steps: 36754, Time: 438.49s\n",
      "Ações: Manter=4994, Comprar=21326, Vender=10434\n",
      "Ganhos Totais: 29442.75, Perdas Totais: -30835.75\n",
      "Modelo e log do episódio 3 salvos em: 4.18.4\\model_episode_3.pth e 4.18.4\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -1455.00, Win Rate: 0.57, Wins: 758, Losses: 579, Steps: 36754, Time: 458.06s\n",
      "Ações: Manter=5642, Comprar=20950, Vender=10162\n",
      "Ganhos Totais: 28774.50, Perdas Totais: -29893.50\n",
      "Modelo e log do episódio 4 salvos em: 4.18.4\\model_episode_4.pth e 4.18.4\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -2071.25, Win Rate: 0.52, Wins: 893, Losses: 834, Steps: 36754, Time: 468.19s\n",
      "Ações: Manter=5957, Comprar=17559, Vender=13238\n",
      "Ganhos Totais: 30618.75, Perdas Totais: -32256.75\n",
      "Modelo e log do episódio 5 salvos em: 4.18.4\\model_episode_5.pth e 4.18.4\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -2425.75, Win Rate: 0.54, Wins: 1118, Losses: 960, Steps: 36754, Time: 465.43s\n",
      "Ações: Manter=11505, Comprar=11427, Vender=13822\n",
      "Ganhos Totais: 32377.25, Perdas Totais: -34281.00\n",
      "Modelo e log do episódio 6 salvos em: 4.18.4\\model_episode_6.pth e 4.18.4\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: 181.00, Win Rate: 0.54, Wins: 1051, Losses: 894, Steps: 36754, Time: 461.30s\n",
      "Ações: Manter=14401, Comprar=11260, Vender=11093\n",
      "Ganhos Totais: 32288.75, Perdas Totais: -31619.50\n",
      "Modelo e log do episódio 7 salvos em: 4.18.4\\model_episode_7.pth e 4.18.4\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 5033.50, Win Rate: 0.53, Wins: 709, Losses: 634, Steps: 36754, Time: 459.35s\n",
      "Ações: Manter=16910, Comprar=15424, Vender=4420\n",
      "Ganhos Totais: 29549.75, Perdas Totais: -24180.00\n",
      "Modelo e log do episódio 8 salvos em: 4.18.4\\model_episode_8.pth e 4.18.4\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: 4204.25, Win Rate: 0.55, Wins: 471, Losses: 388, Steps: 36754, Time: 478.90s\n",
      "Ações: Manter=13652, Comprar=17904, Vender=5198\n",
      "Ganhos Totais: 27185.75, Perdas Totais: -22766.50\n",
      "Modelo e log do episódio 9 salvos em: 4.18.4\\model_episode_9.pth e 4.18.4\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: 5762.00, Win Rate: 0.53, Wins: 498, Losses: 449, Steps: 36754, Time: 485.00s\n",
      "Ações: Manter=13393, Comprar=18083, Vender=5278\n",
      "Ganhos Totais: 28900.75, Perdas Totais: -22901.00\n",
      "Modelo e log do episódio 10 salvos em: 4.18.4\\model_episode_10.pth e 4.18.4\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 3556.00, Win Rate: 0.54, Wins: 541, Losses: 457, Steps: 36754, Time: 488.82s\n",
      "Ações: Manter=10450, Comprar=21965, Vender=4339\n",
      "Ganhos Totais: 28821.00, Perdas Totais: -25014.75\n",
      "Modelo e log do episódio 11 salvos em: 4.18.4\\model_episode_11.pth e 4.18.4\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 6928.00, Win Rate: 0.53, Wins: 943, Losses: 843, Steps: 36754, Time: 488.76s\n",
      "Ações: Manter=10287, Comprar=18388, Vender=8079\n",
      "Ganhos Totais: 34283.50, Perdas Totais: -26907.25\n",
      "Modelo e log do episódio 12 salvos em: 4.18.4\\model_episode_12.pth e 4.18.4\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 596.50, Win Rate: 0.51, Wins: 785, Losses: 741, Steps: 36754, Time: 489.19s\n",
      "Ações: Manter=11409, Comprar=18469, Vender=6876\n",
      "Ganhos Totais: 29229.00, Perdas Totais: -28249.00\n",
      "Modelo e log do episódio 13 salvos em: 4.18.4\\model_episode_13.pth e 4.18.4\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 4698.25, Win Rate: 0.55, Wins: 738, Losses: 611, Steps: 36754, Time: 466.43s\n",
      "Ações: Manter=11721, Comprar=16600, Vender=8433\n",
      "Ganhos Totais: 31546.50, Perdas Totais: -26509.75\n",
      "Modelo e log do episódio 14 salvos em: 4.18.4\\model_episode_14.pth e 4.18.4\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: 3031.75, Win Rate: 0.55, Wins: 750, Losses: 611, Steps: 36754, Time: 439.18s\n",
      "Ações: Manter=11949, Comprar=17036, Vender=7769\n",
      "Ganhos Totais: 31603.50, Perdas Totais: -28230.50\n",
      "Modelo e log do episódio 15 salvos em: 4.18.4\\model_episode_15.pth e 4.18.4\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: 4433.00, Win Rate: 0.54, Wins: 527, Losses: 445, Steps: 36754, Time: 441.90s\n",
      "Ações: Manter=8615, Comprar=24282, Vender=3857\n",
      "Ganhos Totais: 28325.25, Perdas Totais: -23648.00\n",
      "Modelo e log do episódio 16 salvos em: 4.18.4\\model_episode_16.pth e 4.18.4\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: 1749.00, Win Rate: 0.53, Wins: 492, Losses: 443, Steps: 36754, Time: 437.88s\n",
      "Ações: Manter=8766, Comprar=23853, Vender=4135\n",
      "Ganhos Totais: 26209.00, Perdas Totais: -24225.00\n",
      "Modelo e log do episódio 17 salvos em: 4.18.4\\model_episode_17.pth e 4.18.4\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: 1835.25, Win Rate: 0.50, Wins: 667, Losses: 680, Steps: 36754, Time: 436.64s\n",
      "Ações: Manter=5694, Comprar=24905, Vender=6155\n",
      "Ganhos Totais: 28284.25, Perdas Totais: -26111.50\n",
      "Modelo e log do episódio 18 salvos em: 4.18.4\\model_episode_18.pth e 4.18.4\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: 5180.00, Win Rate: 0.52, Wins: 719, Losses: 677, Steps: 36754, Time: 417.26s\n",
      "Ações: Manter=9139, Comprar=23106, Vender=4509\n",
      "Ganhos Totais: 30348.50, Perdas Totais: -24817.75\n",
      "Modelo e log do episódio 19 salvos em: 4.18.4\\model_episode_19.pth e 4.18.4\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 3665.75, Win Rate: 0.50, Wins: 431, Losses: 423, Steps: 36754, Time: 388.45s\n",
      "Ações: Manter=13792, Comprar=18600, Vender=4362\n",
      "Ganhos Totais: 27195.00, Perdas Totais: -23314.50\n",
      "Modelo e log do episódio 20 salvos em: 4.18.4\\model_episode_20.pth e 4.18.4\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: 2680.25, Win Rate: 0.51, Wins: 416, Losses: 394, Steps: 36754, Time: 399.02s\n",
      "Ações: Manter=11248, Comprar=20806, Vender=4700\n",
      "Ganhos Totais: 26912.50, Perdas Totais: -24029.25\n",
      "Episode 22/100, Total Reward: 5137.50, Win Rate: 0.53, Wins: 861, Losses: 756, Steps: 36754, Time: 388.35s\n",
      "Ações: Manter=6138, Comprar=26712, Vender=3904\n",
      "Ganhos Totais: 32805.00, Perdas Totais: -27261.25\n",
      "Modelo e log do episódio 22 salvos em: 4.18.4\\model_episode_22.pth e 4.18.4\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: 3171.50, Win Rate: 0.49, Wins: 480, Losses: 490, Steps: 36754, Time: 375.64s\n",
      "Ações: Manter=9946, Comprar=23994, Vender=2814\n",
      "Ganhos Totais: 26637.25, Perdas Totais: -23223.00\n",
      "Episode 24/100, Total Reward: 1660.00, Win Rate: 0.53, Wins: 638, Losses: 562, Steps: 36754, Time: 373.21s\n",
      "Ações: Manter=4829, Comprar=28200, Vender=3725\n",
      "Ganhos Totais: 29054.75, Perdas Totais: -27094.25\n",
      "Episode 25/100, Total Reward: 7326.50, Win Rate: 0.54, Wins: 944, Losses: 805, Steps: 36754, Time: 372.92s\n",
      "Ações: Manter=7492, Comprar=22458, Vender=6804\n",
      "Ganhos Totais: 33408.25, Perdas Totais: -25642.00\n",
      "Modelo e log do episódio 25 salvos em: 4.18.4\\model_episode_25.pth e 4.18.4\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: 4954.75, Win Rate: 0.54, Wins: 867, Losses: 726, Steps: 36754, Time: 375.12s\n",
      "Ações: Manter=8376, Comprar=24946, Vender=3432\n",
      "Ganhos Totais: 31351.00, Perdas Totais: -25996.25\n",
      "Modelo e log do episódio 26 salvos em: 4.18.4\\model_episode_26.pth e 4.18.4\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: 6976.00, Win Rate: 0.52, Wins: 564, Losses: 525, Steps: 36754, Time: 373.43s\n",
      "Ações: Manter=7533, Comprar=25457, Vender=3764\n",
      "Ganhos Totais: 30207.75, Perdas Totais: -22959.50\n",
      "Modelo e log do episódio 27 salvos em: 4.18.4\\model_episode_27.pth e 4.18.4\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: 4733.25, Win Rate: 0.56, Wins: 665, Losses: 526, Steps: 36754, Time: 373.63s\n",
      "Ações: Manter=7080, Comprar=26263, Vender=3411\n",
      "Ganhos Totais: 29164.25, Perdas Totais: -24132.00\n",
      "Modelo e log do episódio 28 salvos em: 4.18.4\\model_episode_28.pth e 4.18.4\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 5826.75, Win Rate: 0.53, Wins: 781, Losses: 701, Steps: 36754, Time: 373.55s\n",
      "Ações: Manter=7176, Comprar=25754, Vender=3824\n",
      "Ganhos Totais: 31871.50, Perdas Totais: -25672.00\n",
      "Modelo e log do episódio 29 salvos em: 4.18.4\\model_episode_29.pth e 4.18.4\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: 4535.75, Win Rate: 0.51, Wins: 569, Losses: 536, Steps: 36754, Time: 373.64s\n",
      "Ações: Manter=13871, Comprar=17883, Vender=5000\n",
      "Ganhos Totais: 28373.00, Perdas Totais: -23559.25\n",
      "Episode 31/100, Total Reward: 4234.00, Win Rate: 0.52, Wins: 556, Losses: 517, Steps: 36754, Time: 373.22s\n",
      "Ações: Manter=12773, Comprar=17588, Vender=6393\n",
      "Ganhos Totais: 27994.50, Perdas Totais: -23491.25\n",
      "Episode 32/100, Total Reward: 6709.50, Win Rate: 0.55, Wins: 641, Losses: 524, Steps: 36754, Time: 373.60s\n",
      "Ações: Manter=8877, Comprar=23632, Vender=4245\n",
      "Ganhos Totais: 30963.00, Perdas Totais: -23962.00\n",
      "Modelo e log do episódio 32 salvos em: 4.18.4\\model_episode_32.pth e 4.18.4\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 5212.25, Win Rate: 0.56, Wins: 1073, Losses: 841, Steps: 36754, Time: 373.80s\n",
      "Ações: Manter=9315, Comprar=19216, Vender=8223\n",
      "Ganhos Totais: 34385.50, Perdas Totais: -28691.75\n",
      "Modelo e log do episódio 33 salvos em: 4.18.4\\model_episode_33.pth e 4.18.4\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: 7183.25, Win Rate: 0.59, Wins: 1027, Losses: 715, Steps: 36754, Time: 373.22s\n",
      "Ações: Manter=11207, Comprar=18113, Vender=7434\n",
      "Ganhos Totais: 35720.75, Perdas Totais: -28099.00\n",
      "Modelo e log do episódio 34 salvos em: 4.18.4\\model_episode_34.pth e 4.18.4\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: 1601.50, Win Rate: 0.56, Wins: 676, Losses: 536, Steps: 36754, Time: 373.73s\n",
      "Ações: Manter=9481, Comprar=19097, Vender=8176\n",
      "Ganhos Totais: 28370.25, Perdas Totais: -26464.00\n",
      "Episode 36/100, Total Reward: 70.50, Win Rate: 0.52, Wins: 616, Losses: 569, Steps: 36754, Time: 373.47s\n",
      "Ações: Manter=21476, Comprar=9530, Vender=5748\n",
      "Ganhos Totais: 19944.75, Perdas Totais: -19576.50\n",
      "Episode 37/100, Total Reward: -2876.50, Win Rate: 0.46, Wins: 278, Losses: 321, Steps: 36754, Time: 373.59s\n",
      "Ações: Manter=26151, Comprar=3412, Vender=7191\n",
      "Ganhos Totais: 12842.50, Perdas Totais: -15568.25\n",
      "Episode 38/100, Total Reward: 454.75, Win Rate: 0.47, Wins: 119, Losses: 134, Steps: 36754, Time: 374.79s\n",
      "Ações: Manter=29390, Comprar=3378, Vender=3986\n",
      "Ganhos Totais: 8458.75, Perdas Totais: -7940.50\n",
      "Episode 39/100, Total Reward: 30.50, Win Rate: 0.56, Wins: 5, Losses: 4, Steps: 36754, Time: 372.13s\n",
      "Ações: Manter=36535, Comprar=139, Vender=80\n",
      "Ganhos Totais: 344.75, Perdas Totais: -312.00\n",
      "Episode 40/100, Total Reward: 2703.25, Win Rate: 0.52, Wins: 197, Losses: 181, Steps: 36754, Time: 372.44s\n",
      "Ações: Manter=6208, Comprar=30532, Vender=14\n",
      "Ganhos Totais: 20601.75, Perdas Totais: -17804.00\n",
      "Episode 41/100, Total Reward: 419.75, Win Rate: 0.50, Wins: 149, Losses: 148, Steps: 36754, Time: 373.76s\n",
      "Ações: Manter=25732, Comprar=8816, Vender=2206\n",
      "Ganhos Totais: 8455.75, Perdas Totais: -7961.75\n",
      "Episode 42/100, Total Reward: -536.75, Win Rate: 0.54, Wins: 343, Losses: 290, Steps: 36754, Time: 404.44s\n",
      "Ações: Manter=16502, Comprar=8555, Vender=11697\n",
      "Ganhos Totais: 16192.50, Perdas Totais: -16570.50\n",
      "Episode 43/100, Total Reward: 2212.00, Win Rate: 0.70, Wins: 85, Losses: 37, Steps: 36754, Time: 401.08s\n",
      "Ações: Manter=35487, Comprar=887, Vender=380\n",
      "Ganhos Totais: 4336.25, Perdas Totais: -2093.75\n",
      "Episode 44/100, Total Reward: -2096.25, Win Rate: 0.36, Wins: 25, Losses: 44, Steps: 36754, Time: 398.40s\n",
      "Ações: Manter=31049, Comprar=245, Vender=5460\n",
      "Ganhos Totais: 2370.50, Perdas Totais: -4449.50\n",
      "Episode 45/100, Total Reward: 5060.50, Win Rate: 0.55, Wins: 229, Losses: 186, Steps: 36754, Time: 391.25s\n",
      "Ações: Manter=2280, Comprar=34374, Vender=100\n",
      "Ganhos Totais: 23693.00, Perdas Totais: -18528.75\n",
      "Episode 46/100, Total Reward: 4440.75, Win Rate: 0.54, Wins: 237, Losses: 201, Steps: 36754, Time: 401.52s\n",
      "Ações: Manter=307, Comprar=36119, Vender=328\n",
      "Ganhos Totais: 24181.75, Perdas Totais: -19631.50\n",
      "Episode 47/100, Total Reward: 4447.75, Win Rate: 0.56, Wins: 263, Losses: 206, Steps: 36754, Time: 403.24s\n",
      "Ações: Manter=1204, Comprar=35118, Vender=432\n",
      "Ganhos Totais: 24645.25, Perdas Totais: -20080.25\n",
      "Episode 48/100, Total Reward: 4586.75, Win Rate: 0.55, Wins: 242, Losses: 202, Steps: 36754, Time: 403.20s\n",
      "Ações: Manter=61, Comprar=36210, Vender=483\n",
      "Ganhos Totais: 24519.50, Perdas Totais: -19821.75\n",
      "Episode 49/100, Total Reward: 4482.25, Win Rate: 0.54, Wins: 238, Losses: 203, Steps: 36754, Time: 400.59s\n",
      "Ações: Manter=781, Comprar=35593, Vender=380\n",
      "Ganhos Totais: 24423.50, Perdas Totais: -19831.00\n",
      "Episode 50/100, Total Reward: 4491.00, Win Rate: 0.55, Wins: 250, Losses: 205, Steps: 36754, Time: 396.70s\n",
      "Ações: Manter=15, Comprar=36470, Vender=269\n",
      "Ganhos Totais: 24508.75, Perdas Totais: -19904.00\n",
      "Episode 51/100, Total Reward: 4187.00, Win Rate: 0.55, Wins: 252, Losses: 210, Steps: 36754, Time: 417.51s\n",
      "Ações: Manter=19, Comprar=36474, Vender=261\n",
      "Ganhos Totais: 24284.00, Perdas Totais: -19981.50\n",
      "Episode 52/100, Total Reward: 4338.00, Win Rate: 0.54, Wins: 258, Losses: 218, Steps: 36754, Time: 404.11s\n",
      "Ações: Manter=554, Comprar=35925, Vender=275\n",
      "Ganhos Totais: 24475.00, Perdas Totais: -20018.00\n",
      "Episode 53/100, Total Reward: 4203.50, Win Rate: 0.55, Wins: 299, Losses: 243, Steps: 36754, Time: 389.56s\n",
      "Ações: Manter=97, Comprar=36190, Vender=467\n",
      "Ganhos Totais: 25225.00, Perdas Totais: -20885.50\n",
      "Episode 54/100, Total Reward: 4221.75, Win Rate: 0.57, Wins: 332, Losses: 246, Steps: 36754, Time: 347.58s\n",
      "Ações: Manter=625, Comprar=35565, Vender=564\n",
      "Ganhos Totais: 25238.25, Perdas Totais: -20871.50\n",
      "Episode 55/100, Total Reward: 4272.50, Win Rate: 0.57, Wins: 326, Losses: 246, Steps: 36754, Time: 346.64s\n",
      "Ações: Manter=369, Comprar=35848, Vender=537\n",
      "Ganhos Totais: 24990.75, Perdas Totais: -20574.75\n",
      "Episode 56/100, Total Reward: 4000.75, Win Rate: 0.58, Wins: 334, Losses: 240, Steps: 36754, Time: 345.89s\n",
      "Ações: Manter=603, Comprar=35642, Vender=509\n",
      "Ganhos Totais: 24873.50, Perdas Totais: -20728.75\n",
      "Episode 57/100, Total Reward: 4160.00, Win Rate: 0.57, Wins: 327, Losses: 251, Steps: 36754, Time: 350.07s\n",
      "Ações: Manter=93, Comprar=36150, Vender=511\n",
      "Ganhos Totais: 25228.50, Perdas Totais: -20923.50\n",
      "Episode 58/100, Total Reward: 4020.50, Win Rate: 0.57, Wins: 338, Losses: 250, Steps: 36754, Time: 449.68s\n",
      "Ações: Manter=370, Comprar=35903, Vender=481\n",
      "Ganhos Totais: 25164.25, Perdas Totais: -20996.25\n",
      "Episode 59/100, Total Reward: 4173.75, Win Rate: 0.59, Wins: 325, Losses: 227, Steps: 36754, Time: 408.32s\n",
      "Ações: Manter=451, Comprar=35915, Vender=388\n",
      "Ganhos Totais: 24819.00, Perdas Totais: -20506.75\n",
      "Episode 60/100, Total Reward: 4247.75, Win Rate: 0.58, Wins: 334, Losses: 243, Steps: 36754, Time: 411.44s\n",
      "Ações: Manter=24, Comprar=36263, Vender=467\n",
      "Ganhos Totais: 24979.50, Perdas Totais: -20586.75\n",
      "Episode 61/100, Total Reward: 4183.75, Win Rate: 0.58, Wins: 327, Losses: 241, Steps: 36754, Time: 402.49s\n",
      "Ações: Manter=41, Comprar=36258, Vender=455\n",
      "Ganhos Totais: 24976.75, Perdas Totais: -20650.50\n",
      "Episode 62/100, Total Reward: 4131.50, Win Rate: 0.59, Wins: 330, Losses: 233, Steps: 36754, Time: 404.19s\n",
      "Ações: Manter=28, Comprar=36364, Vender=362\n",
      "Ganhos Totais: 24807.75, Perdas Totais: -20534.75\n",
      "Episode 63/100, Total Reward: 4230.75, Win Rate: 0.60, Wins: 339, Losses: 230, Steps: 36754, Time: 430.40s\n",
      "Ações: Manter=555, Comprar=35775, Vender=424\n",
      "Ganhos Totais: 25019.50, Perdas Totais: -20646.00\n",
      "Episode 64/100, Total Reward: 3971.75, Win Rate: 0.59, Wins: 329, Losses: 233, Steps: 36754, Time: 422.47s\n",
      "Ações: Manter=536, Comprar=35706, Vender=512\n",
      "Ganhos Totais: 24942.50, Perdas Totais: -20829.75\n",
      "Episode 65/100, Total Reward: 4204.50, Win Rate: 0.59, Wins: 340, Losses: 235, Steps: 36754, Time: 424.44s\n",
      "Ações: Manter=19, Comprar=36076, Vender=659\n",
      "Ganhos Totais: 24994.75, Perdas Totais: -20646.00\n",
      "Episode 66/100, Total Reward: 4040.50, Win Rate: 0.58, Wins: 328, Losses: 233, Steps: 36754, Time: 420.14s\n",
      "Ações: Manter=169, Comprar=35898, Vender=687\n",
      "Ganhos Totais: 24856.00, Perdas Totais: -20674.75\n",
      "Episode 67/100, Total Reward: 4206.25, Win Rate: 0.59, Wins: 317, Losses: 223, Steps: 36754, Time: 443.80s\n",
      "Ações: Manter=8, Comprar=36091, Vender=655\n",
      "Ganhos Totais: 24931.75, Perdas Totais: -20590.00\n",
      "Episode 68/100, Total Reward: 4265.25, Win Rate: 0.59, Wins: 325, Losses: 225, Steps: 36754, Time: 409.33s\n",
      "Ações: Manter=2, Comprar=36138, Vender=614\n",
      "Ganhos Totais: 24878.00, Perdas Totais: -20474.75\n",
      "Episode 69/100, Total Reward: 4325.50, Win Rate: 0.59, Wins: 323, Losses: 222, Steps: 36754, Time: 412.08s\n",
      "Ações: Manter=4, Comprar=36162, Vender=588\n",
      "Ganhos Totais: 25020.00, Perdas Totais: -20557.75\n",
      "Episode 70/100, Total Reward: 4260.00, Win Rate: 0.59, Wins: 332, Losses: 226, Steps: 36754, Time: 425.62s\n",
      "Ações: Manter=8, Comprar=36040, Vender=706\n",
      "Ganhos Totais: 24951.50, Perdas Totais: -20551.50\n",
      "Episode 71/100, Total Reward: 4198.00, Win Rate: 0.58, Wins: 313, Losses: 223, Steps: 36754, Time: 426.90s\n",
      "Ações: Manter=2, Comprar=36074, Vender=678\n",
      "Ganhos Totais: 24876.00, Perdas Totais: -20544.00\n",
      "Episode 72/100, Total Reward: 4194.50, Win Rate: 0.59, Wins: 320, Losses: 227, Steps: 36754, Time: 430.46s\n",
      "Ações: Manter=2, Comprar=36078, Vender=674\n",
      "Ganhos Totais: 24922.75, Perdas Totais: -20591.00\n",
      "Episode 73/100, Total Reward: 4339.25, Win Rate: 0.58, Wins: 319, Losses: 227, Steps: 36754, Time: 427.98s\n",
      "Ações: Manter=2, Comprar=36086, Vender=666\n",
      "Ganhos Totais: 24917.00, Perdas Totais: -20441.00\n",
      "Episode 74/100, Total Reward: 4212.25, Win Rate: 0.59, Wins: 322, Losses: 226, Steps: 36754, Time: 426.98s\n",
      "Ações: Manter=8, Comprar=36082, Vender=664\n",
      "Ganhos Totais: 24978.50, Perdas Totais: -20628.75\n",
      "Episode 75/100, Total Reward: 4047.00, Win Rate: 0.59, Wins: 335, Losses: 231, Steps: 36754, Time: 427.02s\n",
      "Ações: Manter=22, Comprar=36078, Vender=654\n",
      "Ganhos Totais: 25050.75, Perdas Totais: -20861.75\n",
      "Episode 76/100, Total Reward: 4042.25, Win Rate: 0.59, Wins: 329, Losses: 231, Steps: 36754, Time: 429.66s\n",
      "Ações: Manter=20, Comprar=36149, Vender=585\n",
      "Ganhos Totais: 25001.25, Perdas Totais: -20818.50\n",
      "Episode 77/100, Total Reward: 3950.50, Win Rate: 0.59, Wins: 336, Losses: 236, Steps: 36754, Time: 526.46s\n",
      "Ações: Manter=17, Comprar=36180, Vender=557\n",
      "Ganhos Totais: 24988.00, Perdas Totais: -20894.00\n",
      "Episode 78/100, Total Reward: 3842.25, Win Rate: 0.58, Wins: 345, Losses: 248, Steps: 36754, Time: 525.72s\n",
      "Ações: Manter=12, Comprar=36396, Vender=346\n",
      "Ganhos Totais: 24991.25, Perdas Totais: -21000.25\n",
      "Episode 79/100, Total Reward: 3889.25, Win Rate: 0.57, Wins: 331, Losses: 246, Steps: 36754, Time: 522.07s\n",
      "Ações: Manter=14, Comprar=36487, Vender=253\n",
      "Ganhos Totais: 24963.50, Perdas Totais: -20929.50\n",
      "Episode 80/100, Total Reward: 4070.25, Win Rate: 0.58, Wins: 333, Losses: 238, Steps: 36754, Time: 493.32s\n",
      "Ações: Manter=7, Comprar=36543, Vender=204\n",
      "Ganhos Totais: 25026.75, Perdas Totais: -20813.25\n",
      "Episode 81/100, Total Reward: 4039.75, Win Rate: 0.57, Wins: 333, Losses: 248, Steps: 36754, Time: 443.65s\n",
      "Ações: Manter=6, Comprar=36537, Vender=211\n",
      "Ganhos Totais: 25087.25, Perdas Totais: -20901.75\n",
      "Episode 82/100, Total Reward: 4037.50, Win Rate: 0.59, Wins: 327, Losses: 230, Steps: 36754, Time: 403.36s\n",
      "Ações: Manter=11, Comprar=36561, Vender=182\n",
      "Ganhos Totais: 24920.50, Perdas Totais: -20743.25\n",
      "Episode 83/100, Total Reward: 3895.75, Win Rate: 0.58, Wins: 342, Losses: 248, Steps: 36754, Time: 405.11s\n",
      "Ações: Manter=7, Comprar=36528, Vender=219\n",
      "Ganhos Totais: 25004.25, Perdas Totais: -20960.50\n",
      "Episode 84/100, Total Reward: 4379.00, Win Rate: 0.58, Wins: 342, Losses: 251, Steps: 36754, Time: 399.65s\n",
      "Ações: Manter=3, Comprar=36525, Vender=226\n",
      "Ganhos Totais: 25169.25, Perdas Totais: -20641.50\n",
      "Episode 85/100, Total Reward: 4682.75, Win Rate: 0.58, Wins: 342, Losses: 245, Steps: 36754, Time: 394.71s\n",
      "Ações: Manter=2, Comprar=36526, Vender=226\n",
      "Ganhos Totais: 25326.50, Perdas Totais: -20496.50\n",
      "Episode 86/100, Total Reward: 4864.75, Win Rate: 0.59, Wins: 353, Losses: 242, Steps: 36754, Time: 385.72s\n",
      "Ações: Manter=1, Comprar=36508, Vender=245\n",
      "Ganhos Totais: 25489.00, Perdas Totais: -20475.00\n",
      "Episode 87/100, Total Reward: 4299.75, Win Rate: 0.57, Wins: 358, Losses: 274, Steps: 36754, Time: 385.11s\n",
      "Ações: Manter=0, Comprar=36458, Vender=296\n",
      "Ganhos Totais: 25430.00, Perdas Totais: -20972.00\n",
      "Episode 88/100, Total Reward: 4640.50, Win Rate: 0.57, Wins: 356, Losses: 273, Steps: 36754, Time: 385.24s\n",
      "Ações: Manter=2, Comprar=36454, Vender=298\n",
      "Ganhos Totais: 25524.75, Perdas Totais: -20726.75\n",
      "Episode 89/100, Total Reward: 4467.75, Win Rate: 0.58, Wins: 336, Losses: 239, Steps: 36754, Time: 384.44s\n",
      "Ações: Manter=7, Comprar=36512, Vender=235\n",
      "Ganhos Totais: 25271.75, Perdas Totais: -20659.75\n",
      "Episode 90/100, Total Reward: 4457.75, Win Rate: 0.58, Wins: 352, Losses: 260, Steps: 36754, Time: 384.99s\n",
      "Ações: Manter=0, Comprar=36516, Vender=238\n",
      "Ganhos Totais: 25310.75, Perdas Totais: -20699.75\n",
      "Episode 91/100, Total Reward: 4379.50, Win Rate: 0.59, Wins: 356, Losses: 252, Steps: 36754, Time: 386.30s\n",
      "Ações: Manter=1, Comprar=36491, Vender=262\n",
      "Ganhos Totais: 25353.25, Perdas Totais: -20821.25\n",
      "Episode 92/100, Total Reward: 4272.25, Win Rate: 0.56, Wins: 357, Losses: 276, Steps: 36754, Time: 385.69s\n",
      "Ações: Manter=2, Comprar=36498, Vender=254\n",
      "Ganhos Totais: 25420.75, Perdas Totais: -20990.00\n",
      "Episode 93/100, Total Reward: 4114.00, Win Rate: 0.56, Wins: 360, Losses: 288, Steps: 36754, Time: 385.17s\n",
      "Ações: Manter=1, Comprar=36502, Vender=251\n",
      "Ganhos Totais: 25466.00, Perdas Totais: -21189.75\n",
      "Episode 94/100, Total Reward: 3905.75, Win Rate: 0.55, Wins: 350, Losses: 282, Steps: 36754, Time: 386.60s\n",
      "Ações: Manter=0, Comprar=36509, Vender=245\n",
      "Ganhos Totais: 25232.50, Perdas Totais: -21168.50\n",
      "Episode 95/100, Total Reward: 4048.00, Win Rate: 0.57, Wins: 369, Losses: 282, Steps: 36754, Time: 386.40s\n",
      "Ações: Manter=6, Comprar=36469, Vender=279\n",
      "Ganhos Totais: 25294.00, Perdas Totais: -21082.75\n",
      "Episode 96/100, Total Reward: 4359.75, Win Rate: 0.59, Wins: 345, Losses: 240, Steps: 36754, Time: 385.86s\n",
      "Ações: Manter=4, Comprar=36570, Vender=180\n",
      "Ganhos Totais: 25199.75, Perdas Totais: -20693.25\n",
      "Episode 97/100, Total Reward: 4274.75, Win Rate: 0.57, Wins: 363, Losses: 275, Steps: 36754, Time: 390.01s\n",
      "Ações: Manter=2, Comprar=36506, Vender=246\n",
      "Ganhos Totais: 25542.75, Perdas Totais: -21108.25\n",
      "Episode 98/100, Total Reward: 4500.50, Win Rate: 0.58, Wins: 339, Losses: 249, Steps: 36754, Time: 390.19s\n",
      "Ações: Manter=4, Comprar=36521, Vender=229\n",
      "Ganhos Totais: 25363.75, Perdas Totais: -20715.75\n",
      "Episode 99/100, Total Reward: 4309.25, Win Rate: 0.57, Wins: 355, Losses: 271, Steps: 36754, Time: 386.69s\n",
      "Ações: Manter=3, Comprar=36472, Vender=279\n",
      "Ganhos Totais: 25358.00, Perdas Totais: -20892.00\n",
      "Episode 100/100, Total Reward: 4221.00, Win Rate: 0.59, Wins: 368, Losses: 256, Steps: 36754, Time: 388.66s\n",
      "Ações: Manter=50, Comprar=36433, Vender=271\n",
      "Ganhos Totais: 25231.00, Perdas Totais: -20853.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 25, Total Reward: 7326.50, Win Rate: 0.54, Wins: 944, Losses: 805, Ações: {0: 7492, 1: 22458, 2: 6804}, Steps: 36754, Time: 372.92s\n",
      "Rank 2: Episode 34, Total Reward: 7183.25, Win Rate: 0.59, Wins: 1027, Losses: 715, Ações: {0: 11207, 1: 18113, 2: 7434}, Steps: 36754, Time: 373.22s\n",
      "Rank 3: Episode 27, Total Reward: 6976.00, Win Rate: 0.52, Wins: 564, Losses: 525, Ações: {0: 7533, 1: 25457, 2: 3764}, Steps: 36754, Time: 373.43s\n",
      "Rank 4: Episode 12, Total Reward: 6928.00, Win Rate: 0.53, Wins: 943, Losses: 843, Ações: {0: 10287, 1: 18388, 2: 8079}, Steps: 36754, Time: 488.76s\n",
      "Rank 5: Episode 32, Total Reward: 6709.50, Win Rate: 0.55, Wins: 641, Losses: 524, Ações: {0: 8877, 1: 23632, 2: 4245}, Steps: 36754, Time: 373.60s\n",
      "Rank 6: Episode 29, Total Reward: 5826.75, Win Rate: 0.53, Wins: 781, Losses: 701, Ações: {0: 7176, 1: 25754, 2: 3824}, Steps: 36754, Time: 373.55s\n",
      "Rank 7: Episode 10, Total Reward: 5762.00, Win Rate: 0.53, Wins: 498, Losses: 449, Ações: {0: 13393, 1: 18083, 2: 5278}, Steps: 36754, Time: 485.00s\n",
      "Rank 8: Episode 33, Total Reward: 5212.25, Win Rate: 0.56, Wins: 1073, Losses: 841, Ações: {0: 9315, 1: 19216, 2: 8223}, Steps: 36754, Time: 373.80s\n",
      "Rank 9: Episode 19, Total Reward: 5180.00, Win Rate: 0.52, Wins: 719, Losses: 677, Ações: {0: 9139, 1: 23106, 2: 4509}, Steps: 36754, Time: 417.26s\n",
      "Rank 10: Episode 22, Total Reward: 5137.50, Win Rate: 0.53, Wins: 861, Losses: 756, Ações: {0: 6138, 1: 26712, 2: 3904}, Steps: 36754, Time: 388.35s\n"
     ]
    }
   ],
   "source": [
    "# Configurações do treinamento\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "memory_size = 10000\n",
    "target_update = 1000\n",
    "beta_start = 0.4\n",
    "beta_frames = num_episodes * len(data)\n",
    "alpha = 0.6\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Inicializar o Prioritized Replay Buffer\n",
    "replay_buffer = PrioritizedReplayBuffer(memory_size, alpha=alpha)\n",
    "\n",
    "# Inicializar a lista de melhores episódios\n",
    "best_episodes = []\n",
    "\n",
    "# Função para selecionar ação usando Noisy Nets\n",
    "def select_action(state):\n",
    "    with torch.no_grad():\n",
    "        q_values = q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "save_dir = \"4.18.4\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "beta = beta_start\n",
    "frame_idx = 0  # Contador de frames para ajustar beta\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        replay_buffer.push(obs, action, reward, obs_next, done)\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Resetar ruído das Noisy Nets\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(replay_buffer.buffer) >= batch_size:\n",
    "            beta = min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "            states, actions_batch, rewards_batch, next_states, dones, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando Double DQN\n",
    "            with torch.no_grad():\n",
    "                next_actions = q_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q_values = target_net(next_states).gather(1, next_actions)\n",
    "                target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular o erro para Prioritized Replay\n",
    "            td_errors = (q_values - target_q_values).detach().cpu().numpy().flatten()\n",
    "            new_priorities = np.abs(td_errors) + 1e-6\n",
    "            replay_buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "            # Calcular a perda ponderada\n",
    "            loss = (weights * nn.functional.mse_loss(q_values, target_q_values, reduction='none')).mean()\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Atualizar a rede alvo\n",
    "            if frame_idx % target_update == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista dos melhores e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log do episódio se for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "# Exibir os top 10 episódios ao final do treinamento\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
