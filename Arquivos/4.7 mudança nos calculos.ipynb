{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -5962.00, Win Rate: 0.50, Wins: 1338, Losses: 1314, Epsilon: 0.4950, Steps: 36754, Time: 131.52s\n",
      "Ações: Manter=11341, Comprar=12814, Vender=12599\n",
      "Ganhos Totais: 34658.00, Perdas Totais: -40620.00\n",
      "Modelo e log do episódio 1 salvos em: 4.7\\model_episode_1.pth e 4.7\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -837.50, Win Rate: 0.51, Wins: 1356, Losses: 1302, Epsilon: 0.4900, Steps: 36754, Time: 136.59s\n",
      "Ações: Manter=11296, Comprar=12946, Vender=12512\n",
      "Ganhos Totais: 35737.00, Perdas Totais: -36574.50\n",
      "Modelo e log do episódio 2 salvos em: 4.7\\model_episode_2.pth e 4.7\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -3130.75, Win Rate: 0.51, Wins: 1456, Losses: 1376, Epsilon: 0.4851, Steps: 36754, Time: 134.57s\n",
      "Ações: Manter=10572, Comprar=13113, Vender=13069\n",
      "Ganhos Totais: 36815.75, Perdas Totais: -39946.50\n",
      "Modelo e log do episódio 3 salvos em: 4.7\\model_episode_3.pth e 4.7\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 1308.75, Win Rate: 0.53, Wins: 1460, Losses: 1292, Epsilon: 0.4803, Steps: 36754, Time: 118.53s\n",
      "Ações: Manter=11847, Comprar=12569, Vender=12338\n",
      "Ganhos Totais: 36956.75, Perdas Totais: -35648.00\n",
      "Modelo e log do episódio 4 salvos em: 4.7\\model_episode_4.pth e 4.7\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -8.00, Win Rate: 0.53, Wins: 1486, Losses: 1339, Epsilon: 0.4755, Steps: 36754, Time: 169.75s\n",
      "Ações: Manter=12006, Comprar=12440, Vender=12308\n",
      "Ganhos Totais: 36732.25, Perdas Totais: -36740.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7\\model_episode_5.pth e 4.7\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 263.75, Win Rate: 0.53, Wins: 1478, Losses: 1321, Epsilon: 0.4707, Steps: 36754, Time: 154.65s\n",
      "Ações: Manter=12283, Comprar=12558, Vender=11913\n",
      "Ganhos Totais: 37885.50, Perdas Totais: -37621.75\n",
      "Modelo e log do episódio 6 salvos em: 4.7\\model_episode_6.pth e 4.7\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -1360.00, Win Rate: 0.53, Wins: 1491, Losses: 1338, Epsilon: 0.4660, Steps: 36754, Time: 160.91s\n",
      "Ações: Manter=12508, Comprar=12570, Vender=11676\n",
      "Ganhos Totais: 36795.00, Perdas Totais: -38155.00\n",
      "Modelo e log do episódio 7 salvos em: 4.7\\model_episode_7.pth e 4.7\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 2861.00, Win Rate: 0.55, Wins: 1615, Losses: 1337, Epsilon: 0.4614, Steps: 36754, Time: 163.23s\n",
      "Ações: Manter=12009, Comprar=12505, Vender=12240\n",
      "Ganhos Totais: 39848.75, Perdas Totais: -36987.75\n",
      "Modelo e log do episódio 8 salvos em: 4.7\\model_episode_8.pth e 4.7\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -1442.25, Win Rate: 0.53, Wins: 1472, Losses: 1323, Epsilon: 0.4568, Steps: 36754, Time: 162.67s\n",
      "Ações: Manter=11911, Comprar=13145, Vender=11698\n",
      "Ganhos Totais: 36777.00, Perdas Totais: -38219.25\n",
      "Modelo e log do episódio 9 salvos em: 4.7\\model_episode_9.pth e 4.7\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -1931.50, Win Rate: 0.53, Wins: 1485, Losses: 1297, Epsilon: 0.4522, Steps: 36754, Time: 165.78s\n",
      "Ações: Manter=11806, Comprar=12294, Vender=12654\n",
      "Ganhos Totais: 37143.50, Perdas Totais: -39075.00\n",
      "Modelo e log do episódio 10 salvos em: 4.7\\model_episode_10.pth e 4.7\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -2981.75, Win Rate: 0.52, Wins: 1376, Losses: 1255, Epsilon: 0.4477, Steps: 36754, Time: 158.25s\n",
      "Ações: Manter=13106, Comprar=11240, Vender=12408\n",
      "Ganhos Totais: 34425.00, Perdas Totais: -37406.75\n",
      "Modelo e log do episódio 11 salvos em: 4.7\\model_episode_11.pth e 4.7\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 1362.25, Win Rate: 0.54, Wins: 1509, Losses: 1285, Epsilon: 0.4432, Steps: 36754, Time: 158.55s\n",
      "Ações: Manter=11979, Comprar=12807, Vender=11968\n",
      "Ganhos Totais: 38209.75, Perdas Totais: -36847.50\n",
      "Modelo e log do episódio 12 salvos em: 4.7\\model_episode_12.pth e 4.7\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 1015.50, Win Rate: 0.55, Wins: 1466, Losses: 1215, Epsilon: 0.4388, Steps: 36754, Time: 155.73s\n",
      "Ações: Manter=13079, Comprar=12324, Vender=11351\n",
      "Ganhos Totais: 37720.00, Perdas Totais: -36704.50\n",
      "Modelo e log do episódio 13 salvos em: 4.7\\model_episode_13.pth e 4.7\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 3473.25, Win Rate: 0.55, Wins: 1559, Losses: 1289, Epsilon: 0.4344, Steps: 36754, Time: 157.01s\n",
      "Ações: Manter=11836, Comprar=11486, Vender=13432\n",
      "Ganhos Totais: 39204.25, Perdas Totais: -35731.00\n",
      "Modelo e log do episódio 14 salvos em: 4.7\\model_episode_14.pth e 4.7\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -3129.00, Win Rate: 0.52, Wins: 1502, Losses: 1395, Epsilon: 0.4300, Steps: 36754, Time: 157.86s\n",
      "Ações: Manter=12651, Comprar=11510, Vender=12593\n",
      "Ganhos Totais: 35176.75, Perdas Totais: -38305.75\n",
      "Episode 16/100, Total Reward: 1440.00, Win Rate: 0.53, Wins: 1538, Losses: 1355, Epsilon: 0.4257, Steps: 36754, Time: 157.91s\n",
      "Ações: Manter=12800, Comprar=11626, Vender=12328\n",
      "Ganhos Totais: 38268.25, Perdas Totais: -36828.25\n",
      "Modelo e log do episódio 16 salvos em: 4.7\\model_episode_16.pth e 4.7\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -801.25, Win Rate: 0.54, Wins: 1487, Losses: 1277, Epsilon: 0.4215, Steps: 36754, Time: 158.89s\n",
      "Ações: Manter=12634, Comprar=11844, Vender=12276\n",
      "Ganhos Totais: 36810.50, Perdas Totais: -37611.75\n",
      "Modelo e log do episódio 17 salvos em: 4.7\\model_episode_17.pth e 4.7\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -3406.00, Win Rate: 0.53, Wins: 1447, Losses: 1285, Epsilon: 0.4173, Steps: 36754, Time: 158.06s\n",
      "Ações: Manter=12058, Comprar=11389, Vender=13307\n",
      "Ganhos Totais: 35796.50, Perdas Totais: -39202.50\n",
      "Episode 19/100, Total Reward: -3845.25, Win Rate: 0.52, Wins: 1359, Losses: 1246, Epsilon: 0.4131, Steps: 36754, Time: 160.25s\n",
      "Ações: Manter=12257, Comprar=11339, Vender=13158\n",
      "Ganhos Totais: 35038.25, Perdas Totais: -38883.50\n",
      "Episode 20/100, Total Reward: 5611.25, Win Rate: 0.53, Wins: 1380, Losses: 1227, Epsilon: 0.4090, Steps: 36754, Time: 145.34s\n",
      "Ações: Manter=12601, Comprar=12942, Vender=11211\n",
      "Ganhos Totais: 40838.75, Perdas Totais: -35227.50\n",
      "Modelo e log do episódio 20 salvos em: 4.7\\model_episode_20.pth e 4.7\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -131.50, Win Rate: 0.52, Wins: 1459, Losses: 1349, Epsilon: 0.4049, Steps: 36754, Time: 118.19s\n",
      "Ações: Manter=13391, Comprar=12014, Vender=11349\n",
      "Ganhos Totais: 37172.75, Perdas Totais: -37304.25\n",
      "Modelo e log do episódio 21 salvos em: 4.7\\model_episode_21.pth e 4.7\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -744.00, Win Rate: 0.54, Wins: 1458, Losses: 1245, Epsilon: 0.4008, Steps: 36754, Time: 114.78s\n",
      "Ações: Manter=12664, Comprar=11994, Vender=12096\n",
      "Ganhos Totais: 36162.25, Perdas Totais: -36906.25\n",
      "Episode 23/100, Total Reward: -1771.75, Win Rate: 0.52, Wins: 1289, Losses: 1192, Epsilon: 0.3968, Steps: 36754, Time: 116.68s\n",
      "Ações: Manter=13505, Comprar=13015, Vender=10234\n",
      "Ganhos Totais: 35748.75, Perdas Totais: -37520.50\n",
      "Episode 24/100, Total Reward: 881.00, Win Rate: 0.55, Wins: 1416, Losses: 1171, Epsilon: 0.3928, Steps: 36754, Time: 116.49s\n",
      "Ações: Manter=13962, Comprar=11698, Vender=11094\n",
      "Ganhos Totais: 36091.25, Perdas Totais: -35210.25\n",
      "Modelo e log do episódio 24 salvos em: 4.7\\model_episode_24.pth e 4.7\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -1490.50, Win Rate: 0.53, Wins: 1318, Losses: 1148, Epsilon: 0.3889, Steps: 36754, Time: 118.12s\n",
      "Ações: Manter=13815, Comprar=10876, Vender=12063\n",
      "Ganhos Totais: 36263.50, Perdas Totais: -37754.00\n",
      "Episode 26/100, Total Reward: -2317.00, Win Rate: 0.53, Wins: 1292, Losses: 1131, Epsilon: 0.3850, Steps: 36754, Time: 128.05s\n",
      "Ações: Manter=13289, Comprar=10566, Vender=12899\n",
      "Ganhos Totais: 35472.50, Perdas Totais: -37789.50\n",
      "Episode 27/100, Total Reward: -2988.00, Win Rate: 0.53, Wins: 1233, Losses: 1097, Epsilon: 0.3812, Steps: 36754, Time: 122.30s\n",
      "Ações: Manter=13347, Comprar=10435, Vender=12972\n",
      "Ganhos Totais: 34745.00, Perdas Totais: -37733.00\n",
      "Episode 28/100, Total Reward: -3385.50, Win Rate: 0.53, Wins: 1227, Losses: 1109, Epsilon: 0.3774, Steps: 36754, Time: 128.39s\n",
      "Ações: Manter=14474, Comprar=10874, Vender=11406\n",
      "Ganhos Totais: 34317.50, Perdas Totais: -37703.00\n",
      "Episode 29/100, Total Reward: -4230.50, Win Rate: 0.51, Wins: 1097, Losses: 1060, Epsilon: 0.3736, Steps: 36754, Time: 142.61s\n",
      "Ações: Manter=12633, Comprar=12046, Vender=12075\n",
      "Ganhos Totais: 32886.00, Perdas Totais: -37116.50\n",
      "Episode 30/100, Total Reward: -3746.25, Win Rate: 0.53, Wins: 1172, Losses: 1027, Epsilon: 0.3699, Steps: 36754, Time: 142.05s\n",
      "Ações: Manter=13268, Comprar=11032, Vender=12454\n",
      "Ganhos Totais: 34160.50, Perdas Totais: -37906.75\n",
      "Episode 31/100, Total Reward: 917.25, Win Rate: 0.55, Wins: 1348, Losses: 1081, Epsilon: 0.3662, Steps: 36754, Time: 142.11s\n",
      "Ações: Manter=13590, Comprar=10864, Vender=12300\n",
      "Ganhos Totais: 36809.00, Perdas Totais: -35891.75\n",
      "Modelo e log do episódio 31 salvos em: 4.7\\model_episode_31.pth e 4.7\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 693.50, Win Rate: 0.55, Wins: 1309, Losses: 1082, Epsilon: 0.3625, Steps: 36754, Time: 144.26s\n",
      "Ações: Manter=13191, Comprar=10456, Vender=13107\n",
      "Ganhos Totais: 36106.50, Perdas Totais: -35413.00\n",
      "Modelo e log do episódio 32 salvos em: 4.7\\model_episode_32.pth e 4.7\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: -1998.50, Win Rate: 0.55, Wins: 1285, Losses: 1047, Epsilon: 0.3589, Steps: 36754, Time: 131.15s\n",
      "Ações: Manter=14456, Comprar=11263, Vender=11035\n",
      "Ganhos Totais: 35230.25, Perdas Totais: -37228.75\n",
      "Episode 34/100, Total Reward: -2043.00, Win Rate: 0.54, Wins: 1236, Losses: 1058, Epsilon: 0.3553, Steps: 36754, Time: 137.37s\n",
      "Ações: Manter=14957, Comprar=10832, Vender=10965\n",
      "Ganhos Totais: 33328.00, Perdas Totais: -35371.00\n",
      "Episode 35/100, Total Reward: -2085.50, Win Rate: 0.53, Wins: 1233, Losses: 1103, Epsilon: 0.3517, Steps: 36754, Time: 124.90s\n",
      "Ações: Manter=13677, Comprar=10260, Vender=12817\n",
      "Ganhos Totais: 34175.00, Perdas Totais: -36260.50\n",
      "Episode 36/100, Total Reward: -807.50, Win Rate: 0.54, Wins: 1311, Losses: 1107, Epsilon: 0.3482, Steps: 36754, Time: 114.72s\n",
      "Ações: Manter=14596, Comprar=10715, Vender=11443\n",
      "Ganhos Totais: 34742.25, Perdas Totais: -35549.75\n",
      "Episode 37/100, Total Reward: -3637.25, Win Rate: 0.55, Wins: 1261, Losses: 1043, Epsilon: 0.3447, Steps: 36754, Time: 114.89s\n",
      "Ações: Manter=14668, Comprar=12016, Vender=10070\n",
      "Ganhos Totais: 32765.00, Perdas Totais: -36402.25\n",
      "Episode 38/100, Total Reward: -2277.50, Win Rate: 0.53, Wins: 1125, Losses: 982, Epsilon: 0.3413, Steps: 36754, Time: 113.74s\n",
      "Ações: Manter=14863, Comprar=11269, Vender=10622\n",
      "Ganhos Totais: 33078.50, Perdas Totais: -35356.00\n",
      "Episode 39/100, Total Reward: -2789.50, Win Rate: 0.54, Wins: 1209, Losses: 1025, Epsilon: 0.3379, Steps: 36754, Time: 114.44s\n",
      "Ações: Manter=15493, Comprar=10476, Vender=10785\n",
      "Ganhos Totais: 32642.50, Perdas Totais: -35432.00\n",
      "Episode 40/100, Total Reward: -5441.75, Win Rate: 0.52, Wins: 1086, Losses: 1011, Epsilon: 0.3345, Steps: 36754, Time: 114.69s\n",
      "Ações: Manter=15702, Comprar=9938, Vender=11114\n",
      "Ganhos Totais: 29623.25, Perdas Totais: -35065.00\n",
      "Episode 41/100, Total Reward: -3189.00, Win Rate: 0.53, Wins: 1107, Losses: 990, Epsilon: 0.3311, Steps: 36754, Time: 113.96s\n",
      "Ações: Manter=14992, Comprar=9871, Vender=11891\n",
      "Ganhos Totais: 31257.50, Perdas Totais: -34446.50\n",
      "Episode 42/100, Total Reward: -847.00, Win Rate: 0.54, Wins: 1258, Losses: 1055, Epsilon: 0.3278, Steps: 36754, Time: 114.96s\n",
      "Ações: Manter=13322, Comprar=10812, Vender=12620\n",
      "Ganhos Totais: 34770.75, Perdas Totais: -35617.75\n",
      "Episode 43/100, Total Reward: 541.00, Win Rate: 0.56, Wins: 1178, Losses: 937, Epsilon: 0.3246, Steps: 36754, Time: 114.43s\n",
      "Ações: Manter=15862, Comprar=9583, Vender=11309\n",
      "Ganhos Totais: 33270.00, Perdas Totais: -32729.00\n",
      "Episode 44/100, Total Reward: -4003.75, Win Rate: 0.54, Wins: 1220, Losses: 1039, Epsilon: 0.3213, Steps: 36754, Time: 114.26s\n",
      "Ações: Manter=15218, Comprar=10526, Vender=11010\n",
      "Ganhos Totais: 32679.50, Perdas Totais: -36683.25\n",
      "Episode 45/100, Total Reward: 227.25, Win Rate: 0.56, Wins: 1220, Losses: 962, Epsilon: 0.3181, Steps: 36754, Time: 114.66s\n",
      "Ações: Manter=16310, Comprar=9332, Vender=11112\n",
      "Ganhos Totais: 34720.00, Perdas Totais: -34492.75\n",
      "Episode 46/100, Total Reward: -3260.25, Win Rate: 0.53, Wins: 1232, Losses: 1090, Epsilon: 0.3149, Steps: 36754, Time: 114.66s\n",
      "Ações: Manter=13604, Comprar=10554, Vender=12596\n",
      "Ganhos Totais: 33245.50, Perdas Totais: -36505.75\n",
      "Episode 47/100, Total Reward: -3853.00, Win Rate: 0.54, Wins: 1210, Losses: 1011, Epsilon: 0.3118, Steps: 36754, Time: 114.48s\n",
      "Ações: Manter=12945, Comprar=11900, Vender=11909\n",
      "Ganhos Totais: 33947.00, Perdas Totais: -37800.00\n",
      "Episode 48/100, Total Reward: -5576.00, Win Rate: 0.54, Wins: 1180, Losses: 1008, Epsilon: 0.3086, Steps: 36754, Time: 113.94s\n",
      "Ações: Manter=13178, Comprar=10897, Vender=12679\n",
      "Ganhos Totais: 31771.75, Perdas Totais: -37347.75\n",
      "Episode 49/100, Total Reward: -5046.00, Win Rate: 0.54, Wins: 1297, Losses: 1117, Epsilon: 0.3056, Steps: 36754, Time: 115.21s\n",
      "Ações: Manter=10880, Comprar=12383, Vender=13491\n",
      "Ganhos Totais: 32960.50, Perdas Totais: -38006.50\n",
      "Episode 50/100, Total Reward: -6456.25, Win Rate: 0.52, Wins: 1081, Losses: 995, Epsilon: 0.3025, Steps: 36754, Time: 114.21s\n",
      "Ações: Manter=16309, Comprar=10584, Vender=9861\n",
      "Ganhos Totais: 30290.00, Perdas Totais: -36746.25\n",
      "Episode 51/100, Total Reward: -4094.50, Win Rate: 0.54, Wins: 1094, Losses: 928, Epsilon: 0.2995, Steps: 36754, Time: 114.22s\n",
      "Ações: Manter=16605, Comprar=10428, Vender=9721\n",
      "Ganhos Totais: 31523.25, Perdas Totais: -35617.75\n",
      "Episode 52/100, Total Reward: 637.00, Win Rate: 0.54, Wins: 1191, Losses: 1019, Epsilon: 0.2965, Steps: 36754, Time: 114.68s\n",
      "Ações: Manter=11199, Comprar=15748, Vender=9807\n",
      "Ganhos Totais: 35359.75, Perdas Totais: -34722.75\n",
      "Episode 53/100, Total Reward: -2257.00, Win Rate: 0.54, Wins: 1101, Losses: 946, Epsilon: 0.2935, Steps: 36754, Time: 114.74s\n",
      "Ações: Manter=12370, Comprar=13881, Vender=10503\n",
      "Ganhos Totais: 32617.25, Perdas Totais: -34874.25\n",
      "Episode 54/100, Total Reward: -681.25, Win Rate: 0.55, Wins: 1221, Losses: 991, Epsilon: 0.2906, Steps: 36754, Time: 114.18s\n",
      "Ações: Manter=11719, Comprar=15459, Vender=9576\n",
      "Ganhos Totais: 34346.75, Perdas Totais: -35028.00\n",
      "Episode 55/100, Total Reward: 833.50, Win Rate: 0.56, Wins: 1240, Losses: 962, Epsilon: 0.2877, Steps: 36754, Time: 114.75s\n",
      "Ações: Manter=11442, Comprar=14828, Vender=10484\n",
      "Ganhos Totais: 35960.75, Perdas Totais: -35127.25\n",
      "Modelo e log do episódio 55 salvos em: 4.7\\model_episode_55.pth e 4.7\\log_episode_55.csv\n",
      "\n",
      "Episode 56/100, Total Reward: -2213.50, Win Rate: 0.54, Wins: 1113, Losses: 943, Epsilon: 0.2848, Steps: 36754, Time: 114.73s\n",
      "Ações: Manter=12017, Comprar=14916, Vender=9821\n",
      "Ganhos Totais: 32374.75, Perdas Totais: -34588.25\n",
      "Episode 57/100, Total Reward: -2122.00, Win Rate: 0.56, Wins: 1195, Losses: 958, Epsilon: 0.2820, Steps: 36754, Time: 114.21s\n",
      "Ações: Manter=12289, Comprar=13513, Vender=10952\n",
      "Ganhos Totais: 33824.00, Perdas Totais: -35946.00\n",
      "Episode 58/100, Total Reward: 2500.25, Win Rate: 0.57, Wins: 1197, Losses: 909, Epsilon: 0.2791, Steps: 36754, Time: 114.34s\n",
      "Ações: Manter=10572, Comprar=15524, Vender=10658\n",
      "Ganhos Totais: 36702.00, Perdas Totais: -34201.75\n",
      "Modelo e log do episódio 58 salvos em: 4.7\\model_episode_58.pth e 4.7\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: -982.50, Win Rate: 0.56, Wins: 1234, Losses: 967, Epsilon: 0.2763, Steps: 36754, Time: 114.93s\n",
      "Ações: Manter=11204, Comprar=14571, Vender=10979\n",
      "Ganhos Totais: 35194.00, Perdas Totais: -36176.50\n",
      "Episode 60/100, Total Reward: -1346.00, Win Rate: 0.56, Wins: 1184, Losses: 926, Epsilon: 0.2736, Steps: 36754, Time: 114.60s\n",
      "Ações: Manter=12525, Comprar=12889, Vender=11340\n",
      "Ganhos Totais: 33426.50, Perdas Totais: -34772.50\n",
      "Episode 61/100, Total Reward: -3531.00, Win Rate: 0.53, Wins: 1007, Losses: 893, Epsilon: 0.2708, Steps: 36754, Time: 114.57s\n",
      "Ações: Manter=13098, Comprar=11792, Vender=11864\n",
      "Ganhos Totais: 30647.75, Perdas Totais: -34178.75\n",
      "Episode 62/100, Total Reward: 2729.75, Win Rate: 0.57, Wins: 1200, Losses: 891, Epsilon: 0.2681, Steps: 36754, Time: 114.99s\n",
      "Ações: Manter=9195, Comprar=18095, Vender=9464\n",
      "Ganhos Totais: 35971.75, Perdas Totais: -33242.00\n",
      "Modelo e log do episódio 62 salvos em: 4.7\\model_episode_62.pth e 4.7\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: 236.75, Win Rate: 0.56, Wins: 1167, Losses: 927, Epsilon: 0.2655, Steps: 36754, Time: 115.06s\n",
      "Ações: Manter=11059, Comprar=16109, Vender=9586\n",
      "Ganhos Totais: 35070.25, Perdas Totais: -34833.50\n",
      "Episode 64/100, Total Reward: -3782.75, Win Rate: 0.54, Wins: 1015, Losses: 869, Epsilon: 0.2628, Steps: 36754, Time: 115.06s\n",
      "Ações: Manter=12896, Comprar=14205, Vender=9653\n",
      "Ganhos Totais: 30724.75, Perdas Totais: -34507.50\n",
      "Episode 65/100, Total Reward: 5427.00, Win Rate: 0.58, Wins: 1214, Losses: 886, Epsilon: 0.2602, Steps: 36754, Time: 115.21s\n",
      "Ações: Manter=11457, Comprar=14610, Vender=10687\n",
      "Ganhos Totais: 37832.00, Perdas Totais: -32405.00\n",
      "Modelo e log do episódio 65 salvos em: 4.7\\model_episode_65.pth e 4.7\\log_episode_65.csv\n",
      "\n",
      "Episode 66/100, Total Reward: 4829.25, Win Rate: 0.58, Wins: 1284, Losses: 940, Epsilon: 0.2576, Steps: 36754, Time: 115.49s\n",
      "Ações: Manter=10215, Comprar=16103, Vender=10436\n",
      "Ganhos Totais: 38032.25, Perdas Totais: -33203.00\n",
      "Modelo e log do episódio 66 salvos em: 4.7\\model_episode_66.pth e 4.7\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: 2689.75, Win Rate: 0.56, Wins: 1147, Losses: 910, Epsilon: 0.2550, Steps: 36754, Time: 114.78s\n",
      "Ações: Manter=12025, Comprar=14910, Vender=9819\n",
      "Ganhos Totais: 35182.75, Perdas Totais: -32493.00\n",
      "Modelo e log do episódio 67 salvos em: 4.7\\model_episode_67.pth e 4.7\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: 3042.50, Win Rate: 0.56, Wins: 1131, Losses: 876, Epsilon: 0.2524, Steps: 36754, Time: 115.40s\n",
      "Ações: Manter=12645, Comprar=14634, Vender=9475\n",
      "Ganhos Totais: 34429.00, Perdas Totais: -31386.50\n",
      "Modelo e log do episódio 68 salvos em: 4.7\\model_episode_68.pth e 4.7\\log_episode_68.csv\n",
      "\n",
      "Episode 69/100, Total Reward: -659.75, Win Rate: 0.54, Wins: 1061, Losses: 895, Epsilon: 0.2499, Steps: 36754, Time: 115.46s\n",
      "Ações: Manter=11970, Comprar=13306, Vender=11478\n",
      "Ganhos Totais: 34276.25, Perdas Totais: -34936.00\n",
      "Episode 70/100, Total Reward: 992.75, Win Rate: 0.57, Wins: 1117, Losses: 844, Epsilon: 0.2474, Steps: 36754, Time: 115.23s\n",
      "Ações: Manter=10877, Comprar=15707, Vender=10170\n",
      "Ganhos Totais: 34418.50, Perdas Totais: -33425.75\n",
      "Episode 71/100, Total Reward: -483.00, Win Rate: 0.55, Wins: 1092, Losses: 886, Epsilon: 0.2449, Steps: 36754, Time: 115.89s\n",
      "Ações: Manter=11607, Comprar=14872, Vender=10275\n",
      "Ganhos Totais: 33655.75, Perdas Totais: -34138.75\n",
      "Episode 72/100, Total Reward: 5589.50, Win Rate: 0.58, Wins: 1030, Losses: 761, Epsilon: 0.2425, Steps: 36754, Time: 115.85s\n",
      "Ações: Manter=10952, Comprar=15715, Vender=10087\n",
      "Ganhos Totais: 35893.00, Perdas Totais: -30303.50\n",
      "Modelo e log do episódio 72 salvos em: 4.7\\model_episode_72.pth e 4.7\\log_episode_72.csv\n",
      "\n",
      "Episode 73/100, Total Reward: 3995.25, Win Rate: 0.57, Wins: 997, Losses: 755, Epsilon: 0.2401, Steps: 36754, Time: 115.51s\n",
      "Ações: Manter=11423, Comprar=14928, Vender=10403\n",
      "Ganhos Totais: 34667.25, Perdas Totais: -30672.00\n",
      "Modelo e log do episódio 73 salvos em: 4.7\\model_episode_73.pth e 4.7\\log_episode_73.csv\n",
      "\n",
      "Episode 74/100, Total Reward: -3968.25, Win Rate: 0.55, Wins: 991, Losses: 822, Epsilon: 0.2377, Steps: 36754, Time: 115.67s\n",
      "Ações: Manter=11753, Comprar=14793, Vender=10208\n",
      "Ganhos Totais: 30826.75, Perdas Totais: -34795.00\n",
      "Episode 75/100, Total Reward: 2882.00, Win Rate: 0.56, Wins: 1038, Losses: 819, Epsilon: 0.2353, Steps: 36754, Time: 115.81s\n",
      "Ações: Manter=11306, Comprar=12977, Vender=12471\n",
      "Ganhos Totais: 35478.75, Perdas Totais: -32596.75\n",
      "Modelo e log do episódio 75 salvos em: 4.7\\model_episode_75.pth e 4.7\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: -1853.25, Win Rate: 0.52, Wins: 940, Losses: 873, Epsilon: 0.2329, Steps: 36754, Time: 115.67s\n",
      "Ações: Manter=11697, Comprar=14008, Vender=11049\n",
      "Ganhos Totais: 31975.25, Perdas Totais: -33828.50\n",
      "Episode 77/100, Total Reward: 524.25, Win Rate: 0.55, Wins: 1003, Losses: 807, Epsilon: 0.2306, Steps: 36754, Time: 115.14s\n",
      "Ações: Manter=10031, Comprar=15914, Vender=10809\n",
      "Ganhos Totais: 34329.50, Perdas Totais: -33805.25\n",
      "Episode 78/100, Total Reward: 1880.00, Win Rate: 0.56, Wins: 1054, Losses: 838, Epsilon: 0.2283, Steps: 36754, Time: 115.76s\n",
      "Ações: Manter=9429, Comprar=16742, Vender=10583\n",
      "Ganhos Totais: 34917.25, Perdas Totais: -33037.25\n",
      "Episode 79/100, Total Reward: 2034.75, Win Rate: 0.55, Wins: 1025, Losses: 841, Epsilon: 0.2260, Steps: 36754, Time: 116.15s\n",
      "Ações: Manter=9551, Comprar=17246, Vender=9957\n",
      "Ganhos Totais: 34293.00, Perdas Totais: -32258.25\n",
      "Episode 80/100, Total Reward: 2321.75, Win Rate: 0.54, Wins: 1038, Losses: 887, Epsilon: 0.2238, Steps: 36754, Time: 118.36s\n",
      "Ações: Manter=8322, Comprar=18987, Vender=9445\n",
      "Ganhos Totais: 35584.75, Perdas Totais: -33263.00\n",
      "Episode 81/100, Total Reward: 1847.75, Win Rate: 0.56, Wins: 1075, Losses: 835, Epsilon: 0.2215, Steps: 36754, Time: 118.72s\n",
      "Ações: Manter=8498, Comprar=18419, Vender=9837\n",
      "Ganhos Totais: 34709.50, Perdas Totais: -32861.75\n",
      "Episode 82/100, Total Reward: 6234.25, Win Rate: 0.57, Wins: 1145, Losses: 879, Epsilon: 0.2193, Steps: 36754, Time: 118.87s\n",
      "Ações: Manter=10945, Comprar=15921, Vender=9888\n",
      "Ganhos Totais: 37455.50, Perdas Totais: -31221.25\n",
      "Modelo e log do episódio 82 salvos em: 4.7\\model_episode_82.pth e 4.7\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: 2179.00, Win Rate: 0.56, Wins: 1096, Losses: 874, Epsilon: 0.2171, Steps: 36754, Time: 134.19s\n",
      "Ações: Manter=13188, Comprar=11886, Vender=11680\n",
      "Ganhos Totais: 34122.50, Perdas Totais: -31943.50\n",
      "Episode 84/100, Total Reward: 913.50, Win Rate: 0.55, Wins: 1016, Losses: 818, Epsilon: 0.2149, Steps: 36754, Time: 151.18s\n",
      "Ações: Manter=12419, Comprar=14703, Vender=9632\n",
      "Ganhos Totais: 33965.75, Perdas Totais: -33052.25\n",
      "Episode 85/100, Total Reward: 860.50, Win Rate: 0.55, Wins: 1091, Losses: 904, Epsilon: 0.2128, Steps: 36754, Time: 138.09s\n",
      "Ações: Manter=7900, Comprar=17496, Vender=11358\n",
      "Ganhos Totais: 34811.00, Perdas Totais: -33950.50\n",
      "Episode 86/100, Total Reward: 670.75, Win Rate: 0.56, Wins: 1049, Losses: 841, Epsilon: 0.2107, Steps: 36754, Time: 133.34s\n",
      "Ações: Manter=9860, Comprar=17688, Vender=9206\n",
      "Ganhos Totais: 33875.00, Perdas Totais: -33204.25\n",
      "Episode 87/100, Total Reward: 5792.25, Win Rate: 0.57, Wins: 1105, Losses: 839, Epsilon: 0.2086, Steps: 36754, Time: 137.16s\n",
      "Ações: Manter=11412, Comprar=15479, Vender=9863\n",
      "Ganhos Totais: 36577.50, Perdas Totais: -30785.25\n",
      "Modelo e log do episódio 87 salvos em: 4.7\\model_episode_87.pth e 4.7\\log_episode_87.csv\n",
      "\n",
      "Episode 88/100, Total Reward: 9210.00, Win Rate: 0.56, Wins: 1058, Losses: 815, Epsilon: 0.2065, Steps: 36754, Time: 119.31s\n",
      "Ações: Manter=9460, Comprar=17876, Vender=9418\n",
      "Ganhos Totais: 38585.25, Perdas Totais: -29375.25\n",
      "Modelo e log do episódio 88 salvos em: 4.7\\model_episode_88.pth e 4.7\\log_episode_88.csv\n",
      "\n",
      "Episode 89/100, Total Reward: 2689.75, Win Rate: 0.56, Wins: 1068, Losses: 848, Epsilon: 0.2044, Steps: 36754, Time: 116.59s\n",
      "Ações: Manter=10482, Comprar=15877, Vender=10395\n",
      "Ganhos Totais: 35566.00, Perdas Totais: -32876.25\n",
      "Episode 90/100, Total Reward: 5653.75, Win Rate: 0.58, Wins: 1108, Losses: 811, Epsilon: 0.2024, Steps: 36754, Time: 121.23s\n",
      "Ações: Manter=9402, Comprar=18709, Vender=8643\n",
      "Ganhos Totais: 37252.50, Perdas Totais: -31598.75\n",
      "Modelo e log do episódio 90 salvos em: 4.7\\model_episode_90.pth e 4.7\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: 3497.50, Win Rate: 0.55, Wins: 1108, Losses: 909, Epsilon: 0.2003, Steps: 36754, Time: 134.85s\n",
      "Ações: Manter=10549, Comprar=16575, Vender=9630\n",
      "Ganhos Totais: 36552.00, Perdas Totais: -33054.50\n",
      "Modelo e log do episódio 91 salvos em: 4.7\\model_episode_91.pth e 4.7\\log_episode_91.csv\n",
      "\n",
      "Episode 92/100, Total Reward: 9370.00, Win Rate: 0.57, Wins: 1216, Losses: 904, Epsilon: 0.1983, Steps: 36754, Time: 121.07s\n",
      "Ações: Manter=9754, Comprar=15861, Vender=11139\n",
      "Ganhos Totais: 39907.25, Perdas Totais: -30537.25\n",
      "Modelo e log do episódio 92 salvos em: 4.7\\model_episode_92.pth e 4.7\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: 5028.25, Win Rate: 0.57, Wins: 1135, Losses: 862, Epsilon: 0.1964, Steps: 36754, Time: 118.26s\n",
      "Ações: Manter=10029, Comprar=17007, Vender=9718\n",
      "Ganhos Totais: 36524.00, Perdas Totais: -31495.75\n",
      "Modelo e log do episódio 93 salvos em: 4.7\\model_episode_93.pth e 4.7\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: 3270.50, Win Rate: 0.56, Wins: 1094, Losses: 852, Epsilon: 0.1944, Steps: 36754, Time: 131.12s\n",
      "Ações: Manter=10059, Comprar=17378, Vender=9317\n",
      "Ganhos Totais: 35751.75, Perdas Totais: -32481.25\n",
      "Episode 95/100, Total Reward: -581.75, Win Rate: 0.56, Wins: 1138, Losses: 903, Epsilon: 0.1924, Steps: 36754, Time: 135.19s\n",
      "Ações: Manter=9692, Comprar=16908, Vender=10154\n",
      "Ganhos Totais: 34805.75, Perdas Totais: -35387.50\n",
      "Episode 96/100, Total Reward: 5361.50, Win Rate: 0.57, Wins: 1100, Losses: 830, Epsilon: 0.1905, Steps: 36754, Time: 133.10s\n",
      "Ações: Manter=10646, Comprar=14927, Vender=11181\n",
      "Ganhos Totais: 37114.00, Perdas Totais: -31752.50\n",
      "Modelo e log do episódio 96 salvos em: 4.7\\model_episode_96.pth e 4.7\\log_episode_96.csv\n",
      "\n",
      "Episode 97/100, Total Reward: 1201.75, Win Rate: 0.56, Wins: 1089, Losses: 843, Epsilon: 0.1886, Steps: 36754, Time: 131.11s\n",
      "Ações: Manter=11424, Comprar=14375, Vender=10955\n",
      "Ganhos Totais: 34597.50, Perdas Totais: -33395.75\n",
      "Episode 98/100, Total Reward: 5985.50, Win Rate: 0.57, Wins: 1134, Losses: 866, Epsilon: 0.1867, Steps: 36754, Time: 131.03s\n",
      "Ações: Manter=8143, Comprar=18210, Vender=10401\n",
      "Ganhos Totais: 37330.00, Perdas Totais: -31344.50\n",
      "Modelo e log do episódio 98 salvos em: 4.7\\model_episode_98.pth e 4.7\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: 6227.25, Win Rate: 0.57, Wins: 1109, Losses: 821, Epsilon: 0.1849, Steps: 36754, Time: 119.50s\n",
      "Ações: Manter=7846, Comprar=19025, Vender=9883\n",
      "Ganhos Totais: 37355.75, Perdas Totais: -31128.50\n",
      "Modelo e log do episódio 99 salvos em: 4.7\\model_episode_99.pth e 4.7\\log_episode_99.csv\n",
      "\n",
      "Episode 100/100, Total Reward: 6190.50, Win Rate: 0.58, Wins: 1085, Losses: 785, Epsilon: 0.1830, Steps: 36754, Time: 118.35s\n",
      "Ações: Manter=7748, Comprar=18545, Vender=10461\n",
      "Ganhos Totais: 37936.25, Perdas Totais: -31745.75\n",
      "Modelo e log do episódio 100 salvos em: 4.7\\model_episode_100.pth e 4.7\\log_episode_100.csv\n",
      "\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 92, Total Reward: 9370.00, Win Rate: 0.57, Wins: 1216, Losses: 904, Ações: {0: 9754, 1: 15861, 2: 11139}, Steps: 36754, Time: 121.07s\n",
      "Rank 2: Episode 88, Total Reward: 9210.00, Win Rate: 0.56, Wins: 1058, Losses: 815, Ações: {0: 9460, 1: 17876, 2: 9418}, Steps: 36754, Time: 119.31s\n",
      "Rank 3: Episode 82, Total Reward: 6234.25, Win Rate: 0.57, Wins: 1145, Losses: 879, Ações: {0: 10945, 1: 15921, 2: 9888}, Steps: 36754, Time: 118.87s\n",
      "Rank 4: Episode 99, Total Reward: 6227.25, Win Rate: 0.57, Wins: 1109, Losses: 821, Ações: {0: 7846, 1: 19025, 2: 9883}, Steps: 36754, Time: 119.50s\n",
      "Rank 5: Episode 100, Total Reward: 6190.50, Win Rate: 0.58, Wins: 1085, Losses: 785, Ações: {0: 7748, 1: 18545, 2: 10461}, Steps: 36754, Time: 118.35s\n",
      "Rank 6: Episode 98, Total Reward: 5985.50, Win Rate: 0.57, Wins: 1134, Losses: 866, Ações: {0: 8143, 1: 18210, 2: 10401}, Steps: 36754, Time: 131.03s\n",
      "Rank 7: Episode 87, Total Reward: 5792.25, Win Rate: 0.57, Wins: 1105, Losses: 839, Ações: {0: 11412, 1: 15479, 2: 9863}, Steps: 36754, Time: 137.16s\n",
      "Rank 8: Episode 90, Total Reward: 5653.75, Win Rate: 0.58, Wins: 1108, Losses: 811, Ações: {0: 9402, 1: 18709, 2: 8643}, Steps: 36754, Time: 121.23s\n",
      "Rank 9: Episode 20, Total Reward: 5611.25, Win Rate: 0.53, Wins: 1380, Losses: 1227, Ações: {0: 12601, 1: 12942, 2: 11211}, Steps: 36754, Time: 145.34s\n",
      "Rank 10: Episode 72, Total Reward: 5589.50, Win Rate: 0.58, Wins: 1030, Losses: 761, Ações: {0: 10952, 1: 15715, 2: 10087}, Steps: 36754, Time: 115.85s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
