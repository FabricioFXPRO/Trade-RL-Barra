{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 1486.75, Win Rate: 0.51, Wins: 433, Losses: 414, Epsilon: 0.4950, Steps: 36754, Time: 140.13s\n",
      "Ações: Manter=12225, Comprar=12721, Vender=11808\n",
      "Ganhos Totais: 10098.00, Perdas Totais: -8611.25\n",
      "Modelo e log do episódio 1 salvos em: 4.9\\model_episode_1.pth e 4.9\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -102.00, Win Rate: 0.53, Wins: 466, Losses: 420, Epsilon: 0.4900, Steps: 36754, Time: 148.80s\n",
      "Ações: Manter=10127, Comprar=12494, Vender=14133\n",
      "Ganhos Totais: 8587.50, Perdas Totais: -8689.50\n",
      "Modelo e log do episódio 2 salvos em: 4.9\\model_episode_2.pth e 4.9\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -2139.25, Win Rate: 0.47, Wins: 383, Losses: 424, Epsilon: 0.4851, Steps: 36754, Time: 185.11s\n",
      "Ações: Manter=11829, Comprar=12288, Vender=12637\n",
      "Ganhos Totais: 7544.25, Perdas Totais: -9683.50\n",
      "Modelo e log do episódio 3 salvos em: 4.9\\model_episode_3.pth e 4.9\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -402.25, Win Rate: 0.50, Wins: 407, Losses: 415, Epsilon: 0.4803, Steps: 36754, Time: 176.10s\n",
      "Ações: Manter=12409, Comprar=12101, Vender=12244\n",
      "Ganhos Totais: 9179.25, Perdas Totais: -9581.50\n",
      "Modelo e log do episódio 4 salvos em: 4.9\\model_episode_4.pth e 4.9\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: 292.00, Win Rate: 0.52, Wins: 456, Losses: 417, Epsilon: 0.4755, Steps: 36754, Time: 167.42s\n",
      "Ações: Manter=10676, Comprar=13574, Vender=12504\n",
      "Ganhos Totais: 9196.25, Perdas Totais: -8904.25\n",
      "Modelo e log do episódio 5 salvos em: 4.9\\model_episode_5.pth e 4.9\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -132.00, Win Rate: 0.50, Wins: 410, Losses: 418, Epsilon: 0.4707, Steps: 36754, Time: 162.85s\n",
      "Ações: Manter=12870, Comprar=12023, Vender=11861\n",
      "Ganhos Totais: 8945.75, Perdas Totais: -9077.75\n",
      "Modelo e log do episódio 6 salvos em: 4.9\\model_episode_6.pth e 4.9\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -276.25, Win Rate: 0.50, Wins: 413, Losses: 419, Epsilon: 0.4660, Steps: 36754, Time: 159.61s\n",
      "Ações: Manter=11755, Comprar=13442, Vender=11557\n",
      "Ganhos Totais: 8535.75, Perdas Totais: -8812.00\n",
      "Modelo e log do episódio 7 salvos em: 4.9\\model_episode_7.pth e 4.9\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -45.50, Win Rate: 0.50, Wins: 416, Losses: 412, Epsilon: 0.4614, Steps: 36754, Time: 157.36s\n",
      "Ações: Manter=12528, Comprar=11635, Vender=12591\n",
      "Ganhos Totais: 9946.50, Perdas Totais: -9992.00\n",
      "Modelo e log do episódio 8 salvos em: 4.9\\model_episode_8.pth e 4.9\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -2262.25, Win Rate: 0.49, Wins: 405, Losses: 417, Epsilon: 0.4568, Steps: 36754, Time: 159.51s\n",
      "Ações: Manter=12984, Comprar=11996, Vender=11774\n",
      "Ganhos Totais: 8051.50, Perdas Totais: -10313.75\n",
      "Modelo e log do episódio 9 salvos em: 4.9\\model_episode_9.pth e 4.9\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -1436.75, Win Rate: 0.50, Wins: 422, Losses: 416, Epsilon: 0.4522, Steps: 36754, Time: 161.23s\n",
      "Ações: Manter=11793, Comprar=11478, Vender=13483\n",
      "Ganhos Totais: 7603.00, Perdas Totais: -9039.75\n",
      "Modelo e log do episódio 10 salvos em: 4.9\\model_episode_10.pth e 4.9\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 935.50, Win Rate: 0.52, Wins: 450, Losses: 411, Epsilon: 0.4477, Steps: 36754, Time: 159.22s\n",
      "Ações: Manter=13957, Comprar=11873, Vender=10924\n",
      "Ganhos Totais: 11242.75, Perdas Totais: -10307.25\n",
      "Modelo e log do episódio 11 salvos em: 4.9\\model_episode_11.pth e 4.9\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 145.50, Win Rate: 0.51, Wins: 442, Losses: 420, Epsilon: 0.4432, Steps: 36754, Time: 158.00s\n",
      "Ações: Manter=11499, Comprar=11649, Vender=13606\n",
      "Ganhos Totais: 9150.25, Perdas Totais: -9004.75\n",
      "Modelo e log do episódio 12 salvos em: 4.9\\model_episode_12.pth e 4.9\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 1772.25, Win Rate: 0.53, Wins: 459, Losses: 409, Epsilon: 0.4388, Steps: 36754, Time: 121.43s\n",
      "Ações: Manter=11662, Comprar=12169, Vender=12923\n",
      "Ganhos Totais: 10772.50, Perdas Totais: -9000.25\n",
      "Modelo e log do episódio 13 salvos em: 4.9\\model_episode_13.pth e 4.9\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 1029.50, Win Rate: 0.54, Wins: 488, Losses: 411, Epsilon: 0.4344, Steps: 36754, Time: 120.89s\n",
      "Ações: Manter=11869, Comprar=12396, Vender=12489\n",
      "Ganhos Totais: 10075.25, Perdas Totais: -9045.75\n",
      "Modelo e log do episódio 14 salvos em: 4.9\\model_episode_14.pth e 4.9\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -41.50, Win Rate: 0.49, Wins: 399, Losses: 418, Epsilon: 0.4300, Steps: 36754, Time: 130.58s\n",
      "Ações: Manter=11690, Comprar=13506, Vender=11558\n",
      "Ganhos Totais: 8515.00, Perdas Totais: -8556.50\n",
      "Modelo e log do episódio 15 salvos em: 4.9\\model_episode_15.pth e 4.9\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -124.25, Win Rate: 0.50, Wins: 413, Losses: 419, Epsilon: 0.4257, Steps: 36754, Time: 126.98s\n",
      "Ações: Manter=10384, Comprar=14519, Vender=11851\n",
      "Ganhos Totais: 8095.50, Perdas Totais: -8219.75\n",
      "Modelo e log do episódio 16 salvos em: 4.9\\model_episode_16.pth e 4.9\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -765.25, Win Rate: 0.50, Wins: 420, Losses: 415, Epsilon: 0.4215, Steps: 36754, Time: 121.77s\n",
      "Ações: Manter=12304, Comprar=12183, Vender=12267\n",
      "Ganhos Totais: 8989.00, Perdas Totais: -9754.25\n",
      "Episode 18/100, Total Reward: 773.50, Win Rate: 0.52, Wins: 437, Losses: 410, Epsilon: 0.4173, Steps: 36754, Time: 121.57s\n",
      "Ações: Manter=10955, Comprar=15200, Vender=10599\n",
      "Ganhos Totais: 9890.75, Perdas Totais: -9117.25\n",
      "Modelo e log do episódio 18 salvos em: 4.9\\model_episode_18.pth e 4.9\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: 351.50, Win Rate: 0.50, Wins: 411, Losses: 409, Epsilon: 0.4131, Steps: 36754, Time: 122.61s\n",
      "Ações: Manter=12842, Comprar=11407, Vender=12505\n",
      "Ganhos Totais: 10130.50, Perdas Totais: -9779.00\n",
      "Modelo e log do episódio 19 salvos em: 4.9\\model_episode_19.pth e 4.9\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 55.50, Win Rate: 0.52, Wins: 456, Losses: 418, Epsilon: 0.4090, Steps: 36754, Time: 122.04s\n",
      "Ações: Manter=12451, Comprar=12038, Vender=12265\n",
      "Ganhos Totais: 9740.50, Perdas Totais: -9685.00\n",
      "Modelo e log do episódio 20 salvos em: 4.9\\model_episode_20.pth e 4.9\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: 455.00, Win Rate: 0.53, Wins: 447, Losses: 397, Epsilon: 0.4049, Steps: 36754, Time: 122.18s\n",
      "Ações: Manter=13505, Comprar=13380, Vender=9869\n",
      "Ganhos Totais: 10475.75, Perdas Totais: -10020.75\n",
      "Modelo e log do episódio 21 salvos em: 4.9\\model_episode_21.pth e 4.9\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: 1134.25, Win Rate: 0.49, Wins: 394, Losses: 412, Epsilon: 0.4008, Steps: 36754, Time: 122.34s\n",
      "Ações: Manter=12800, Comprar=11485, Vender=12469\n",
      "Ganhos Totais: 10419.00, Perdas Totais: -9284.75\n",
      "Modelo e log do episódio 22 salvos em: 4.9\\model_episode_22.pth e 4.9\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: 1573.25, Win Rate: 0.52, Wins: 438, Losses: 410, Epsilon: 0.3968, Steps: 36754, Time: 122.51s\n",
      "Ações: Manter=12854, Comprar=13263, Vender=10637\n",
      "Ganhos Totais: 10837.25, Perdas Totais: -9264.00\n",
      "Modelo e log do episódio 23 salvos em: 4.9\\model_episode_23.pth e 4.9\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: 987.00, Win Rate: 0.54, Wins: 467, Losses: 404, Epsilon: 0.3928, Steps: 36754, Time: 122.80s\n",
      "Ações: Manter=12118, Comprar=13248, Vender=11388\n",
      "Ganhos Totais: 10840.75, Perdas Totais: -9853.75\n",
      "Modelo e log do episódio 24 salvos em: 4.9\\model_episode_24.pth e 4.9\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 669.25, Win Rate: 0.51, Wins: 432, Losses: 407, Epsilon: 0.3889, Steps: 36754, Time: 122.76s\n",
      "Ações: Manter=13202, Comprar=12443, Vender=11109\n",
      "Ganhos Totais: 10532.00, Perdas Totais: -9862.75\n",
      "Modelo e log do episódio 25 salvos em: 4.9\\model_episode_25.pth e 4.9\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -957.75, Win Rate: 0.52, Wins: 446, Losses: 412, Epsilon: 0.3850, Steps: 36754, Time: 123.24s\n",
      "Ações: Manter=12280, Comprar=11586, Vender=12888\n",
      "Ganhos Totais: 10078.75, Perdas Totais: -11036.50\n",
      "Episode 27/100, Total Reward: 51.00, Win Rate: 0.51, Wins: 422, Losses: 411, Epsilon: 0.3812, Steps: 36754, Time: 122.27s\n",
      "Ações: Manter=13034, Comprar=11526, Vender=12194\n",
      "Ganhos Totais: 9738.25, Perdas Totais: -9687.25\n",
      "Episode 28/100, Total Reward: 1394.50, Win Rate: 0.54, Wins: 477, Losses: 400, Epsilon: 0.3774, Steps: 36754, Time: 123.28s\n",
      "Ações: Manter=12217, Comprar=11458, Vender=13079\n",
      "Ganhos Totais: 12196.25, Perdas Totais: -10801.75\n",
      "Modelo e log do episódio 28 salvos em: 4.9\\model_episode_28.pth e 4.9\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 926.75, Win Rate: 0.53, Wins: 441, Losses: 399, Epsilon: 0.3736, Steps: 36754, Time: 122.85s\n",
      "Ações: Manter=11175, Comprar=11495, Vender=14084\n",
      "Ganhos Totais: 10891.75, Perdas Totais: -9965.00\n",
      "Modelo e log do episódio 29 salvos em: 4.9\\model_episode_29.pth e 4.9\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: 911.25, Win Rate: 0.55, Wins: 486, Losses: 401, Epsilon: 0.3699, Steps: 36754, Time: 122.73s\n",
      "Ações: Manter=11399, Comprar=12356, Vender=12999\n",
      "Ganhos Totais: 12414.75, Perdas Totais: -11503.50\n",
      "Modelo e log do episódio 30 salvos em: 4.9\\model_episode_30.pth e 4.9\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -3093.50, Win Rate: 0.51, Wins: 426, Losses: 403, Epsilon: 0.3662, Steps: 36754, Time: 122.86s\n",
      "Ações: Manter=12305, Comprar=11991, Vender=12458\n",
      "Ganhos Totais: 10655.50, Perdas Totais: -13749.00\n",
      "Episode 32/100, Total Reward: -750.00, Win Rate: 0.50, Wins: 402, Losses: 403, Epsilon: 0.3625, Steps: 36754, Time: 122.98s\n",
      "Ações: Manter=11835, Comprar=11965, Vender=12954\n",
      "Ganhos Totais: 10275.50, Perdas Totais: -11025.50\n",
      "Episode 33/100, Total Reward: 693.50, Win Rate: 0.51, Wins: 428, Losses: 406, Epsilon: 0.3589, Steps: 36754, Time: 123.31s\n",
      "Ações: Manter=10636, Comprar=14072, Vender=12046\n",
      "Ganhos Totais: 11390.25, Perdas Totais: -10696.75\n",
      "Episode 34/100, Total Reward: -177.75, Win Rate: 0.52, Wins: 450, Losses: 409, Epsilon: 0.3553, Steps: 36754, Time: 123.30s\n",
      "Ações: Manter=9953, Comprar=13754, Vender=13047\n",
      "Ganhos Totais: 11455.75, Perdas Totais: -11633.50\n",
      "Episode 35/100, Total Reward: 1551.00, Win Rate: 0.53, Wins: 457, Losses: 406, Epsilon: 0.3517, Steps: 36754, Time: 122.86s\n",
      "Ações: Manter=14242, Comprar=9761, Vender=12751\n",
      "Ganhos Totais: 12238.25, Perdas Totais: -10687.25\n",
      "Modelo e log do episódio 35 salvos em: 4.9\\model_episode_35.pth e 4.9\\log_episode_35.csv\n",
      "\n",
      "Episode 36/100, Total Reward: -184.75, Win Rate: 0.53, Wins: 455, Losses: 404, Epsilon: 0.3482, Steps: 36754, Time: 123.14s\n",
      "Ações: Manter=10740, Comprar=14417, Vender=11597\n",
      "Ganhos Totais: 11858.00, Perdas Totais: -12042.75\n",
      "Episode 37/100, Total Reward: -1656.50, Win Rate: 0.53, Wins: 464, Losses: 410, Epsilon: 0.3447, Steps: 36754, Time: 123.10s\n",
      "Ações: Manter=10214, Comprar=13747, Vender=12793\n",
      "Ganhos Totais: 11542.75, Perdas Totais: -13199.25\n",
      "Episode 38/100, Total Reward: -746.25, Win Rate: 0.53, Wins: 448, Losses: 399, Epsilon: 0.3413, Steps: 36754, Time: 122.98s\n",
      "Ações: Manter=11772, Comprar=11163, Vender=13819\n",
      "Ganhos Totais: 10848.75, Perdas Totais: -11595.00\n",
      "Episode 39/100, Total Reward: -1545.50, Win Rate: 0.50, Wins: 406, Losses: 409, Epsilon: 0.3379, Steps: 36754, Time: 123.15s\n",
      "Ações: Manter=13833, Comprar=11260, Vender=11661\n",
      "Ganhos Totais: 11256.25, Perdas Totais: -12801.75\n",
      "Episode 40/100, Total Reward: -2326.75, Win Rate: 0.52, Wins: 442, Losses: 405, Epsilon: 0.3345, Steps: 36754, Time: 123.12s\n",
      "Ações: Manter=13941, Comprar=10601, Vender=12212\n",
      "Ganhos Totais: 10541.00, Perdas Totais: -12867.75\n",
      "Episode 41/100, Total Reward: -719.00, Win Rate: 0.48, Wins: 377, Losses: 405, Epsilon: 0.3311, Steps: 36754, Time: 123.35s\n",
      "Ações: Manter=11048, Comprar=12959, Vender=12747\n",
      "Ganhos Totais: 11085.00, Perdas Totais: -11804.00\n",
      "Episode 42/100, Total Reward: 1123.50, Win Rate: 0.52, Wins: 433, Losses: 407, Epsilon: 0.3278, Steps: 36754, Time: 123.20s\n",
      "Ações: Manter=9919, Comprar=16925, Vender=9910\n",
      "Ganhos Totais: 12518.75, Perdas Totais: -11395.25\n",
      "Modelo e log do episódio 42 salvos em: 4.9\\model_episode_42.pth e 4.9\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: 1351.00, Win Rate: 0.54, Wins: 479, Losses: 410, Epsilon: 0.3246, Steps: 36754, Time: 123.60s\n",
      "Ações: Manter=10563, Comprar=12754, Vender=13437\n",
      "Ganhos Totais: 13141.75, Perdas Totais: -11790.75\n",
      "Modelo e log do episódio 43 salvos em: 4.9\\model_episode_43.pth e 4.9\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -267.75, Win Rate: 0.53, Wins: 460, Losses: 402, Epsilon: 0.3213, Steps: 36754, Time: 123.42s\n",
      "Ações: Manter=11901, Comprar=12793, Vender=12060\n",
      "Ganhos Totais: 12184.75, Perdas Totais: -12452.50\n",
      "Episode 45/100, Total Reward: -3213.50, Win Rate: 0.49, Wins: 379, Losses: 402, Epsilon: 0.3181, Steps: 36754, Time: 123.39s\n",
      "Ações: Manter=13428, Comprar=12888, Vender=10438\n",
      "Ganhos Totais: 11184.75, Perdas Totais: -14398.25\n",
      "Episode 46/100, Total Reward: 388.00, Win Rate: 0.53, Wins: 461, Losses: 410, Epsilon: 0.3149, Steps: 36754, Time: 123.41s\n",
      "Ações: Manter=9903, Comprar=11705, Vender=15146\n",
      "Ganhos Totais: 12394.50, Perdas Totais: -12006.50\n",
      "Episode 47/100, Total Reward: -1852.50, Win Rate: 0.51, Wins: 424, Losses: 405, Epsilon: 0.3118, Steps: 36754, Time: 123.57s\n",
      "Ações: Manter=12685, Comprar=10014, Vender=14055\n",
      "Ganhos Totais: 11339.50, Perdas Totais: -13192.00\n",
      "Episode 48/100, Total Reward: 2213.25, Win Rate: 0.54, Wins: 467, Losses: 399, Epsilon: 0.3086, Steps: 36754, Time: 125.27s\n",
      "Ações: Manter=11575, Comprar=11562, Vender=13617\n",
      "Ganhos Totais: 13851.25, Perdas Totais: -11638.00\n",
      "Modelo e log do episódio 48 salvos em: 4.9\\model_episode_48.pth e 4.9\\log_episode_48.csv\n",
      "\n",
      "Episode 49/100, Total Reward: -1383.50, Win Rate: 0.53, Wins: 441, Losses: 398, Epsilon: 0.3056, Steps: 36754, Time: 123.45s\n",
      "Ações: Manter=14189, Comprar=10888, Vender=11677\n",
      "Ganhos Totais: 11564.50, Perdas Totais: -12948.00\n",
      "Episode 50/100, Total Reward: -852.50, Win Rate: 0.51, Wins: 409, Losses: 397, Epsilon: 0.3025, Steps: 36754, Time: 123.46s\n",
      "Ações: Manter=11481, Comprar=12471, Vender=12802\n",
      "Ganhos Totais: 12843.75, Perdas Totais: -13696.25\n",
      "Episode 51/100, Total Reward: -700.75, Win Rate: 0.52, Wins: 437, Losses: 398, Epsilon: 0.2995, Steps: 36754, Time: 123.61s\n",
      "Ações: Manter=8755, Comprar=11973, Vender=16026\n",
      "Ganhos Totais: 11966.50, Perdas Totais: -12667.25\n",
      "Episode 52/100, Total Reward: -769.50, Win Rate: 0.53, Wins: 455, Losses: 400, Epsilon: 0.2965, Steps: 36754, Time: 123.76s\n",
      "Ações: Manter=8316, Comprar=12896, Vender=15542\n",
      "Ganhos Totais: 12888.00, Perdas Totais: -13657.50\n",
      "Episode 53/100, Total Reward: -1332.50, Win Rate: 0.53, Wins: 454, Losses: 395, Epsilon: 0.2935, Steps: 36754, Time: 123.91s\n",
      "Ações: Manter=12488, Comprar=12293, Vender=11973\n",
      "Ganhos Totais: 12872.75, Perdas Totais: -14205.25\n",
      "Episode 54/100, Total Reward: -710.25, Win Rate: 0.55, Wins: 467, Losses: 387, Epsilon: 0.2906, Steps: 36754, Time: 124.01s\n",
      "Ações: Manter=15508, Comprar=10680, Vender=10566\n",
      "Ganhos Totais: 13694.50, Perdas Totais: -14404.75\n",
      "Episode 55/100, Total Reward: -1229.00, Win Rate: 0.48, Wins: 373, Losses: 399, Epsilon: 0.2877, Steps: 36754, Time: 124.24s\n",
      "Ações: Manter=13754, Comprar=12407, Vender=10593\n",
      "Ganhos Totais: 12218.50, Perdas Totais: -13447.50\n",
      "Episode 56/100, Total Reward: 494.75, Win Rate: 0.54, Wins: 452, Losses: 378, Epsilon: 0.2848, Steps: 36754, Time: 123.72s\n",
      "Ações: Manter=14751, Comprar=11229, Vender=10774\n",
      "Ganhos Totais: 14381.00, Perdas Totais: -13886.25\n",
      "Episode 57/100, Total Reward: -2338.25, Win Rate: 0.51, Wins: 414, Losses: 397, Epsilon: 0.2820, Steps: 36754, Time: 124.13s\n",
      "Ações: Manter=11599, Comprar=12233, Vender=12922\n",
      "Ganhos Totais: 12220.25, Perdas Totais: -14558.50\n",
      "Episode 58/100, Total Reward: -1491.25, Win Rate: 0.51, Wins: 412, Losses: 399, Epsilon: 0.2791, Steps: 36754, Time: 124.37s\n",
      "Ações: Manter=14194, Comprar=11423, Vender=11137\n",
      "Ganhos Totais: 11865.25, Perdas Totais: -13356.50\n",
      "Episode 59/100, Total Reward: 1656.25, Win Rate: 0.52, Wins: 419, Losses: 391, Epsilon: 0.2763, Steps: 36754, Time: 124.75s\n",
      "Ações: Manter=13649, Comprar=11699, Vender=11406\n",
      "Ganhos Totais: 14061.75, Perdas Totais: -12405.50\n",
      "Modelo e log do episódio 59 salvos em: 4.9\\model_episode_59.pth e 4.9\\log_episode_59.csv\n",
      "\n",
      "Episode 60/100, Total Reward: 635.00, Win Rate: 0.53, Wins: 440, Losses: 388, Epsilon: 0.2736, Steps: 36754, Time: 124.74s\n",
      "Ações: Manter=11674, Comprar=13690, Vender=11390\n",
      "Ganhos Totais: 14782.75, Perdas Totais: -14147.75\n",
      "Episode 61/100, Total Reward: 642.00, Win Rate: 0.53, Wins: 452, Losses: 396, Epsilon: 0.2708, Steps: 36754, Time: 124.29s\n",
      "Ações: Manter=9330, Comprar=11800, Vender=15624\n",
      "Ganhos Totais: 14420.00, Perdas Totais: -13778.00\n",
      "Episode 62/100, Total Reward: 109.25, Win Rate: 0.57, Wins: 506, Losses: 383, Epsilon: 0.2681, Steps: 36754, Time: 124.75s\n",
      "Ações: Manter=11553, Comprar=13439, Vender=11762\n",
      "Ganhos Totais: 13446.50, Perdas Totais: -13337.25\n",
      "Episode 63/100, Total Reward: 372.00, Win Rate: 0.55, Wins: 483, Losses: 389, Epsilon: 0.2655, Steps: 36754, Time: 124.75s\n",
      "Ações: Manter=13315, Comprar=11723, Vender=11716\n",
      "Ganhos Totais: 14723.25, Perdas Totais: -14351.25\n",
      "Episode 64/100, Total Reward: 1641.75, Win Rate: 0.55, Wins: 469, Losses: 384, Epsilon: 0.2628, Steps: 36754, Time: 124.73s\n",
      "Ações: Manter=15274, Comprar=12351, Vender=9129\n",
      "Ganhos Totais: 15729.00, Perdas Totais: -14087.25\n",
      "Modelo e log do episódio 64 salvos em: 4.9\\model_episode_64.pth e 4.9\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: 136.50, Win Rate: 0.55, Wins: 474, Losses: 383, Epsilon: 0.2602, Steps: 36754, Time: 124.78s\n",
      "Ações: Manter=14074, Comprar=13336, Vender=9344\n",
      "Ganhos Totais: 14820.50, Perdas Totais: -14684.00\n",
      "Episode 66/100, Total Reward: -273.25, Win Rate: 0.55, Wins: 471, Losses: 390, Epsilon: 0.2576, Steps: 36754, Time: 124.61s\n",
      "Ações: Manter=9629, Comprar=12706, Vender=14419\n",
      "Ganhos Totais: 13618.50, Perdas Totais: -13891.75\n",
      "Episode 67/100, Total Reward: -990.00, Win Rate: 0.53, Wins: 426, Losses: 381, Epsilon: 0.2550, Steps: 36754, Time: 124.78s\n",
      "Ações: Manter=14221, Comprar=13295, Vender=9238\n",
      "Ganhos Totais: 13468.50, Perdas Totais: -14458.50\n",
      "Episode 68/100, Total Reward: -671.50, Win Rate: 0.54, Wins: 472, Losses: 398, Epsilon: 0.2524, Steps: 36754, Time: 124.85s\n",
      "Ações: Manter=11361, Comprar=14104, Vender=11289\n",
      "Ganhos Totais: 13237.75, Perdas Totais: -13909.25\n",
      "Episode 69/100, Total Reward: 1355.75, Win Rate: 0.57, Wins: 500, Losses: 381, Epsilon: 0.2499, Steps: 36754, Time: 124.54s\n",
      "Ações: Manter=11008, Comprar=13489, Vender=12257\n",
      "Ganhos Totais: 14698.50, Perdas Totais: -13342.75\n",
      "Modelo e log do episódio 69 salvos em: 4.9\\model_episode_69.pth e 4.9\\log_episode_69.csv\n",
      "\n",
      "Episode 70/100, Total Reward: 431.00, Win Rate: 0.55, Wins: 473, Losses: 393, Epsilon: 0.2474, Steps: 36754, Time: 124.95s\n",
      "Ações: Manter=11938, Comprar=12018, Vender=12798\n",
      "Ganhos Totais: 13966.75, Perdas Totais: -13535.75\n",
      "Episode 71/100, Total Reward: 2686.75, Win Rate: 0.55, Wins: 475, Losses: 383, Epsilon: 0.2449, Steps: 36754, Time: 124.74s\n",
      "Ações: Manter=13099, Comprar=13502, Vender=10153\n",
      "Ganhos Totais: 14985.50, Perdas Totais: -12298.75\n",
      "Modelo e log do episódio 71 salvos em: 4.9\\model_episode_71.pth e 4.9\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: -628.25, Win Rate: 0.55, Wins: 464, Losses: 375, Epsilon: 0.2425, Steps: 36754, Time: 125.27s\n",
      "Ações: Manter=11511, Comprar=13707, Vender=11536\n",
      "Ganhos Totais: 14815.25, Perdas Totais: -15443.50\n",
      "Episode 73/100, Total Reward: 337.25, Win Rate: 0.54, Wins: 458, Losses: 391, Epsilon: 0.2401, Steps: 36754, Time: 125.05s\n",
      "Ações: Manter=10507, Comprar=13080, Vender=13167\n",
      "Ganhos Totais: 13441.00, Perdas Totais: -13103.75\n",
      "Episode 74/100, Total Reward: 1051.00, Win Rate: 0.53, Wins: 447, Losses: 399, Epsilon: 0.2377, Steps: 36754, Time: 124.99s\n",
      "Ações: Manter=7435, Comprar=16567, Vender=12752\n",
      "Ganhos Totais: 15180.25, Perdas Totais: -14129.25\n",
      "Episode 75/100, Total Reward: -736.75, Win Rate: 0.54, Wins: 463, Losses: 391, Epsilon: 0.2353, Steps: 36754, Time: 124.80s\n",
      "Ações: Manter=12833, Comprar=13300, Vender=10621\n",
      "Ganhos Totais: 15609.00, Perdas Totais: -16345.75\n",
      "Episode 76/100, Total Reward: -59.75, Win Rate: 0.54, Wins: 455, Losses: 389, Epsilon: 0.2329, Steps: 36754, Time: 125.37s\n",
      "Ações: Manter=10543, Comprar=15698, Vender=10513\n",
      "Ganhos Totais: 12858.50, Perdas Totais: -12918.25\n",
      "Episode 77/100, Total Reward: -1115.75, Win Rate: 0.52, Wins: 436, Losses: 398, Epsilon: 0.2306, Steps: 36754, Time: 125.76s\n",
      "Ações: Manter=9439, Comprar=17440, Vender=9875\n",
      "Ganhos Totais: 11792.00, Perdas Totais: -12907.75\n",
      "Episode 78/100, Total Reward: -1527.75, Win Rate: 0.51, Wins: 416, Losses: 392, Epsilon: 0.2283, Steps: 36754, Time: 125.19s\n",
      "Ações: Manter=14500, Comprar=12178, Vender=10076\n",
      "Ganhos Totais: 13067.75, Perdas Totais: -14595.50\n",
      "Episode 79/100, Total Reward: -1006.00, Win Rate: 0.53, Wins: 446, Losses: 394, Epsilon: 0.2260, Steps: 36754, Time: 125.03s\n",
      "Ações: Manter=11872, Comprar=13930, Vender=10952\n",
      "Ganhos Totais: 12563.75, Perdas Totais: -13569.75\n",
      "Episode 80/100, Total Reward: -675.00, Win Rate: 0.54, Wins: 475, Losses: 403, Epsilon: 0.2238, Steps: 36754, Time: 125.18s\n",
      "Ações: Manter=8985, Comprar=16472, Vender=11297\n",
      "Ganhos Totais: 13192.00, Perdas Totais: -13867.00\n",
      "Episode 81/100, Total Reward: 1337.75, Win Rate: 0.53, Wins: 454, Losses: 395, Epsilon: 0.2215, Steps: 36754, Time: 124.80s\n",
      "Ações: Manter=13213, Comprar=13065, Vender=10476\n",
      "Ganhos Totais: 14783.75, Perdas Totais: -13446.00\n",
      "Episode 82/100, Total Reward: -1438.75, Win Rate: 0.53, Wins: 443, Losses: 395, Epsilon: 0.2193, Steps: 36754, Time: 125.29s\n",
      "Ações: Manter=15385, Comprar=11485, Vender=9884\n",
      "Ganhos Totais: 14391.50, Perdas Totais: -15830.25\n",
      "Episode 83/100, Total Reward: -14.50, Win Rate: 0.56, Wins: 477, Losses: 376, Epsilon: 0.2171, Steps: 36754, Time: 124.80s\n",
      "Ações: Manter=15178, Comprar=13045, Vender=8531\n",
      "Ganhos Totais: 16951.25, Perdas Totais: -16965.75\n",
      "Episode 84/100, Total Reward: 1957.75, Win Rate: 0.54, Wins: 441, Losses: 383, Epsilon: 0.2149, Steps: 36754, Time: 125.02s\n",
      "Ações: Manter=13614, Comprar=12197, Vender=10943\n",
      "Ganhos Totais: 15993.25, Perdas Totais: -14035.50\n",
      "Modelo e log do episódio 84 salvos em: 4.9\\model_episode_84.pth e 4.9\\log_episode_84.csv\n",
      "\n",
      "Episode 85/100, Total Reward: 1405.50, Win Rate: 0.54, Wins: 469, Losses: 397, Epsilon: 0.2128, Steps: 36754, Time: 124.94s\n",
      "Ações: Manter=14745, Comprar=12080, Vender=9929\n",
      "Ganhos Totais: 14518.25, Perdas Totais: -13112.75\n",
      "Modelo e log do episódio 85 salvos em: 4.9\\model_episode_85.pth e 4.9\\log_episode_85.csv\n",
      "\n",
      "Episode 86/100, Total Reward: 721.25, Win Rate: 0.57, Wins: 522, Losses: 393, Epsilon: 0.2107, Steps: 36754, Time: 125.54s\n",
      "Ações: Manter=13991, Comprar=12847, Vender=9916\n",
      "Ganhos Totais: 14693.00, Perdas Totais: -13971.75\n",
      "Episode 87/100, Total Reward: -466.00, Win Rate: 0.55, Wins: 470, Losses: 387, Epsilon: 0.2086, Steps: 36754, Time: 124.97s\n",
      "Ações: Manter=15019, Comprar=12508, Vender=9227\n",
      "Ganhos Totais: 14648.00, Perdas Totais: -15114.00\n",
      "Episode 88/100, Total Reward: 95.75, Win Rate: 0.57, Wins: 515, Losses: 394, Epsilon: 0.2065, Steps: 36754, Time: 125.33s\n",
      "Ações: Manter=9650, Comprar=16987, Vender=10117\n",
      "Ganhos Totais: 12135.00, Perdas Totais: -12039.25\n",
      "Episode 89/100, Total Reward: -331.00, Win Rate: 0.55, Wins: 507, Losses: 407, Epsilon: 0.2044, Steps: 36754, Time: 128.46s\n",
      "Ações: Manter=11151, Comprar=15651, Vender=9952\n",
      "Ganhos Totais: 13366.00, Perdas Totais: -13697.00\n",
      "Episode 90/100, Total Reward: 229.00, Win Rate: 0.55, Wins: 472, Losses: 385, Epsilon: 0.2024, Steps: 36754, Time: 126.69s\n",
      "Ações: Manter=16346, Comprar=11787, Vender=8621\n",
      "Ganhos Totais: 14451.25, Perdas Totais: -14222.25\n",
      "Episode 91/100, Total Reward: 1300.00, Win Rate: 0.56, Wins: 514, Losses: 396, Epsilon: 0.2003, Steps: 36754, Time: 124.61s\n",
      "Ações: Manter=13516, Comprar=15571, Vender=7667\n",
      "Ganhos Totais: 14513.25, Perdas Totais: -13213.25\n",
      "Episode 92/100, Total Reward: 714.00, Win Rate: 0.54, Wins: 453, Losses: 385, Epsilon: 0.1983, Steps: 36754, Time: 124.66s\n",
      "Ações: Manter=13056, Comprar=15501, Vender=8197\n",
      "Ganhos Totais: 13846.75, Perdas Totais: -13132.75\n",
      "Episode 93/100, Total Reward: 581.50, Win Rate: 0.57, Wins: 520, Losses: 388, Epsilon: 0.1964, Steps: 36754, Time: 125.34s\n",
      "Ações: Manter=12295, Comprar=14890, Vender=9569\n",
      "Ganhos Totais: 15279.25, Perdas Totais: -14697.75\n",
      "Episode 94/100, Total Reward: -1308.25, Win Rate: 0.54, Wins: 457, Losses: 390, Epsilon: 0.1944, Steps: 36754, Time: 125.20s\n",
      "Ações: Manter=15922, Comprar=11831, Vender=9001\n",
      "Ganhos Totais: 14596.50, Perdas Totais: -15904.75\n",
      "Episode 95/100, Total Reward: -1411.75, Win Rate: 0.54, Wins: 450, Losses: 383, Epsilon: 0.1924, Steps: 36754, Time: 125.29s\n",
      "Ações: Manter=16067, Comprar=12437, Vender=8250\n",
      "Ganhos Totais: 13750.75, Perdas Totais: -15162.50\n",
      "Episode 96/100, Total Reward: 1964.25, Win Rate: 0.58, Wins: 527, Losses: 383, Epsilon: 0.1905, Steps: 36754, Time: 125.52s\n",
      "Ações: Manter=16054, Comprar=11602, Vender=9098\n",
      "Ganhos Totais: 16946.25, Perdas Totais: -14982.00\n",
      "Modelo e log do episódio 96 salvos em: 4.9\\model_episode_96.pth e 4.9\\log_episode_96.csv\n",
      "\n",
      "Episode 97/100, Total Reward: -1306.50, Win Rate: 0.54, Wins: 467, Losses: 390, Epsilon: 0.1886, Steps: 36754, Time: 125.33s\n",
      "Ações: Manter=15550, Comprar=10930, Vender=10274\n",
      "Ganhos Totais: 13917.25, Perdas Totais: -15223.75\n",
      "Episode 98/100, Total Reward: -228.75, Win Rate: 0.54, Wins: 469, Losses: 396, Epsilon: 0.1867, Steps: 36754, Time: 125.78s\n",
      "Ações: Manter=14098, Comprar=14636, Vender=8020\n",
      "Ganhos Totais: 13425.00, Perdas Totais: -13653.75\n",
      "Episode 99/100, Total Reward: 877.00, Win Rate: 0.55, Wins: 488, Losses: 393, Epsilon: 0.1849, Steps: 36754, Time: 125.42s\n",
      "Ações: Manter=11326, Comprar=16747, Vender=8681\n",
      "Ganhos Totais: 12947.00, Perdas Totais: -12070.00\n",
      "Episode 100/100, Total Reward: -1677.50, Win Rate: 0.54, Wins: 475, Losses: 397, Epsilon: 0.1830, Steps: 36754, Time: 125.51s\n",
      "Ações: Manter=10178, Comprar=16914, Vender=9662\n",
      "Ganhos Totais: 14107.25, Perdas Totais: -15784.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 71, Total Reward: 2686.75, Win Rate: 0.55, Wins: 475, Losses: 383, Ações: {0: 13099, 1: 13502, 2: 10153}, Steps: 36754, Time: 124.74s\n",
      "Rank 2: Episode 48, Total Reward: 2213.25, Win Rate: 0.54, Wins: 467, Losses: 399, Ações: {0: 11575, 1: 11562, 2: 13617}, Steps: 36754, Time: 125.27s\n",
      "Rank 3: Episode 96, Total Reward: 1964.25, Win Rate: 0.58, Wins: 527, Losses: 383, Ações: {0: 16054, 1: 11602, 2: 9098}, Steps: 36754, Time: 125.52s\n",
      "Rank 4: Episode 84, Total Reward: 1957.75, Win Rate: 0.54, Wins: 441, Losses: 383, Ações: {0: 13614, 1: 12197, 2: 10943}, Steps: 36754, Time: 125.02s\n",
      "Rank 5: Episode 13, Total Reward: 1772.25, Win Rate: 0.53, Wins: 459, Losses: 409, Ações: {0: 11662, 1: 12169, 2: 12923}, Steps: 36754, Time: 121.43s\n",
      "Rank 6: Episode 59, Total Reward: 1656.25, Win Rate: 0.52, Wins: 419, Losses: 391, Ações: {0: 13649, 1: 11699, 2: 11406}, Steps: 36754, Time: 124.75s\n",
      "Rank 7: Episode 64, Total Reward: 1641.75, Win Rate: 0.55, Wins: 469, Losses: 384, Ações: {0: 15274, 1: 12351, 2: 9129}, Steps: 36754, Time: 124.73s\n",
      "Rank 8: Episode 23, Total Reward: 1573.25, Win Rate: 0.52, Wins: 438, Losses: 410, Ações: {0: 12854, 1: 13263, 2: 10637}, Steps: 36754, Time: 122.51s\n",
      "Rank 9: Episode 35, Total Reward: 1551.00, Win Rate: 0.53, Wins: 457, Losses: 406, Ações: {0: 14242, 1: 9761, 2: 12751}, Steps: 36754, Time: 122.86s\n",
      "Rank 10: Episode 1, Total Reward: 1486.75, Win Rate: 0.51, Wins: 433, Losses: 414, Ações: {0: 12225, 1: 12721, 2: 11808}, Steps: 36754, Time: 140.13s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "        self.operation_limit = 0  # Variável de controle para limitar operações após perda\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        self.operation_limit = 0  # Resetar a limitação de operações ao resetar o ambiente\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo e o agente não estiver limitado por perda anterior\n",
    "        if gatilho == 1 and self.operation_limit == 0:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "\n",
    "                    # Atualizar operation_limit caso a operação tenha dado prejuízo\n",
    "                    if profit < 0:\n",
    "                        self.operation_limit = 1\n",
    "\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "\n",
    "                    # Atualizar operation_limit caso a operação tenha dado prejuízo\n",
    "                    if profit < 0:\n",
    "                        self.operation_limit = 1\n",
    "        elif gatilho == 0:\n",
    "            # Quando o gatilho for zero, liberar a operação novamente\n",
    "            self.operation_limit = 0\n",
    "\n",
    "            # Fechar qualquer posição aberta\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.9\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
