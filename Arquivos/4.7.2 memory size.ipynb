{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -2030.00, Win Rate: 0.50, Wins: 1367, Losses: 1378, Epsilon: 0.4950, Steps: 36754, Time: 119.90s\n",
      "Ações: Manter=10628, Comprar=13822, Vender=12304\n",
      "Ganhos Totais: 37637.75, Perdas Totais: -39667.75\n",
      "Modelo e log do episódio 1 salvos em: 4.7.2\\model_episode_1.pth e 4.7.2\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: 517.50, Win Rate: 0.52, Wins: 1329, Losses: 1241, Epsilon: 0.4900, Steps: 36754, Time: 115.76s\n",
      "Ações: Manter=9314, Comprar=14263, Vender=13177\n",
      "Ganhos Totais: 37380.00, Perdas Totais: -36862.50\n",
      "Modelo e log do episódio 2 salvos em: 4.7.2\\model_episode_2.pth e 4.7.2\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1924.25, Win Rate: 0.51, Wins: 1375, Losses: 1307, Epsilon: 0.4851, Steps: 36754, Time: 125.62s\n",
      "Ações: Manter=9470, Comprar=14474, Vender=12810\n",
      "Ganhos Totais: 35697.00, Perdas Totais: -37621.25\n",
      "Modelo e log do episódio 3 salvos em: 4.7.2\\model_episode_3.pth e 4.7.2\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 17.25, Win Rate: 0.52, Wins: 1341, Losses: 1250, Epsilon: 0.4803, Steps: 36754, Time: 127.34s\n",
      "Ações: Manter=10553, Comprar=13944, Vender=12257\n",
      "Ganhos Totais: 36511.50, Perdas Totais: -36494.25\n",
      "Modelo e log do episódio 4 salvos em: 4.7.2\\model_episode_4.pth e 4.7.2\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -318.75, Win Rate: 0.51, Wins: 1284, Losses: 1243, Epsilon: 0.4755, Steps: 36754, Time: 124.90s\n",
      "Ações: Manter=12121, Comprar=13438, Vender=11195\n",
      "Ganhos Totais: 37263.50, Perdas Totais: -37582.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.2\\model_episode_5.pth e 4.7.2\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 1632.25, Win Rate: 0.54, Wins: 1447, Losses: 1243, Epsilon: 0.4707, Steps: 36754, Time: 124.87s\n",
      "Ações: Manter=11808, Comprar=13264, Vender=11682\n",
      "Ganhos Totais: 37546.75, Perdas Totais: -35914.50\n",
      "Modelo e log do episódio 6 salvos em: 4.7.2\\model_episode_6.pth e 4.7.2\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -5047.25, Win Rate: 0.50, Wins: 1342, Losses: 1333, Epsilon: 0.4660, Steps: 36754, Time: 125.94s\n",
      "Ações: Manter=10547, Comprar=13730, Vender=12477\n",
      "Ganhos Totais: 34879.00, Perdas Totais: -39926.25\n",
      "Modelo e log do episódio 7 salvos em: 4.7.2\\model_episode_7.pth e 4.7.2\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -196.50, Win Rate: 0.51, Wins: 1273, Losses: 1227, Epsilon: 0.4614, Steps: 36754, Time: 125.15s\n",
      "Ações: Manter=13177, Comprar=11401, Vender=12176\n",
      "Ganhos Totais: 36818.75, Perdas Totais: -37015.25\n",
      "Modelo e log do episódio 8 salvos em: 4.7.2\\model_episode_8.pth e 4.7.2\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: 1180.75, Win Rate: 0.51, Wins: 1349, Losses: 1311, Epsilon: 0.4568, Steps: 36754, Time: 124.80s\n",
      "Ações: Manter=12406, Comprar=12913, Vender=11435\n",
      "Ganhos Totais: 38282.00, Perdas Totais: -37101.25\n",
      "Modelo e log do episódio 9 salvos em: 4.7.2\\model_episode_9.pth e 4.7.2\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: 2072.00, Win Rate: 0.51, Wins: 1351, Losses: 1293, Epsilon: 0.4522, Steps: 36754, Time: 124.89s\n",
      "Ações: Manter=12126, Comprar=12557, Vender=12071\n",
      "Ganhos Totais: 37976.50, Perdas Totais: -35904.50\n",
      "Modelo e log do episódio 10 salvos em: 4.7.2\\model_episode_10.pth e 4.7.2\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 3962.00, Win Rate: 0.52, Wins: 1393, Losses: 1268, Epsilon: 0.4477, Steps: 36754, Time: 125.41s\n",
      "Ações: Manter=12722, Comprar=11750, Vender=12282\n",
      "Ganhos Totais: 40116.75, Perdas Totais: -36154.75\n",
      "Modelo e log do episódio 11 salvos em: 4.7.2\\model_episode_11.pth e 4.7.2\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -1152.75, Win Rate: 0.50, Wins: 1307, Losses: 1298, Epsilon: 0.4432, Steps: 36754, Time: 124.89s\n",
      "Ações: Manter=12014, Comprar=12343, Vender=12397\n",
      "Ganhos Totais: 36429.00, Perdas Totais: -37581.75\n",
      "Modelo e log do episódio 12 salvos em: 4.7.2\\model_episode_12.pth e 4.7.2\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -6460.25, Win Rate: 0.48, Wins: 1263, Losses: 1345, Epsilon: 0.4388, Steps: 36754, Time: 125.19s\n",
      "Ações: Manter=11053, Comprar=12426, Vender=13275\n",
      "Ganhos Totais: 33834.25, Perdas Totais: -40294.50\n",
      "Episode 14/100, Total Reward: 491.75, Win Rate: 0.52, Wins: 1385, Losses: 1302, Epsilon: 0.4344, Steps: 36754, Time: 125.62s\n",
      "Ações: Manter=10495, Comprar=12618, Vender=13641\n",
      "Ganhos Totais: 38497.75, Perdas Totais: -38006.00\n",
      "Modelo e log do episódio 14 salvos em: 4.7.2\\model_episode_14.pth e 4.7.2\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -2332.00, Win Rate: 0.52, Wins: 1352, Losses: 1273, Epsilon: 0.4300, Steps: 36754, Time: 125.17s\n",
      "Ações: Manter=12326, Comprar=12527, Vender=11901\n",
      "Ganhos Totais: 36525.75, Perdas Totais: -38857.75\n",
      "Episode 16/100, Total Reward: 807.75, Win Rate: 0.50, Wins: 1347, Losses: 1371, Epsilon: 0.4257, Steps: 36754, Time: 125.41s\n",
      "Ações: Manter=11438, Comprar=13219, Vender=12097\n",
      "Ganhos Totais: 37035.00, Perdas Totais: -36227.25\n",
      "Modelo e log do episódio 16 salvos em: 4.7.2\\model_episode_16.pth e 4.7.2\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -1620.75, Win Rate: 0.50, Wins: 1301, Losses: 1277, Epsilon: 0.4215, Steps: 36754, Time: 125.96s\n",
      "Ações: Manter=13085, Comprar=12121, Vender=11548\n",
      "Ganhos Totais: 36714.75, Perdas Totais: -38335.50\n",
      "Episode 18/100, Total Reward: -2346.25, Win Rate: 0.51, Wins: 1302, Losses: 1258, Epsilon: 0.4173, Steps: 36754, Time: 125.63s\n",
      "Ações: Manter=13264, Comprar=11565, Vender=11925\n",
      "Ganhos Totais: 35114.25, Perdas Totais: -37460.50\n",
      "Episode 19/100, Total Reward: -2951.75, Win Rate: 0.52, Wins: 1390, Losses: 1271, Epsilon: 0.4131, Steps: 36754, Time: 127.00s\n",
      "Ações: Manter=11431, Comprar=11388, Vender=13935\n",
      "Ganhos Totais: 35534.75, Perdas Totais: -38486.50\n",
      "Episode 20/100, Total Reward: -1466.00, Win Rate: 0.52, Wins: 1352, Losses: 1227, Epsilon: 0.4090, Steps: 36754, Time: 126.54s\n",
      "Ações: Manter=11721, Comprar=13032, Vender=12001\n",
      "Ganhos Totais: 37007.00, Perdas Totais: -38473.00\n",
      "Episode 21/100, Total Reward: -2100.00, Win Rate: 0.50, Wins: 1329, Losses: 1313, Epsilon: 0.4049, Steps: 36754, Time: 126.48s\n",
      "Ações: Manter=10855, Comprar=11495, Vender=14404\n",
      "Ganhos Totais: 35923.50, Perdas Totais: -38023.50\n",
      "Episode 22/100, Total Reward: 1252.00, Win Rate: 0.53, Wins: 1427, Losses: 1281, Epsilon: 0.4008, Steps: 36754, Time: 126.56s\n",
      "Ações: Manter=11821, Comprar=11662, Vender=13271\n",
      "Ganhos Totais: 38696.75, Perdas Totais: -37444.75\n",
      "Modelo e log do episódio 22 salvos em: 4.7.2\\model_episode_22.pth e 4.7.2\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: -3013.75, Win Rate: 0.52, Wins: 1323, Losses: 1204, Epsilon: 0.3968, Steps: 36754, Time: 126.57s\n",
      "Ações: Manter=11416, Comprar=12500, Vender=12838\n",
      "Ganhos Totais: 34161.50, Perdas Totais: -37175.25\n",
      "Episode 24/100, Total Reward: 1200.50, Win Rate: 0.53, Wins: 1348, Losses: 1203, Epsilon: 0.3928, Steps: 36754, Time: 118.54s\n",
      "Ações: Manter=13466, Comprar=12113, Vender=11175\n",
      "Ganhos Totais: 36844.75, Perdas Totais: -35644.25\n",
      "Modelo e log do episódio 24 salvos em: 4.7.2\\model_episode_24.pth e 4.7.2\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -1759.75, Win Rate: 0.52, Wins: 1401, Losses: 1297, Epsilon: 0.3889, Steps: 36754, Time: 111.74s\n",
      "Ações: Manter=10722, Comprar=13476, Vender=12556\n",
      "Ganhos Totais: 36137.75, Perdas Totais: -37897.50\n",
      "Episode 26/100, Total Reward: 1807.25, Win Rate: 0.51, Wins: 1373, Losses: 1325, Epsilon: 0.3850, Steps: 36754, Time: 110.51s\n",
      "Ações: Manter=12411, Comprar=12318, Vender=12025\n",
      "Ganhos Totais: 38887.25, Perdas Totais: -37080.00\n",
      "Modelo e log do episódio 26 salvos em: 4.7.2\\model_episode_26.pth e 4.7.2\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -991.75, Win Rate: 0.52, Wins: 1401, Losses: 1293, Epsilon: 0.3812, Steps: 36754, Time: 112.24s\n",
      "Ações: Manter=11045, Comprar=12272, Vender=13437\n",
      "Ganhos Totais: 38108.75, Perdas Totais: -39100.50\n",
      "Episode 28/100, Total Reward: -1817.75, Win Rate: 0.50, Wins: 1300, Losses: 1285, Epsilon: 0.3774, Steps: 36754, Time: 110.65s\n",
      "Ações: Manter=12300, Comprar=11707, Vender=12747\n",
      "Ganhos Totais: 35433.00, Perdas Totais: -37250.75\n",
      "Episode 29/100, Total Reward: -5167.00, Win Rate: 0.50, Wins: 1307, Losses: 1300, Epsilon: 0.3736, Steps: 36754, Time: 111.21s\n",
      "Ações: Manter=13221, Comprar=10942, Vender=12591\n",
      "Ganhos Totais: 34993.00, Perdas Totais: -40160.00\n",
      "Episode 30/100, Total Reward: 1887.25, Win Rate: 0.53, Wins: 1381, Losses: 1218, Epsilon: 0.3699, Steps: 36754, Time: 110.76s\n",
      "Ações: Manter=13150, Comprar=11492, Vender=12112\n",
      "Ganhos Totais: 37738.75, Perdas Totais: -35851.50\n",
      "Modelo e log do episódio 30 salvos em: 4.7.2\\model_episode_30.pth e 4.7.2\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -1022.25, Win Rate: 0.52, Wins: 1327, Losses: 1202, Epsilon: 0.3662, Steps: 36754, Time: 110.64s\n",
      "Ações: Manter=11258, Comprar=11539, Vender=13957\n",
      "Ganhos Totais: 36305.25, Perdas Totais: -37327.50\n",
      "Episode 32/100, Total Reward: -1814.00, Win Rate: 0.50, Wins: 1173, Losses: 1172, Epsilon: 0.3625, Steps: 36754, Time: 110.88s\n",
      "Ações: Manter=16107, Comprar=9193, Vender=11454\n",
      "Ganhos Totais: 33649.75, Perdas Totais: -35463.75\n",
      "Episode 33/100, Total Reward: -2559.50, Win Rate: 0.52, Wins: 1306, Losses: 1210, Epsilon: 0.3589, Steps: 36754, Time: 110.63s\n",
      "Ações: Manter=11673, Comprar=11263, Vender=13818\n",
      "Ganhos Totais: 35501.00, Perdas Totais: -38060.50\n",
      "Episode 34/100, Total Reward: -3369.50, Win Rate: 0.52, Wins: 1335, Losses: 1233, Epsilon: 0.3553, Steps: 36754, Time: 112.84s\n",
      "Ações: Manter=10693, Comprar=12050, Vender=14011\n",
      "Ganhos Totais: 35270.00, Perdas Totais: -38639.50\n",
      "Episode 35/100, Total Reward: -2005.25, Win Rate: 0.53, Wins: 1316, Losses: 1184, Epsilon: 0.3517, Steps: 36754, Time: 111.06s\n",
      "Ações: Manter=11508, Comprar=10969, Vender=14277\n",
      "Ganhos Totais: 36245.00, Perdas Totais: -38250.25\n",
      "Episode 36/100, Total Reward: -1851.00, Win Rate: 0.53, Wins: 1271, Losses: 1132, Epsilon: 0.3482, Steps: 36754, Time: 111.16s\n",
      "Ações: Manter=13393, Comprar=9907, Vender=13454\n",
      "Ganhos Totais: 34913.25, Perdas Totais: -36764.25\n",
      "Episode 37/100, Total Reward: 1492.75, Win Rate: 0.52, Wins: 1301, Losses: 1196, Epsilon: 0.3447, Steps: 36754, Time: 110.80s\n",
      "Ações: Manter=12803, Comprar=11667, Vender=12284\n",
      "Ganhos Totais: 35936.25, Perdas Totais: -34443.50\n",
      "Modelo e log do episódio 37 salvos em: 4.7.2\\model_episode_37.pth e 4.7.2\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: -1434.75, Win Rate: 0.51, Wins: 1251, Losses: 1219, Epsilon: 0.3413, Steps: 36754, Time: 110.83s\n",
      "Ações: Manter=11348, Comprar=12906, Vender=12500\n",
      "Ganhos Totais: 35759.00, Perdas Totais: -37193.75\n",
      "Episode 39/100, Total Reward: 5258.25, Win Rate: 0.55, Wins: 1413, Losses: 1159, Epsilon: 0.3379, Steps: 36754, Time: 111.13s\n",
      "Ações: Manter=11128, Comprar=14958, Vender=10668\n",
      "Ganhos Totais: 39273.25, Perdas Totais: -34015.00\n",
      "Modelo e log do episódio 39 salvos em: 4.7.2\\model_episode_39.pth e 4.7.2\\log_episode_39.csv\n",
      "\n",
      "Episode 40/100, Total Reward: -576.25, Win Rate: 0.53, Wins: 1336, Losses: 1162, Epsilon: 0.3345, Steps: 36754, Time: 110.97s\n",
      "Ações: Manter=13585, Comprar=11553, Vender=11616\n",
      "Ganhos Totais: 35765.50, Perdas Totais: -36341.75\n",
      "Episode 41/100, Total Reward: -4971.00, Win Rate: 0.50, Wins: 1145, Losses: 1148, Epsilon: 0.3311, Steps: 36754, Time: 111.21s\n",
      "Ações: Manter=15293, Comprar=9941, Vender=11520\n",
      "Ganhos Totais: 32868.75, Perdas Totais: -37839.75\n",
      "Episode 42/100, Total Reward: -1994.25, Win Rate: 0.51, Wins: 1276, Losses: 1214, Epsilon: 0.3278, Steps: 36754, Time: 111.21s\n",
      "Ações: Manter=11928, Comprar=12670, Vender=12156\n",
      "Ganhos Totais: 36813.50, Perdas Totais: -38807.75\n",
      "Episode 43/100, Total Reward: -3851.00, Win Rate: 0.50, Wins: 1208, Losses: 1212, Epsilon: 0.3246, Steps: 36754, Time: 111.24s\n",
      "Ações: Manter=10588, Comprar=14241, Vender=11925\n",
      "Ganhos Totais: 33593.00, Perdas Totais: -37444.00\n",
      "Episode 44/100, Total Reward: -1407.75, Win Rate: 0.51, Wins: 1230, Losses: 1193, Epsilon: 0.3213, Steps: 36754, Time: 111.46s\n",
      "Ações: Manter=11529, Comprar=13460, Vender=11765\n",
      "Ganhos Totais: 35350.00, Perdas Totais: -36757.75\n",
      "Episode 45/100, Total Reward: -2737.00, Win Rate: 0.51, Wins: 1233, Losses: 1203, Epsilon: 0.3181, Steps: 36754, Time: 111.45s\n",
      "Ações: Manter=11133, Comprar=13941, Vender=11680\n",
      "Ganhos Totais: 34724.25, Perdas Totais: -37461.25\n",
      "Episode 46/100, Total Reward: -5557.50, Win Rate: 0.49, Wins: 1080, Losses: 1137, Epsilon: 0.3149, Steps: 36754, Time: 111.36s\n",
      "Ações: Manter=12932, Comprar=10945, Vender=12877\n",
      "Ganhos Totais: 31427.50, Perdas Totais: -36985.00\n",
      "Episode 47/100, Total Reward: -2466.00, Win Rate: 0.52, Wins: 1200, Losses: 1123, Epsilon: 0.3118, Steps: 36754, Time: 111.27s\n",
      "Ações: Manter=12189, Comprar=12097, Vender=12468\n",
      "Ganhos Totais: 33773.00, Perdas Totais: -36239.00\n",
      "Episode 48/100, Total Reward: 670.50, Win Rate: 0.53, Wins: 1255, Losses: 1134, Epsilon: 0.3086, Steps: 36754, Time: 111.39s\n",
      "Ações: Manter=13102, Comprar=11797, Vender=11855\n",
      "Ganhos Totais: 36094.25, Perdas Totais: -35423.75\n",
      "Episode 49/100, Total Reward: -237.00, Win Rate: 0.51, Wins: 1196, Losses: 1150, Epsilon: 0.3056, Steps: 36754, Time: 111.55s\n",
      "Ações: Manter=12366, Comprar=11541, Vender=12847\n",
      "Ganhos Totais: 36177.25, Perdas Totais: -36414.25\n",
      "Episode 50/100, Total Reward: 432.00, Win Rate: 0.53, Wins: 1211, Losses: 1062, Epsilon: 0.3025, Steps: 36754, Time: 111.83s\n",
      "Ações: Manter=12555, Comprar=12095, Vender=12104\n",
      "Ganhos Totais: 34742.00, Perdas Totais: -34310.00\n",
      "Episode 51/100, Total Reward: 4206.75, Win Rate: 0.54, Wins: 1221, Losses: 1028, Epsilon: 0.2995, Steps: 36754, Time: 111.69s\n",
      "Ações: Manter=13086, Comprar=11950, Vender=11718\n",
      "Ganhos Totais: 36225.50, Perdas Totais: -32018.75\n",
      "Modelo e log do episódio 51 salvos em: 4.7.2\\model_episode_51.pth e 4.7.2\\log_episode_51.csv\n",
      "\n",
      "Episode 52/100, Total Reward: -3287.00, Win Rate: 0.52, Wins: 1124, Losses: 1051, Epsilon: 0.2965, Steps: 36754, Time: 111.85s\n",
      "Ações: Manter=11415, Comprar=10458, Vender=14881\n",
      "Ganhos Totais: 33376.00, Perdas Totais: -36663.00\n",
      "Episode 53/100, Total Reward: -6234.75, Win Rate: 0.50, Wins: 1044, Losses: 1051, Epsilon: 0.2935, Steps: 36754, Time: 111.58s\n",
      "Ações: Manter=13444, Comprar=10162, Vender=13148\n",
      "Ganhos Totais: 31378.75, Perdas Totais: -37613.50\n",
      "Episode 54/100, Total Reward: -2471.50, Win Rate: 0.52, Wins: 1189, Losses: 1108, Epsilon: 0.2906, Steps: 36754, Time: 111.74s\n",
      "Ações: Manter=12983, Comprar=11396, Vender=12375\n",
      "Ganhos Totais: 34865.00, Perdas Totais: -37336.50\n",
      "Episode 55/100, Total Reward: -5860.00, Win Rate: 0.50, Wins: 1119, Losses: 1097, Epsilon: 0.2877, Steps: 36754, Time: 111.74s\n",
      "Ações: Manter=12312, Comprar=9971, Vender=14471\n",
      "Ganhos Totais: 32835.75, Perdas Totais: -38695.75\n",
      "Episode 56/100, Total Reward: -214.75, Win Rate: 0.52, Wins: 1252, Losses: 1144, Epsilon: 0.2848, Steps: 36754, Time: 111.95s\n",
      "Ações: Manter=11554, Comprar=11265, Vender=13935\n",
      "Ganhos Totais: 36370.75, Perdas Totais: -36585.50\n",
      "Episode 57/100, Total Reward: -1367.50, Win Rate: 0.53, Wins: 1161, Losses: 1018, Epsilon: 0.2820, Steps: 36754, Time: 111.97s\n",
      "Ações: Manter=13879, Comprar=11042, Vender=11833\n",
      "Ganhos Totais: 32940.25, Perdas Totais: -34307.75\n",
      "Episode 58/100, Total Reward: -2066.00, Win Rate: 0.53, Wins: 1216, Losses: 1062, Epsilon: 0.2791, Steps: 36754, Time: 112.05s\n",
      "Ações: Manter=12767, Comprar=11595, Vender=12392\n",
      "Ganhos Totais: 34055.75, Perdas Totais: -36121.75\n",
      "Episode 59/100, Total Reward: -4454.50, Win Rate: 0.52, Wins: 1151, Losses: 1062, Epsilon: 0.2763, Steps: 36754, Time: 112.82s\n",
      "Ações: Manter=13178, Comprar=11247, Vender=12329\n",
      "Ganhos Totais: 32788.75, Perdas Totais: -37243.25\n",
      "Episode 60/100, Total Reward: -293.25, Win Rate: 0.52, Wins: 1132, Losses: 1059, Epsilon: 0.2736, Steps: 36754, Time: 112.42s\n",
      "Ações: Manter=13485, Comprar=11166, Vender=12103\n",
      "Ganhos Totais: 35714.50, Perdas Totais: -36007.75\n",
      "Episode 61/100, Total Reward: -3917.50, Win Rate: 0.52, Wins: 1198, Losses: 1119, Epsilon: 0.2708, Steps: 36754, Time: 112.94s\n",
      "Ações: Manter=11496, Comprar=11866, Vender=13392\n",
      "Ganhos Totais: 35294.75, Perdas Totais: -39212.25\n",
      "Episode 62/100, Total Reward: -5956.75, Win Rate: 0.52, Wins: 1135, Losses: 1063, Epsilon: 0.2681, Steps: 36754, Time: 112.27s\n",
      "Ações: Manter=11289, Comprar=12511, Vender=12954\n",
      "Ganhos Totais: 32142.50, Perdas Totais: -38099.25\n",
      "Episode 63/100, Total Reward: 4297.00, Win Rate: 0.54, Wins: 1152, Losses: 982, Epsilon: 0.2655, Steps: 36754, Time: 112.24s\n",
      "Ações: Manter=12116, Comprar=12637, Vender=12001\n",
      "Ganhos Totais: 36968.25, Perdas Totais: -32671.25\n",
      "Modelo e log do episódio 63 salvos em: 4.7.2\\model_episode_63.pth e 4.7.2\\log_episode_63.csv\n",
      "\n",
      "Episode 64/100, Total Reward: 421.75, Win Rate: 0.53, Wins: 1134, Losses: 1017, Epsilon: 0.2628, Steps: 36754, Time: 112.25s\n",
      "Ações: Manter=11274, Comprar=12074, Vender=13406\n",
      "Ganhos Totais: 36236.50, Perdas Totais: -35814.75\n",
      "Episode 65/100, Total Reward: -1730.25, Win Rate: 0.51, Wins: 1099, Losses: 1061, Epsilon: 0.2602, Steps: 36754, Time: 112.21s\n",
      "Ações: Manter=10719, Comprar=10476, Vender=15559\n",
      "Ganhos Totais: 34356.50, Perdas Totais: -36086.75\n",
      "Episode 66/100, Total Reward: 818.50, Win Rate: 0.53, Wins: 1153, Losses: 1041, Epsilon: 0.2576, Steps: 36754, Time: 112.27s\n",
      "Ações: Manter=11633, Comprar=12500, Vender=12621\n",
      "Ganhos Totais: 35850.25, Perdas Totais: -35031.75\n",
      "Episode 67/100, Total Reward: -1623.75, Win Rate: 0.52, Wins: 1133, Losses: 1035, Epsilon: 0.2550, Steps: 36754, Time: 112.42s\n",
      "Ações: Manter=12999, Comprar=10901, Vender=12854\n",
      "Ganhos Totais: 34429.25, Perdas Totais: -36053.00\n",
      "Episode 68/100, Total Reward: -1968.50, Win Rate: 0.52, Wins: 1103, Losses: 1000, Epsilon: 0.2524, Steps: 36754, Time: 112.44s\n",
      "Ações: Manter=11073, Comprar=13397, Vender=12284\n",
      "Ganhos Totais: 34828.75, Perdas Totais: -36797.25\n",
      "Episode 69/100, Total Reward: -1317.00, Win Rate: 0.52, Wins: 1234, Losses: 1123, Epsilon: 0.2499, Steps: 36754, Time: 112.42s\n",
      "Ações: Manter=10069, Comprar=12426, Vender=14259\n",
      "Ganhos Totais: 35529.75, Perdas Totais: -36846.75\n",
      "Episode 70/100, Total Reward: -3971.00, Win Rate: 0.51, Wins: 1056, Losses: 1005, Epsilon: 0.2474, Steps: 36754, Time: 112.53s\n",
      "Ações: Manter=13023, Comprar=11402, Vender=12329\n",
      "Ganhos Totais: 32408.25, Perdas Totais: -36379.25\n",
      "Episode 71/100, Total Reward: -4236.75, Win Rate: 0.53, Wins: 1195, Losses: 1055, Epsilon: 0.2449, Steps: 36754, Time: 113.09s\n",
      "Ações: Manter=11592, Comprar=12297, Vender=12865\n",
      "Ganhos Totais: 33677.75, Perdas Totais: -37914.50\n",
      "Episode 72/100, Total Reward: -1320.25, Win Rate: 0.54, Wins: 1155, Losses: 970, Epsilon: 0.2425, Steps: 36754, Time: 112.64s\n",
      "Ações: Manter=12373, Comprar=11078, Vender=13303\n",
      "Ganhos Totais: 34983.75, Perdas Totais: -36304.00\n",
      "Episode 73/100, Total Reward: -2449.00, Win Rate: 0.52, Wins: 1110, Losses: 1005, Epsilon: 0.2401, Steps: 36754, Time: 112.66s\n",
      "Ações: Manter=13499, Comprar=10241, Vender=13014\n",
      "Ganhos Totais: 33526.75, Perdas Totais: -35975.75\n",
      "Episode 74/100, Total Reward: -3596.25, Win Rate: 0.51, Wins: 1049, Losses: 990, Epsilon: 0.2377, Steps: 36754, Time: 112.96s\n",
      "Ações: Manter=14458, Comprar=10947, Vender=11349\n",
      "Ganhos Totais: 32538.00, Perdas Totais: -36134.25\n",
      "Episode 75/100, Total Reward: -862.00, Win Rate: 0.52, Wins: 1154, Losses: 1059, Epsilon: 0.2353, Steps: 36754, Time: 112.87s\n",
      "Ações: Manter=11477, Comprar=11085, Vender=14192\n",
      "Ganhos Totais: 35023.50, Perdas Totais: -35885.50\n",
      "Episode 76/100, Total Reward: -1315.25, Win Rate: 0.52, Wins: 1131, Losses: 1053, Epsilon: 0.2329, Steps: 36754, Time: 112.78s\n",
      "Ações: Manter=13625, Comprar=10899, Vender=12230\n",
      "Ganhos Totais: 33127.75, Perdas Totais: -34443.00\n",
      "Episode 77/100, Total Reward: 2232.75, Win Rate: 0.54, Wins: 1303, Losses: 1096, Epsilon: 0.2306, Steps: 36754, Time: 113.05s\n",
      "Ações: Manter=11437, Comprar=9981, Vender=15336\n",
      "Ganhos Totais: 37475.75, Perdas Totais: -35243.00\n",
      "Modelo e log do episódio 77 salvos em: 4.7.2\\model_episode_77.pth e 4.7.2\\log_episode_77.csv\n",
      "\n",
      "Episode 78/100, Total Reward: 1772.25, Win Rate: 0.52, Wins: 1108, Losses: 1016, Epsilon: 0.2283, Steps: 36754, Time: 112.72s\n",
      "Ações: Manter=12321, Comprar=10807, Vender=13626\n",
      "Ganhos Totais: 34507.25, Perdas Totais: -32735.00\n",
      "Modelo e log do episódio 78 salvos em: 4.7.2\\model_episode_78.pth e 4.7.2\\log_episode_78.csv\n",
      "\n",
      "Episode 79/100, Total Reward: -4444.00, Win Rate: 0.52, Wins: 1147, Losses: 1055, Epsilon: 0.2260, Steps: 36754, Time: 112.59s\n",
      "Ações: Manter=13349, Comprar=10081, Vender=13324\n",
      "Ganhos Totais: 33527.25, Perdas Totais: -37971.25\n",
      "Episode 80/100, Total Reward: -3831.50, Win Rate: 0.53, Wins: 1051, Losses: 944, Epsilon: 0.2238, Steps: 36754, Time: 112.86s\n",
      "Ações: Manter=15842, Comprar=9154, Vender=11758\n",
      "Ganhos Totais: 31488.25, Perdas Totais: -35319.75\n",
      "Episode 81/100, Total Reward: -671.75, Win Rate: 0.52, Wins: 1111, Losses: 1029, Epsilon: 0.2215, Steps: 36754, Time: 113.32s\n",
      "Ações: Manter=11983, Comprar=9700, Vender=15071\n",
      "Ganhos Totais: 34827.50, Perdas Totais: -35499.25\n",
      "Episode 82/100, Total Reward: -3318.25, Win Rate: 0.53, Wins: 1089, Losses: 982, Epsilon: 0.2193, Steps: 36754, Time: 112.73s\n",
      "Ações: Manter=11485, Comprar=10252, Vender=15017\n",
      "Ganhos Totais: 33383.75, Perdas Totais: -36702.00\n",
      "Episode 83/100, Total Reward: -3180.75, Win Rate: 0.52, Wins: 1052, Losses: 964, Epsilon: 0.2171, Steps: 36754, Time: 112.89s\n",
      "Ações: Manter=13697, Comprar=11455, Vender=11602\n",
      "Ganhos Totais: 31719.50, Perdas Totais: -34900.25\n",
      "Episode 84/100, Total Reward: -2842.75, Win Rate: 0.54, Wins: 1122, Losses: 944, Epsilon: 0.2149, Steps: 36754, Time: 113.22s\n",
      "Ações: Manter=16603, Comprar=8379, Vender=11772\n",
      "Ganhos Totais: 31377.75, Perdas Totais: -34220.50\n",
      "Episode 85/100, Total Reward: -5245.25, Win Rate: 0.53, Wins: 1073, Losses: 963, Epsilon: 0.2128, Steps: 36754, Time: 112.61s\n",
      "Ações: Manter=16518, Comprar=8378, Vender=11858\n",
      "Ganhos Totais: 30859.25, Perdas Totais: -36104.50\n",
      "Episode 86/100, Total Reward: -1621.75, Win Rate: 0.53, Wins: 1075, Losses: 943, Epsilon: 0.2107, Steps: 36754, Time: 112.74s\n",
      "Ações: Manter=13356, Comprar=8854, Vender=14544\n",
      "Ganhos Totais: 33555.75, Perdas Totais: -35177.50\n",
      "Episode 87/100, Total Reward: -1287.50, Win Rate: 0.53, Wins: 1087, Losses: 971, Epsilon: 0.2086, Steps: 36754, Time: 113.23s\n",
      "Ações: Manter=13300, Comprar=10209, Vender=13245\n",
      "Ganhos Totais: 35078.00, Perdas Totais: -36365.50\n",
      "Episode 88/100, Total Reward: -342.25, Win Rate: 0.54, Wins: 1155, Losses: 972, Epsilon: 0.2065, Steps: 36754, Time: 113.22s\n",
      "Ações: Manter=14580, Comprar=9296, Vender=12878\n",
      "Ganhos Totais: 34194.75, Perdas Totais: -34537.00\n",
      "Episode 89/100, Total Reward: -1541.50, Win Rate: 0.53, Wins: 1104, Losses: 992, Epsilon: 0.2044, Steps: 36754, Time: 112.82s\n",
      "Ações: Manter=13257, Comprar=9507, Vender=13990\n",
      "Ganhos Totais: 34230.00, Perdas Totais: -35771.50\n",
      "Episode 90/100, Total Reward: -289.25, Win Rate: 0.54, Wins: 1259, Losses: 1090, Epsilon: 0.2024, Steps: 36754, Time: 112.87s\n",
      "Ações: Manter=10610, Comprar=11155, Vender=14989\n",
      "Ganhos Totais: 35897.75, Perdas Totais: -36187.00\n",
      "Episode 91/100, Total Reward: -2925.50, Win Rate: 0.53, Wins: 1063, Losses: 954, Epsilon: 0.2003, Steps: 36754, Time: 113.99s\n",
      "Ações: Manter=13230, Comprar=10741, Vender=12783\n",
      "Ganhos Totais: 33958.00, Perdas Totais: -36883.50\n",
      "Episode 92/100, Total Reward: -2074.00, Win Rate: 0.53, Wins: 987, Losses: 886, Epsilon: 0.1983, Steps: 36754, Time: 113.00s\n",
      "Ações: Manter=14049, Comprar=8675, Vender=14030\n",
      "Ganhos Totais: 31705.75, Perdas Totais: -33779.75\n",
      "Episode 93/100, Total Reward: -2765.00, Win Rate: 0.54, Wins: 1058, Losses: 907, Epsilon: 0.1964, Steps: 36754, Time: 113.06s\n",
      "Ações: Manter=14778, Comprar=8586, Vender=13390\n",
      "Ganhos Totais: 33337.00, Perdas Totais: -36102.00\n",
      "Episode 94/100, Total Reward: -1065.50, Win Rate: 0.53, Wins: 1050, Losses: 925, Epsilon: 0.1944, Steps: 36754, Time: 113.30s\n",
      "Ações: Manter=12967, Comprar=9491, Vender=14296\n",
      "Ganhos Totais: 33936.75, Perdas Totais: -35002.25\n",
      "Episode 95/100, Total Reward: -233.00, Win Rate: 0.54, Wins: 1051, Losses: 905, Epsilon: 0.1924, Steps: 36754, Time: 112.95s\n",
      "Ações: Manter=14075, Comprar=8405, Vender=14274\n",
      "Ganhos Totais: 33302.50, Perdas Totais: -33535.50\n",
      "Episode 96/100, Total Reward: -2343.75, Win Rate: 0.54, Wins: 1099, Losses: 946, Epsilon: 0.1905, Steps: 36754, Time: 113.10s\n",
      "Ações: Manter=13029, Comprar=10501, Vender=13224\n",
      "Ganhos Totais: 33865.75, Perdas Totais: -36209.50\n",
      "Episode 97/100, Total Reward: -1769.25, Win Rate: 0.55, Wins: 1061, Losses: 854, Epsilon: 0.1886, Steps: 36754, Time: 113.08s\n",
      "Ações: Manter=13022, Comprar=10438, Vender=13294\n",
      "Ganhos Totais: 33555.75, Perdas Totais: -35325.00\n",
      "Episode 98/100, Total Reward: -2211.50, Win Rate: 0.54, Wins: 1042, Losses: 892, Epsilon: 0.1867, Steps: 36754, Time: 112.85s\n",
      "Ações: Manter=14997, Comprar=8715, Vender=13042\n",
      "Ganhos Totais: 32395.25, Perdas Totais: -34606.75\n",
      "Episode 99/100, Total Reward: -832.25, Win Rate: 0.55, Wins: 1056, Losses: 855, Epsilon: 0.1849, Steps: 36754, Time: 112.80s\n",
      "Ações: Manter=13686, Comprar=10609, Vender=12459\n",
      "Ganhos Totais: 34291.50, Perdas Totais: -35123.75\n",
      "Episode 100/100, Total Reward: -2113.50, Win Rate: 0.54, Wins: 1169, Losses: 1008, Epsilon: 0.1830, Steps: 36754, Time: 113.33s\n",
      "Ações: Manter=12088, Comprar=9886, Vender=14780\n",
      "Ganhos Totais: 32871.25, Perdas Totais: -34984.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 39, Total Reward: 5258.25, Win Rate: 0.55, Wins: 1413, Losses: 1159, Ações: {0: 11128, 1: 14958, 2: 10668}, Steps: 36754, Time: 111.13s\n",
      "Rank 2: Episode 63, Total Reward: 4297.00, Win Rate: 0.54, Wins: 1152, Losses: 982, Ações: {0: 12116, 1: 12637, 2: 12001}, Steps: 36754, Time: 112.24s\n",
      "Rank 3: Episode 51, Total Reward: 4206.75, Win Rate: 0.54, Wins: 1221, Losses: 1028, Ações: {0: 13086, 1: 11950, 2: 11718}, Steps: 36754, Time: 111.69s\n",
      "Rank 4: Episode 11, Total Reward: 3962.00, Win Rate: 0.52, Wins: 1393, Losses: 1268, Ações: {0: 12722, 1: 11750, 2: 12282}, Steps: 36754, Time: 125.41s\n",
      "Rank 5: Episode 77, Total Reward: 2232.75, Win Rate: 0.54, Wins: 1303, Losses: 1096, Ações: {0: 11437, 1: 9981, 2: 15336}, Steps: 36754, Time: 113.05s\n",
      "Rank 6: Episode 10, Total Reward: 2072.00, Win Rate: 0.51, Wins: 1351, Losses: 1293, Ações: {0: 12126, 1: 12557, 2: 12071}, Steps: 36754, Time: 124.89s\n",
      "Rank 7: Episode 30, Total Reward: 1887.25, Win Rate: 0.53, Wins: 1381, Losses: 1218, Ações: {0: 13150, 1: 11492, 2: 12112}, Steps: 36754, Time: 110.76s\n",
      "Rank 8: Episode 26, Total Reward: 1807.25, Win Rate: 0.51, Wins: 1373, Losses: 1325, Ações: {0: 12411, 1: 12318, 2: 12025}, Steps: 36754, Time: 110.51s\n",
      "Rank 9: Episode 78, Total Reward: 1772.25, Win Rate: 0.52, Wins: 1108, Losses: 1016, Ações: {0: 12321, 1: 10807, 2: 13626}, Steps: 36754, Time: 112.72s\n",
      "Rank 10: Episode 6, Total Reward: 1632.25, Win Rate: 0.54, Wins: 1447, Losses: 1243, Ações: {0: 11808, 1: 13264, 2: 11682}, Steps: 36754, Time: 124.87s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 20000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
