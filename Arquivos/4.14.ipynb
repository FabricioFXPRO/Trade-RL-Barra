{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -33595.25, Total Real Reward: 380.00, Total Real Profit: 992.50, Win Rate: 0.52, Wins: 1267, Losses: 1173, Epsilon: 0.4950, Steps: 36754, Time: 178.97s\n",
      "Ações: Manter=14326, Comprar=10225, Vender=12203\n",
      "Ganhos Totais: 34967.75, Perdas Totais: -33975.25\n",
      "Modelo e log do episódio 1 salvos em: 4.14\\model_episode_1.pth e 4.14\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -41720.25, Total Real Reward: -3901.75, Total Real Profit: -3324.25, Win Rate: 0.50, Wins: 1158, Losses: 1146, Epsilon: 0.4900, Steps: 36754, Time: 184.50s\n",
      "Ações: Manter=13168, Comprar=10372, Vender=13214\n",
      "Ganhos Totais: 34494.25, Perdas Totais: -37818.50\n",
      "Modelo e log do episódio 2 salvos em: 4.14\\model_episode_2.pth e 4.14\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -38648.50, Total Real Reward: -2206.00, Total Real Profit: -1634.00, Win Rate: 0.51, Wins: 1151, Losses: 1122, Epsilon: 0.4851, Steps: 36754, Time: 189.96s\n",
      "Ações: Manter=13336, Comprar=10773, Vender=12645\n",
      "Ganhos Totais: 34808.50, Perdas Totais: -36442.50\n",
      "Modelo e log do episódio 3 salvos em: 4.14\\model_episode_3.pth e 4.14\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -35215.75, Total Real Reward: -160.25, Total Real Profit: 392.25, Win Rate: 0.51, Wins: 1118, Losses: 1087, Epsilon: 0.4803, Steps: 36754, Time: 187.63s\n",
      "Ações: Manter=12778, Comprar=10972, Vender=13004\n",
      "Ganhos Totais: 35447.75, Perdas Totais: -35055.50\n",
      "Modelo e log do episódio 4 salvos em: 4.14\\model_episode_4.pth e 4.14\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -35807.75, Total Real Reward: -891.75, Total Real Profit: -338.25, Win Rate: 0.52, Wins: 1139, Losses: 1060, Epsilon: 0.4755, Steps: 36754, Time: 188.99s\n",
      "Ações: Manter=13468, Comprar=10584, Vender=12702\n",
      "Ganhos Totais: 34577.75, Perdas Totais: -34916.00\n",
      "Modelo e log do episódio 5 salvos em: 4.14\\model_episode_5.pth e 4.14\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -42351.50, Total Real Reward: -5125.00, Total Real Profit: -4579.25, Win Rate: 0.50, Wins: 1090, Losses: 1083, Epsilon: 0.4707, Steps: 36754, Time: 188.72s\n",
      "Ações: Manter=13189, Comprar=11205, Vender=12360\n",
      "Ganhos Totais: 32647.25, Perdas Totais: -37226.50\n",
      "Modelo e log do episódio 6 salvos em: 4.14\\model_episode_6.pth e 4.14\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -37292.25, Total Real Reward: -1694.25, Total Real Profit: -1137.25, Win Rate: 0.51, Wins: 1133, Losses: 1080, Epsilon: 0.4660, Steps: 36754, Time: 187.26s\n",
      "Ações: Manter=13177, Comprar=10889, Vender=12688\n",
      "Ganhos Totais: 34460.75, Perdas Totais: -35598.00\n",
      "Modelo e log do episódio 7 salvos em: 4.14\\model_episode_7.pth e 4.14\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -40715.75, Total Real Reward: -3681.50, Total Real Profit: -3115.50, Win Rate: 0.51, Wins: 1142, Losses: 1109, Epsilon: 0.4614, Steps: 36754, Time: 189.08s\n",
      "Ações: Manter=12941, Comprar=11389, Vender=12424\n",
      "Ganhos Totais: 33918.75, Perdas Totais: -37034.25\n",
      "Modelo e log do episódio 8 salvos em: 4.14\\model_episode_8.pth e 4.14\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -42990.00, Total Real Reward: -5191.75, Total Real Profit: -4640.75, Win Rate: 0.50, Wins: 1103, Losses: 1094, Epsilon: 0.4568, Steps: 36754, Time: 188.57s\n",
      "Ações: Manter=12969, Comprar=11762, Vender=12023\n",
      "Ganhos Totais: 33157.50, Perdas Totais: -37798.25\n",
      "Modelo e log do episódio 9 salvos em: 4.14\\model_episode_9.pth e 4.14\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -33997.75, Total Real Reward: 398.50, Total Real Profit: 946.25, Win Rate: 0.52, Wins: 1142, Losses: 1035, Epsilon: 0.4522, Steps: 36754, Time: 188.82s\n",
      "Ações: Manter=14022, Comprar=10959, Vender=11773\n",
      "Ganhos Totais: 35342.50, Perdas Totais: -34396.25\n",
      "Modelo e log do episódio 10 salvos em: 4.14\\model_episode_10.pth e 4.14\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -37746.25, Total Real Reward: -1680.75, Total Real Profit: -1112.25, Win Rate: 0.52, Wins: 1183, Losses: 1082, Epsilon: 0.4477, Steps: 36754, Time: 185.87s\n",
      "Ações: Manter=14202, Comprar=10565, Vender=11987\n",
      "Ganhos Totais: 34953.25, Perdas Totais: -36065.50\n",
      "Modelo e log do episódio 11 salvos em: 4.14\\model_episode_11.pth e 4.14\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -41048.25, Total Real Reward: -4342.75, Total Real Profit: -3785.00, Win Rate: 0.50, Wins: 1115, Losses: 1100, Epsilon: 0.4432, Steps: 36754, Time: 184.10s\n",
      "Ações: Manter=13358, Comprar=11427, Vender=11969\n",
      "Ganhos Totais: 32920.50, Perdas Totais: -36705.50\n",
      "Modelo e log do episódio 12 salvos em: 4.14\\model_episode_12.pth e 4.14\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -38010.25, Total Real Reward: -2496.25, Total Real Profit: -1971.50, Win Rate: 0.52, Wins: 1091, Losses: 1000, Epsilon: 0.4388, Steps: 36754, Time: 184.00s\n",
      "Ações: Manter=12465, Comprar=12507, Vender=11782\n",
      "Ganhos Totais: 33542.50, Perdas Totais: -35514.00\n",
      "Modelo e log do episódio 13 salvos em: 4.14\\model_episode_13.pth e 4.14\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -35465.25, Total Real Reward: -1273.50, Total Real Profit: -729.75, Win Rate: 0.51, Wins: 1112, Losses: 1056, Epsilon: 0.4344, Steps: 36754, Time: 184.26s\n",
      "Ações: Manter=13809, Comprar=11512, Vender=11433\n",
      "Ganhos Totais: 33462.00, Perdas Totais: -34191.75\n",
      "Modelo e log do episódio 14 salvos em: 4.14\\model_episode_14.pth e 4.14\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -35793.75, Total Real Reward: -464.25, Total Real Profit: 63.25, Win Rate: 0.51, Wins: 1083, Losses: 1020, Epsilon: 0.4300, Steps: 36754, Time: 185.04s\n",
      "Ações: Manter=13832, Comprar=11331, Vender=11591\n",
      "Ganhos Totais: 35392.75, Perdas Totais: -35329.50\n",
      "Modelo e log do episódio 15 salvos em: 4.14\\model_episode_15.pth e 4.14\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -35316.75, Total Real Reward: -1474.75, Total Real Profit: -949.25, Win Rate: 0.53, Wins: 1104, Losses: 988, Epsilon: 0.4257, Steps: 36754, Time: 184.64s\n",
      "Ações: Manter=14265, Comprar=11670, Vender=10819\n",
      "Ganhos Totais: 32892.75, Perdas Totais: -33842.00\n",
      "Modelo e log do episódio 16 salvos em: 4.14\\model_episode_16.pth e 4.14\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -35190.75, Total Real Reward: -735.75, Total Real Profit: -214.25, Win Rate: 0.51, Wins: 1058, Losses: 1017, Epsilon: 0.4215, Steps: 36754, Time: 185.77s\n",
      "Ações: Manter=12756, Comprar=12735, Vender=11263\n",
      "Ganhos Totais: 34240.75, Perdas Totais: -34455.00\n",
      "Modelo e log do episódio 17 salvos em: 4.14\\model_episode_17.pth e 4.14\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -37743.50, Total Real Reward: -2657.25, Total Real Profit: -2146.75, Win Rate: 0.50, Wins: 1029, Losses: 1009, Epsilon: 0.4173, Steps: 36754, Time: 185.24s\n",
      "Ações: Manter=13694, Comprar=10891, Vender=12169\n",
      "Ganhos Totais: 32939.50, Perdas Totais: -35086.25\n",
      "Modelo e log do episódio 18 salvos em: 4.14\\model_episode_18.pth e 4.14\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -37308.00, Total Real Reward: -1979.75, Total Real Profit: -1440.50, Win Rate: 0.51, Wins: 1098, Losses: 1051, Epsilon: 0.4131, Steps: 36754, Time: 184.00s\n",
      "Ações: Manter=14124, Comprar=10324, Vender=12306\n",
      "Ganhos Totais: 33887.75, Perdas Totais: -35328.25\n",
      "Modelo e log do episódio 19 salvos em: 4.14\\model_episode_19.pth e 4.14\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: -39914.75, Total Real Reward: -4149.25, Total Real Profit: -3615.50, Win Rate: 0.53, Wins: 1115, Losses: 1008, Epsilon: 0.4090, Steps: 36754, Time: 184.29s\n",
      "Ações: Manter=13471, Comprar=11378, Vender=11905\n",
      "Ganhos Totais: 32150.00, Perdas Totais: -35765.50\n",
      "Episode 21/100, Total Reward: -34761.25, Total Real Reward: 381.00, Total Real Profit: 908.75, Win Rate: 0.52, Wins: 1101, Losses: 1002, Epsilon: 0.4049, Steps: 36754, Time: 183.38s\n",
      "Ações: Manter=13142, Comprar=11500, Vender=12112\n",
      "Ganhos Totais: 36051.00, Perdas Totais: -35142.25\n",
      "Modelo e log do episódio 21 salvos em: 4.14\\model_episode_21.pth e 4.14\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -33119.00, Total Real Reward: 133.00, Total Real Profit: 669.50, Win Rate: 0.52, Wins: 1115, Losses: 1021, Epsilon: 0.4008, Steps: 36754, Time: 179.81s\n",
      "Ações: Manter=14223, Comprar=10623, Vender=11908\n",
      "Ganhos Totais: 33921.50, Perdas Totais: -33252.00\n",
      "Modelo e log do episódio 22 salvos em: 4.14\\model_episode_22.pth e 4.14\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: -37500.25, Total Real Reward: -2530.75, Total Real Profit: -2001.00, Win Rate: 0.51, Wins: 1071, Losses: 1041, Epsilon: 0.3968, Steps: 36754, Time: 182.81s\n",
      "Ações: Manter=14066, Comprar=10962, Vender=11726\n",
      "Ganhos Totais: 32968.50, Perdas Totais: -34969.50\n",
      "Episode 24/100, Total Reward: -35373.75, Total Real Reward: -555.25, Total Real Profit: -29.25, Win Rate: 0.53, Wins: 1106, Losses: 985, Epsilon: 0.3928, Steps: 36754, Time: 176.31s\n",
      "Ações: Manter=14504, Comprar=11133, Vender=11117\n",
      "Ganhos Totais: 34789.25, Perdas Totais: -34818.50\n",
      "Modelo e log do episódio 24 salvos em: 4.14\\model_episode_24.pth e 4.14\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -34996.25, Total Real Reward: -1143.25, Total Real Profit: -628.00, Win Rate: 0.51, Wins: 1056, Losses: 997, Epsilon: 0.3889, Steps: 36754, Time: 177.19s\n",
      "Ações: Manter=14998, Comprar=10330, Vender=11426\n",
      "Ganhos Totais: 33225.00, Perdas Totais: -33853.00\n",
      "Modelo e log do episódio 25 salvos em: 4.14\\model_episode_25.pth e 4.14\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -36834.00, Total Real Reward: -1160.00, Total Real Profit: -636.25, Win Rate: 0.52, Wins: 1087, Losses: 1002, Epsilon: 0.3850, Steps: 36754, Time: 178.80s\n",
      "Ações: Manter=12809, Comprar=11646, Vender=12299\n",
      "Ganhos Totais: 35037.75, Perdas Totais: -35674.00\n",
      "Episode 27/100, Total Reward: -38230.50, Total Real Reward: -2888.50, Total Real Profit: -2384.25, Win Rate: 0.53, Wins: 1055, Losses: 953, Epsilon: 0.3812, Steps: 36754, Time: 182.52s\n",
      "Ações: Manter=14615, Comprar=10541, Vender=11598\n",
      "Ganhos Totais: 32957.75, Perdas Totais: -35342.00\n",
      "Episode 28/100, Total Reward: -31355.00, Total Real Reward: 2292.50, Total Real Profit: 2812.25, Win Rate: 0.53, Wins: 1094, Losses: 975, Epsilon: 0.3774, Steps: 36754, Time: 178.42s\n",
      "Ações: Manter=13459, Comprar=11289, Vender=12006\n",
      "Ganhos Totais: 36459.75, Perdas Totais: -33647.50\n",
      "Modelo e log do episódio 28 salvos em: 4.14\\model_episode_28.pth e 4.14\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: -35979.25, Total Real Reward: -821.75, Total Real Profit: -299.50, Win Rate: 0.54, Wins: 1115, Losses: 966, Epsilon: 0.3736, Steps: 36754, Time: 179.67s\n",
      "Ações: Manter=14313, Comprar=9761, Vender=12680\n",
      "Ganhos Totais: 34858.00, Perdas Totais: -35157.50\n",
      "Episode 30/100, Total Reward: -34302.25, Total Real Reward: -168.50, Total Real Profit: 343.50, Win Rate: 0.53, Wins: 1071, Losses: 967, Epsilon: 0.3699, Steps: 36754, Time: 165.86s\n",
      "Ações: Manter=14029, Comprar=11480, Vender=11245\n",
      "Ganhos Totais: 34477.25, Perdas Totais: -34133.75\n",
      "Modelo e log do episódio 30 salvos em: 4.14\\model_episode_30.pth e 4.14\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -30645.00, Total Real Reward: 2124.25, Total Real Profit: 2611.25, Win Rate: 0.54, Wins: 1050, Losses: 890, Epsilon: 0.3662, Steps: 36754, Time: 136.36s\n",
      "Ações: Manter=14711, Comprar=9808, Vender=12235\n",
      "Ganhos Totais: 35380.50, Perdas Totais: -32769.25\n",
      "Modelo e log do episódio 31 salvos em: 4.14\\model_episode_31.pth e 4.14\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: -36681.00, Total Real Reward: -1950.50, Total Real Profit: -1461.50, Win Rate: 0.52, Wins: 1022, Losses: 930, Epsilon: 0.3625, Steps: 36754, Time: 147.49s\n",
      "Ações: Manter=14201, Comprar=10144, Vender=12409\n",
      "Ganhos Totais: 33269.00, Perdas Totais: -34730.50\n",
      "Episode 33/100, Total Reward: -38438.50, Total Real Reward: -2635.25, Total Real Profit: -2152.50, Win Rate: 0.52, Wins: 997, Losses: 927, Epsilon: 0.3589, Steps: 36754, Time: 147.80s\n",
      "Ações: Manter=13509, Comprar=10994, Vender=12251\n",
      "Ganhos Totais: 33650.75, Perdas Totais: -35803.25\n",
      "Episode 34/100, Total Reward: -38823.25, Total Real Reward: -3090.50, Total Real Profit: -2621.50, Win Rate: 0.51, Wins: 950, Losses: 922, Epsilon: 0.3553, Steps: 36754, Time: 147.55s\n",
      "Ações: Manter=14461, Comprar=10839, Vender=11454\n",
      "Ganhos Totais: 33111.25, Perdas Totais: -35732.75\n",
      "Episode 35/100, Total Reward: -41191.75, Total Real Reward: -5247.25, Total Real Profit: -4779.25, Win Rate: 0.51, Wins: 960, Losses: 905, Epsilon: 0.3517, Steps: 36754, Time: 147.50s\n",
      "Ações: Manter=14865, Comprar=11177, Vender=10712\n",
      "Ganhos Totais: 31165.25, Perdas Totais: -35944.50\n",
      "Episode 36/100, Total Reward: -40122.25, Total Real Reward: -4247.75, Total Real Profit: -3795.00, Win Rate: 0.48, Wins: 872, Losses: 928, Epsilon: 0.3482, Steps: 36754, Time: 147.81s\n",
      "Ações: Manter=14493, Comprar=10779, Vender=11482\n",
      "Ganhos Totais: 32079.50, Perdas Totais: -35874.50\n",
      "Episode 37/100, Total Reward: -35352.25, Total Real Reward: -1833.75, Total Real Profit: -1355.00, Win Rate: 0.53, Wins: 1002, Losses: 903, Epsilon: 0.3447, Steps: 36754, Time: 147.63s\n",
      "Ações: Manter=15113, Comprar=9835, Vender=11806\n",
      "Ganhos Totais: 32163.50, Perdas Totais: -33518.50\n",
      "Episode 38/100, Total Reward: -36215.75, Total Real Reward: -1953.50, Total Real Profit: -1489.25, Win Rate: 0.52, Wins: 965, Losses: 883, Epsilon: 0.3413, Steps: 36754, Time: 147.94s\n",
      "Ações: Manter=14018, Comprar=11052, Vender=11684\n",
      "Ganhos Totais: 32773.00, Perdas Totais: -34262.25\n",
      "Episode 39/100, Total Reward: -41264.00, Total Real Reward: -5750.75, Total Real Profit: -5300.00, Win Rate: 0.50, Wins: 902, Losses: 891, Epsilon: 0.3379, Steps: 36754, Time: 147.75s\n",
      "Ações: Manter=14584, Comprar=9610, Vender=12560\n",
      "Ganhos Totais: 30213.25, Perdas Totais: -35513.25\n",
      "Episode 40/100, Total Reward: -36039.50, Total Real Reward: -1157.25, Total Real Profit: -666.50, Win Rate: 0.53, Wins: 1027, Losses: 924, Epsilon: 0.3345, Steps: 36754, Time: 147.80s\n",
      "Ações: Manter=12997, Comprar=10678, Vender=13079\n",
      "Ganhos Totais: 34215.75, Perdas Totais: -34882.25\n",
      "Episode 41/100, Total Reward: -34221.50, Total Real Reward: -1600.25, Total Real Profit: -1167.00, Win Rate: 0.51, Wins: 883, Losses: 843, Epsilon: 0.3311, Steps: 36754, Time: 148.92s\n",
      "Ações: Manter=14866, Comprar=10489, Vender=11399\n",
      "Ganhos Totais: 31454.25, Perdas Totais: -32621.25\n",
      "Modelo e log do episódio 41 salvos em: 4.14\\model_episode_41.pth e 4.14\\log_episode_41.csv\n",
      "\n",
      "Episode 42/100, Total Reward: -37692.75, Total Real Reward: -2758.00, Total Real Profit: -2297.25, Win Rate: 0.52, Wins: 953, Losses: 877, Epsilon: 0.3278, Steps: 36754, Time: 147.59s\n",
      "Ações: Manter=12186, Comprar=11620, Vender=12948\n",
      "Ganhos Totais: 32637.50, Perdas Totais: -34934.75\n",
      "Episode 43/100, Total Reward: -33777.75, Total Real Reward: -529.50, Total Real Profit: -70.75, Win Rate: 0.53, Wins: 962, Losses: 865, Epsilon: 0.3246, Steps: 36754, Time: 148.10s\n",
      "Ações: Manter=13850, Comprar=10940, Vender=11964\n",
      "Ganhos Totais: 33177.50, Perdas Totais: -33248.25\n",
      "Modelo e log do episódio 43 salvos em: 4.14\\model_episode_43.pth e 4.14\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -38383.50, Total Real Reward: -3543.25, Total Real Profit: -3088.00, Win Rate: 0.52, Wins: 939, Losses: 876, Epsilon: 0.3213, Steps: 36754, Time: 147.96s\n",
      "Ações: Manter=13330, Comprar=9950, Vender=13474\n",
      "Ganhos Totais: 31752.25, Perdas Totais: -34840.25\n",
      "Episode 45/100, Total Reward: -32235.25, Total Real Reward: 144.00, Total Real Profit: 608.25, Win Rate: 0.53, Wins: 986, Losses: 866, Epsilon: 0.3181, Steps: 36754, Time: 147.86s\n",
      "Ações: Manter=14964, Comprar=11218, Vender=10572\n",
      "Ganhos Totais: 32987.50, Perdas Totais: -32379.25\n",
      "Modelo e log do episódio 45 salvos em: 4.14\\model_episode_45.pth e 4.14\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: -28048.75, Total Real Reward: 3387.25, Total Real Profit: 3841.25, Win Rate: 0.52, Wins: 949, Losses: 862, Epsilon: 0.3149, Steps: 36754, Time: 147.98s\n",
      "Ações: Manter=14229, Comprar=9608, Vender=12917\n",
      "Ganhos Totais: 35277.25, Perdas Totais: -31436.00\n",
      "Modelo e log do episódio 46 salvos em: 4.14\\model_episode_46.pth e 4.14\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: -34242.25, Total Real Reward: -150.50, Total Real Profit: 298.25, Win Rate: 0.52, Wins: 925, Losses: 861, Epsilon: 0.3118, Steps: 36754, Time: 149.08s\n",
      "Ações: Manter=14698, Comprar=9655, Vender=12401\n",
      "Ganhos Totais: 34390.00, Perdas Totais: -34091.75\n",
      "Modelo e log do episódio 47 salvos em: 4.14\\model_episode_47.pth e 4.14\\log_episode_47.csv\n",
      "\n",
      "Episode 48/100, Total Reward: -39265.75, Total Real Reward: -3024.00, Total Real Profit: -2526.50, Win Rate: 0.53, Wins: 1050, Losses: 930, Epsilon: 0.3086, Steps: 36754, Time: 141.80s\n",
      "Ações: Manter=13283, Comprar=11003, Vender=12468\n",
      "Ganhos Totais: 33715.25, Perdas Totais: -36241.75\n",
      "Episode 49/100, Total Reward: -31499.50, Total Real Reward: 225.25, Total Real Profit: 657.00, Win Rate: 0.54, Wins: 927, Losses: 791, Epsilon: 0.3056, Steps: 36754, Time: 150.31s\n",
      "Ações: Manter=15569, Comprar=9991, Vender=11194\n",
      "Ganhos Totais: 32381.75, Perdas Totais: -31724.75\n",
      "Modelo e log do episódio 49 salvos em: 4.14\\model_episode_49.pth e 4.14\\log_episode_49.csv\n",
      "\n",
      "Episode 50/100, Total Reward: -36995.00, Total Real Reward: -2595.25, Total Real Profit: -2147.50, Win Rate: 0.52, Wins: 928, Losses: 857, Epsilon: 0.3025, Steps: 36754, Time: 148.81s\n",
      "Ações: Manter=15352, Comprar=9533, Vender=11869\n",
      "Ganhos Totais: 32252.25, Perdas Totais: -34399.75\n",
      "Episode 51/100, Total Reward: -36124.25, Total Real Reward: -2748.50, Total Real Profit: -2309.50, Win Rate: 0.52, Wins: 911, Losses: 843, Epsilon: 0.2995, Steps: 36754, Time: 148.79s\n",
      "Ações: Manter=14811, Comprar=10167, Vender=11776\n",
      "Ganhos Totais: 31066.25, Perdas Totais: -33375.75\n",
      "Episode 52/100, Total Reward: -34685.50, Total Real Reward: -1596.50, Total Real Profit: -1159.00, Win Rate: 0.53, Wins: 914, Losses: 826, Epsilon: 0.2965, Steps: 36754, Time: 148.79s\n",
      "Ações: Manter=15279, Comprar=10140, Vender=11335\n",
      "Ganhos Totais: 31930.00, Perdas Totais: -33089.00\n",
      "Episode 53/100, Total Reward: -35376.50, Total Real Reward: -1759.25, Total Real Profit: -1285.75, Win Rate: 0.52, Wins: 985, Losses: 899, Epsilon: 0.2935, Steps: 36754, Time: 148.70s\n",
      "Ações: Manter=14914, Comprar=9888, Vender=11952\n",
      "Ganhos Totais: 32331.50, Perdas Totais: -33617.25\n",
      "Episode 54/100, Total Reward: -38635.25, Total Real Reward: -4892.25, Total Real Profit: -4469.25, Win Rate: 0.52, Wins: 869, Losses: 811, Epsilon: 0.2906, Steps: 36754, Time: 149.12s\n",
      "Ações: Manter=16256, Comprar=10560, Vender=9938\n",
      "Ganhos Totais: 29273.75, Perdas Totais: -33743.00\n",
      "Episode 55/100, Total Reward: -35535.50, Total Real Reward: -2973.00, Total Real Profit: -2559.50, Win Rate: 0.54, Wins: 883, Losses: 761, Epsilon: 0.2877, Steps: 36754, Time: 149.46s\n",
      "Ações: Manter=18306, Comprar=9595, Vender=8853\n",
      "Ganhos Totais: 30003.00, Perdas Totais: -32562.50\n",
      "Episode 56/100, Total Reward: -28158.50, Total Real Reward: 2993.00, Total Real Profit: 3456.50, Win Rate: 0.54, Wins: 995, Losses: 849, Epsilon: 0.2848, Steps: 36754, Time: 149.68s\n",
      "Ações: Manter=15599, Comprar=10385, Vender=10770\n",
      "Ganhos Totais: 34608.00, Perdas Totais: -31151.50\n",
      "Modelo e log do episódio 56 salvos em: 4.14\\model_episode_56.pth e 4.14\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: -35444.00, Total Real Reward: -2074.25, Total Real Profit: -1605.25, Win Rate: 0.54, Wins: 1015, Losses: 853, Epsilon: 0.2820, Steps: 36754, Time: 150.13s\n",
      "Ações: Manter=15583, Comprar=10497, Vender=10674\n",
      "Ganhos Totais: 31764.50, Perdas Totais: -33369.75\n",
      "Episode 58/100, Total Reward: -26964.00, Total Real Reward: 3898.25, Total Real Profit: 4358.50, Win Rate: 0.55, Wins: 1016, Losses: 819, Epsilon: 0.2791, Steps: 36754, Time: 149.43s\n",
      "Ações: Manter=14537, Comprar=11343, Vender=10874\n",
      "Ganhos Totais: 35220.75, Perdas Totais: -30862.25\n",
      "Modelo e log do episódio 58 salvos em: 4.14\\model_episode_58.pth e 4.14\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: -32351.25, Total Real Reward: -274.75, Total Real Profit: 210.00, Win Rate: 0.54, Wins: 1033, Losses: 895, Epsilon: 0.2763, Steps: 36754, Time: 149.59s\n",
      "Ações: Manter=15018, Comprar=9966, Vender=11770\n",
      "Ganhos Totais: 32286.50, Perdas Totais: -32076.50\n",
      "Modelo e log do episódio 59 salvos em: 4.14\\model_episode_59.pth e 4.14\\log_episode_59.csv\n",
      "\n",
      "Episode 60/100, Total Reward: -37280.75, Total Real Reward: -2339.75, Total Real Profit: -1866.00, Win Rate: 0.56, Wins: 1055, Losses: 828, Epsilon: 0.2736, Steps: 36754, Time: 149.96s\n",
      "Ações: Manter=14771, Comprar=10602, Vender=11381\n",
      "Ganhos Totais: 33075.00, Perdas Totais: -34941.00\n",
      "Episode 61/100, Total Reward: -35053.75, Total Real Reward: -1183.25, Total Real Profit: -712.25, Win Rate: 0.53, Wins: 993, Losses: 880, Epsilon: 0.2708, Steps: 36754, Time: 149.90s\n",
      "Ações: Manter=15275, Comprar=10166, Vender=11313\n",
      "Ganhos Totais: 33158.25, Perdas Totais: -33870.50\n",
      "Episode 62/100, Total Reward: -32764.25, Total Real Reward: -254.75, Total Real Profit: 186.25, Win Rate: 0.53, Wins: 930, Losses: 825, Epsilon: 0.2681, Steps: 36754, Time: 150.20s\n",
      "Ações: Manter=14881, Comprar=10550, Vender=11323\n",
      "Ganhos Totais: 32695.75, Perdas Totais: -32509.50\n",
      "Modelo e log do episódio 62 salvos em: 4.14\\model_episode_62.pth e 4.14\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: -25800.75, Total Real Reward: 3073.50, Total Real Profit: 3474.25, Win Rate: 0.54, Wins: 867, Losses: 733, Epsilon: 0.2655, Steps: 36754, Time: 150.29s\n",
      "Ações: Manter=18128, Comprar=10266, Vender=8360\n",
      "Ganhos Totais: 32348.50, Perdas Totais: -28874.25\n",
      "Modelo e log do episódio 63 salvos em: 4.14\\model_episode_63.pth e 4.14\\log_episode_63.csv\n",
      "\n",
      "Episode 64/100, Total Reward: -32279.75, Total Real Reward: -335.50, Total Real Profit: 99.75, Win Rate: 0.54, Wins: 933, Losses: 797, Epsilon: 0.2628, Steps: 36754, Time: 149.60s\n",
      "Ações: Manter=16715, Comprar=10269, Vender=9770\n",
      "Ganhos Totais: 32044.00, Perdas Totais: -31944.25\n",
      "Modelo e log do episódio 64 salvos em: 4.14\\model_episode_64.pth e 4.14\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: -34324.00, Total Real Reward: -3170.25, Total Real Profit: -2754.50, Win Rate: 0.53, Wins: 871, Losses: 781, Epsilon: 0.2602, Steps: 36754, Time: 150.41s\n",
      "Ações: Manter=17771, Comprar=10484, Vender=8499\n",
      "Ganhos Totais: 28399.25, Perdas Totais: -31153.75\n",
      "Episode 66/100, Total Reward: -35288.50, Total Real Reward: -3056.00, Total Real Profit: -2664.25, Win Rate: 0.51, Wins: 802, Losses: 761, Epsilon: 0.2576, Steps: 36754, Time: 150.74s\n",
      "Ações: Manter=17347, Comprar=10460, Vender=8947\n",
      "Ganhos Totais: 29568.25, Perdas Totais: -32232.50\n",
      "Episode 67/100, Total Reward: -34604.25, Total Real Reward: -3921.25, Total Real Profit: -3536.50, Win Rate: 0.51, Wins: 785, Losses: 748, Epsilon: 0.2550, Steps: 36754, Time: 150.34s\n",
      "Ações: Manter=17245, Comprar=9989, Vender=9520\n",
      "Ganhos Totais: 27146.50, Perdas Totais: -30683.00\n",
      "Episode 68/100, Total Reward: -42893.75, Total Real Reward: -7995.00, Total Real Profit: -7607.75, Win Rate: 0.50, Wins: 771, Losses: 768, Epsilon: 0.2524, Steps: 36754, Time: 150.48s\n",
      "Ações: Manter=16074, Comprar=10816, Vender=9864\n",
      "Ganhos Totais: 27291.00, Perdas Totais: -34898.75\n",
      "Episode 69/100, Total Reward: -35896.50, Total Real Reward: -3936.25, Total Real Profit: -3547.50, Win Rate: 0.51, Wins: 796, Losses: 754, Epsilon: 0.2499, Steps: 36754, Time: 150.96s\n",
      "Ações: Manter=17898, Comprar=10571, Vender=8285\n",
      "Ganhos Totais: 28412.75, Perdas Totais: -31960.25\n",
      "Episode 70/100, Total Reward: -31602.00, Total Real Reward: -682.25, Total Real Profit: -292.75, Win Rate: 0.53, Wins: 821, Losses: 735, Epsilon: 0.2474, Steps: 36754, Time: 151.09s\n",
      "Ações: Manter=17761, Comprar=10843, Vender=8150\n",
      "Ganhos Totais: 30627.00, Perdas Totais: -30919.75\n",
      "Modelo e log do episódio 70 salvos em: 4.14\\model_episode_70.pth e 4.14\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: -30949.75, Total Real Reward: 1032.25, Total Real Profit: 1457.75, Win Rate: 0.55, Wins: 926, Losses: 768, Epsilon: 0.2449, Steps: 36754, Time: 150.81s\n",
      "Ações: Manter=16591, Comprar=11064, Vender=9099\n",
      "Ganhos Totais: 33439.75, Perdas Totais: -31982.00\n",
      "Modelo e log do episódio 71 salvos em: 4.14\\model_episode_71.pth e 4.14\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: -28504.50, Total Real Reward: 2395.50, Total Real Profit: 2854.00, Win Rate: 0.57, Wins: 1035, Losses: 794, Epsilon: 0.2425, Steps: 36754, Time: 151.58s\n",
      "Ações: Manter=14491, Comprar=10183, Vender=12080\n",
      "Ganhos Totais: 33754.00, Perdas Totais: -30900.00\n",
      "Modelo e log do episódio 72 salvos em: 4.14\\model_episode_72.pth e 4.14\\log_episode_72.csv\n",
      "\n",
      "Episode 73/100, Total Reward: -34398.75, Total Real Reward: -2811.00, Total Real Profit: -2417.50, Win Rate: 0.52, Wins: 815, Losses: 751, Epsilon: 0.2401, Steps: 36754, Time: 151.19s\n",
      "Ações: Manter=15367, Comprar=10529, Vender=10858\n",
      "Ganhos Totais: 29170.25, Perdas Totais: -31587.75\n",
      "Episode 74/100, Total Reward: -35159.50, Total Real Reward: -2795.25, Total Real Profit: -2410.25, Win Rate: 0.52, Wins: 802, Losses: 731, Epsilon: 0.2377, Steps: 36754, Time: 150.98s\n",
      "Ações: Manter=15149, Comprar=10140, Vender=11465\n",
      "Ganhos Totais: 29954.00, Perdas Totais: -32364.25\n",
      "Episode 75/100, Total Reward: -38774.00, Total Real Reward: -4053.25, Total Real Profit: -3636.00, Win Rate: 0.52, Wins: 871, Losses: 797, Epsilon: 0.2353, Steps: 36754, Time: 150.66s\n",
      "Ações: Manter=15420, Comprar=9423, Vender=11911\n",
      "Ganhos Totais: 31084.75, Perdas Totais: -34720.75\n",
      "Episode 76/100, Total Reward: -30933.50, Total Real Reward: -998.50, Total Real Profit: -622.75, Win Rate: 0.55, Wins: 819, Losses: 678, Epsilon: 0.2329, Steps: 36754, Time: 150.75s\n",
      "Ações: Manter=17378, Comprar=10262, Vender=9114\n",
      "Ganhos Totais: 29312.25, Perdas Totais: -29935.00\n",
      "Modelo e log do episódio 76 salvos em: 4.14\\model_episode_76.pth e 4.14\\log_episode_76.csv\n",
      "\n",
      "Episode 77/100, Total Reward: -32372.75, Total Real Reward: -867.50, Total Real Profit: -461.25, Win Rate: 0.53, Wins: 862, Losses: 758, Epsilon: 0.2306, Steps: 36754, Time: 143.30s\n",
      "Ações: Manter=16528, Comprar=10145, Vender=10081\n",
      "Ganhos Totais: 31044.00, Perdas Totais: -31505.25\n",
      "Episode 78/100, Total Reward: -35695.75, Total Real Reward: -3410.25, Total Real Profit: -3033.00, Win Rate: 0.53, Wins: 800, Losses: 707, Epsilon: 0.2283, Steps: 36754, Time: 152.13s\n",
      "Ações: Manter=15110, Comprar=10883, Vender=10761\n",
      "Ganhos Totais: 29252.50, Perdas Totais: -32285.50\n",
      "Episode 79/100, Total Reward: -29870.25, Total Real Reward: 1176.00, Total Real Profit: 1590.75, Win Rate: 0.54, Wins: 892, Losses: 760, Epsilon: 0.2260, Steps: 36754, Time: 153.18s\n",
      "Ações: Manter=16366, Comprar=9461, Vender=10927\n",
      "Ganhos Totais: 32637.00, Perdas Totais: -31046.25\n",
      "Modelo e log do episódio 79 salvos em: 4.14\\model_episode_79.pth e 4.14\\log_episode_79.csv\n",
      "\n",
      "Episode 80/100, Total Reward: -31504.50, Total Real Reward: -52.50, Total Real Profit: 343.75, Win Rate: 0.56, Wins: 890, Losses: 692, Epsilon: 0.2238, Steps: 36754, Time: 159.83s\n",
      "Ações: Manter=17268, Comprar=9204, Vender=10282\n",
      "Ganhos Totais: 31795.75, Perdas Totais: -31452.00\n",
      "Episode 81/100, Total Reward: -33114.75, Total Real Reward: -2603.75, Total Real Profit: -2230.00, Win Rate: 0.53, Wins: 795, Losses: 695, Epsilon: 0.2215, Steps: 36754, Time: 160.31s\n",
      "Ações: Manter=18304, Comprar=9236, Vender=9214\n",
      "Ganhos Totais: 28281.00, Perdas Totais: -30511.00\n",
      "Episode 82/100, Total Reward: -33367.75, Total Real Reward: -900.25, Total Real Profit: -481.75, Win Rate: 0.57, Wins: 946, Losses: 724, Epsilon: 0.2193, Steps: 36754, Time: 158.11s\n",
      "Ações: Manter=15662, Comprar=11587, Vender=9505\n",
      "Ganhos Totais: 31985.75, Perdas Totais: -32467.50\n",
      "Episode 83/100, Total Reward: -34135.50, Total Real Reward: -1697.75, Total Real Profit: -1272.50, Win Rate: 0.54, Wins: 912, Losses: 785, Epsilon: 0.2171, Steps: 36754, Time: 159.53s\n",
      "Ações: Manter=15481, Comprar=10976, Vender=10297\n",
      "Ganhos Totais: 31165.25, Perdas Totais: -32437.75\n",
      "Episode 84/100, Total Reward: -39712.25, Total Real Reward: -6007.00, Total Real Profit: -5596.75, Win Rate: 0.53, Wins: 858, Losses: 773, Epsilon: 0.2149, Steps: 36754, Time: 156.85s\n",
      "Ações: Manter=17225, Comprar=10134, Vender=9395\n",
      "Ganhos Totais: 28108.50, Perdas Totais: -33705.25\n",
      "Episode 85/100, Total Reward: -33297.50, Total Real Reward: -1720.50, Total Real Profit: -1318.00, Win Rate: 0.56, Wins: 893, Losses: 713, Epsilon: 0.2128, Steps: 36754, Time: 155.44s\n",
      "Ações: Manter=17623, Comprar=10238, Vender=8893\n",
      "Ganhos Totais: 30259.00, Perdas Totais: -31577.00\n",
      "Episode 86/100, Total Reward: -32987.50, Total Real Reward: -999.25, Total Real Profit: -532.00, Win Rate: 0.55, Wins: 1031, Losses: 834, Epsilon: 0.2107, Steps: 36754, Time: 155.82s\n",
      "Ações: Manter=16444, Comprar=10576, Vender=9734\n",
      "Ganhos Totais: 31456.25, Perdas Totais: -31988.25\n",
      "Episode 87/100, Total Reward: -36581.00, Total Real Reward: -2743.75, Total Real Profit: -2262.25, Win Rate: 0.54, Wins: 1031, Losses: 884, Epsilon: 0.2086, Steps: 36754, Time: 159.31s\n",
      "Ações: Manter=17119, Comprar=10662, Vender=8973\n",
      "Ganhos Totais: 31575.00, Perdas Totais: -33837.25\n",
      "Episode 88/100, Total Reward: -33266.75, Total Real Reward: -1933.00, Total Real Profit: -1482.75, Win Rate: 0.54, Wins: 963, Losses: 831, Epsilon: 0.2065, Steps: 36754, Time: 160.99s\n",
      "Ações: Manter=17187, Comprar=10052, Vender=9515\n",
      "Ganhos Totais: 29851.00, Perdas Totais: -31333.75\n",
      "Episode 89/100, Total Reward: -30640.00, Total Real Reward: 899.75, Total Real Profit: 1342.50, Win Rate: 0.55, Wins: 975, Losses: 787, Epsilon: 0.2044, Steps: 36754, Time: 160.06s\n",
      "Ações: Manter=16225, Comprar=11501, Vender=9028\n",
      "Ganhos Totais: 32882.25, Perdas Totais: -31539.75\n",
      "Modelo e log do episódio 89 salvos em: 4.14\\model_episode_89.pth e 4.14\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: -25620.00, Total Real Reward: 4405.00, Total Real Profit: 4891.00, Win Rate: 0.57, Wins: 1099, Losses: 838, Epsilon: 0.2024, Steps: 36754, Time: 158.67s\n",
      "Ações: Manter=16344, Comprar=11403, Vender=9007\n",
      "Ganhos Totais: 34916.00, Perdas Totais: -30025.00\n",
      "Modelo e log do episódio 90 salvos em: 4.14\\model_episode_90.pth e 4.14\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: -30462.00, Total Real Reward: 489.25, Total Real Profit: 918.75, Win Rate: 0.55, Wins: 935, Losses: 780, Epsilon: 0.2003, Steps: 36754, Time: 161.54s\n",
      "Ações: Manter=16597, Comprar=10831, Vender=9326\n",
      "Ganhos Totais: 31870.00, Perdas Totais: -30951.25\n",
      "Modelo e log do episódio 91 salvos em: 4.14\\model_episode_91.pth e 4.14\\log_episode_91.csv\n",
      "\n",
      "Episode 92/100, Total Reward: -31855.00, Total Real Reward: 264.75, Total Real Profit: 683.75, Win Rate: 0.55, Wins: 926, Losses: 748, Epsilon: 0.1983, Steps: 36754, Time: 158.40s\n",
      "Ações: Manter=15483, Comprar=10556, Vender=10715\n",
      "Ganhos Totais: 32803.50, Perdas Totais: -32119.75\n",
      "Episode 93/100, Total Reward: -34093.50, Total Real Reward: -1585.50, Total Real Profit: -1143.50, Win Rate: 0.56, Wins: 989, Losses: 775, Epsilon: 0.1964, Steps: 36754, Time: 162.51s\n",
      "Ações: Manter=15715, Comprar=10884, Vender=10155\n",
      "Ganhos Totais: 31364.50, Perdas Totais: -32508.00\n",
      "Episode 94/100, Total Reward: -26938.75, Total Real Reward: 3344.75, Total Real Profit: 3772.00, Win Rate: 0.57, Wins: 973, Losses: 733, Epsilon: 0.1944, Steps: 36754, Time: 161.89s\n",
      "Ações: Manter=15146, Comprar=10840, Vender=10768\n",
      "Ganhos Totais: 34055.50, Perdas Totais: -30283.50\n",
      "Modelo e log do episódio 94 salvos em: 4.14\\model_episode_94.pth e 4.14\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: -30979.75, Total Real Reward: 667.50, Total Real Profit: 1111.50, Win Rate: 0.55, Wins: 970, Losses: 799, Epsilon: 0.1924, Steps: 36754, Time: 160.53s\n",
      "Ações: Manter=16317, Comprar=11755, Vender=8682\n",
      "Ganhos Totais: 32758.75, Perdas Totais: -31647.25\n",
      "Episode 96/100, Total Reward: -26719.00, Total Real Reward: 4446.25, Total Real Profit: 4939.25, Win Rate: 0.60, Wins: 1171, Losses: 793, Epsilon: 0.1905, Steps: 36754, Time: 160.81s\n",
      "Ações: Manter=15359, Comprar=13072, Vender=8323\n",
      "Ganhos Totais: 36104.50, Perdas Totais: -31165.25\n",
      "Modelo e log do episódio 96 salvos em: 4.14\\model_episode_96.pth e 4.14\\log_episode_96.csv\n",
      "\n",
      "Episode 97/100, Total Reward: -29258.25, Total Real Reward: 1839.00, Total Real Profit: 2291.00, Win Rate: 0.56, Wins: 1000, Losses: 799, Epsilon: 0.1886, Steps: 36754, Time: 156.91s\n",
      "Ações: Manter=16399, Comprar=10575, Vender=9780\n",
      "Ganhos Totais: 33388.25, Perdas Totais: -31097.25\n",
      "Modelo e log do episódio 97 salvos em: 4.14\\model_episode_97.pth e 4.14\\log_episode_97.csv\n",
      "\n",
      "Episode 98/100, Total Reward: -30395.75, Total Real Reward: -174.00, Total Real Profit: 232.50, Win Rate: 0.56, Wins: 907, Losses: 706, Epsilon: 0.1867, Steps: 36754, Time: 151.74s\n",
      "Ações: Manter=17425, Comprar=11062, Vender=8267\n",
      "Ganhos Totais: 30454.25, Perdas Totais: -30221.75\n",
      "Episode 99/100, Total Reward: -27730.25, Total Real Reward: 2042.00, Total Real Profit: 2424.00, Win Rate: 0.55, Wins: 844, Losses: 682, Epsilon: 0.1849, Steps: 36754, Time: 133.78s\n",
      "Ações: Manter=17392, Comprar=10051, Vender=9311\n",
      "Ganhos Totais: 32196.25, Perdas Totais: -29772.25\n",
      "Modelo e log do episódio 99 salvos em: 4.14\\model_episode_99.pth e 4.14\\log_episode_99.csv\n",
      "\n",
      "Episode 100/100, Total Reward: -34164.75, Total Real Reward: -1215.25, Total Real Profit: -795.75, Win Rate: 0.55, Wins: 928, Losses: 745, Epsilon: 0.1830, Steps: 36754, Time: 133.12s\n",
      "Ações: Manter=16623, Comprar=10695, Vender=9436\n",
      "Ganhos Totais: 32153.75, Perdas Totais: -32949.50\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 90, Total Reward: -25620.00, Total Real Reward: 4405.00, Total Real Profit: 4891.00, Win Rate: 0.57, Wins: 1099, Losses: 838, Ações: {0: 16344, 1: 11403, 2: 9007}, Steps: 36754, Time: 158.67s\n",
      "Rank 2: Episode 63, Total Reward: -25800.75, Total Real Reward: 3073.50, Total Real Profit: 3474.25, Win Rate: 0.54, Wins: 867, Losses: 733, Ações: {0: 18128, 1: 10266, 2: 8360}, Steps: 36754, Time: 150.29s\n",
      "Rank 3: Episode 96, Total Reward: -26719.00, Total Real Reward: 4446.25, Total Real Profit: 4939.25, Win Rate: 0.60, Wins: 1171, Losses: 793, Ações: {0: 15359, 1: 13072, 2: 8323}, Steps: 36754, Time: 160.81s\n",
      "Rank 4: Episode 94, Total Reward: -26938.75, Total Real Reward: 3344.75, Total Real Profit: 3772.00, Win Rate: 0.57, Wins: 973, Losses: 733, Ações: {0: 15146, 1: 10840, 2: 10768}, Steps: 36754, Time: 161.89s\n",
      "Rank 5: Episode 58, Total Reward: -26964.00, Total Real Reward: 3898.25, Total Real Profit: 4358.50, Win Rate: 0.55, Wins: 1016, Losses: 819, Ações: {0: 14537, 1: 11343, 2: 10874}, Steps: 36754, Time: 149.43s\n",
      "Rank 6: Episode 99, Total Reward: -27730.25, Total Real Reward: 2042.00, Total Real Profit: 2424.00, Win Rate: 0.55, Wins: 844, Losses: 682, Ações: {0: 17392, 1: 10051, 2: 9311}, Steps: 36754, Time: 133.78s\n",
      "Rank 7: Episode 46, Total Reward: -28048.75, Total Real Reward: 3387.25, Total Real Profit: 3841.25, Win Rate: 0.52, Wins: 949, Losses: 862, Ações: {0: 14229, 1: 9608, 2: 12917}, Steps: 36754, Time: 147.98s\n",
      "Rank 8: Episode 56, Total Reward: -28158.50, Total Real Reward: 2993.00, Total Real Profit: 3456.50, Win Rate: 0.54, Wins: 995, Losses: 849, Ações: {0: 15599, 1: 10385, 2: 10770}, Steps: 36754, Time: 149.68s\n",
      "Rank 9: Episode 72, Total Reward: -28504.50, Total Real Reward: 2395.50, Total Real Profit: 2854.00, Win Rate: 0.57, Wins: 1035, Losses: 794, Ações: {0: 14491, 1: 10183, 2: 12080}, Steps: 36754, Time: 151.58s\n",
      "Rank 10: Episode 97, Total Reward: -29258.25, Total Real Reward: 1839.00, Total Real Profit: 2291.00, Win Rate: 0.56, Wins: 1000, Losses: 799, Ações: {0: 16399, 1: 10575, 2: 9780}, Steps: 36754, Time: 156.91s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "        # Alteração 1: Variáveis para Recompensa e Lucro Reais\n",
    "        self.reward_real = 0.0\n",
    "        self.profit_real = 0.0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        self.reward_real = 0.0  # Resetar recompensa real\n",
    "        self.profit_real = 0.0  # Resetar lucro real\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Alteração 2: Inicializar variáveis para recompensa e lucro reais\n",
    "        profit_real = 0  # Lucro real da operação\n",
    "        reward_real = 0  # Recompensa real da operação\n",
    "        reward = 0       # Recompensa modificada para o agente\n",
    "        penalty_factor = 2.0  # Fator de penalidade (pode ajustar conforme necessário)\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    reward_real -= 0.25  # Custo real\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit_real = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.profit_real += profit_real\n",
    "                    reward_real += profit_real\n",
    "                    # Aplicar punição mais severa se houve perda\n",
    "                    if profit_real < 0:\n",
    "                        reward += profit_real * penalty_factor\n",
    "                    else:\n",
    "                        reward += profit_real\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real,\n",
    "                        'reward_real': reward_real\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    reward_real -= 0.25  # Custo real\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit_real = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.profit_real += profit_real\n",
    "                    reward_real += profit_real\n",
    "                    # Aplicar punição mais severa se houve perda\n",
    "                    if profit_real < 0:\n",
    "                        reward += profit_real * penalty_factor\n",
    "                    else:\n",
    "                        reward += profit_real\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit_real,\n",
    "                        'reward_real': reward_real\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit_real = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.profit_real += profit_real\n",
    "                reward_real += profit_real\n",
    "                # Aplicar punição mais severa se houve perda\n",
    "                if profit_real < 0:\n",
    "                    reward += profit_real * penalty_factor\n",
    "                else:\n",
    "                    reward += profit_real\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real,\n",
    "                    'reward_real': reward_real\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit_real = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.profit_real += profit_real\n",
    "                reward_real += profit_real\n",
    "                # Aplicar punição mais severa se houve perda\n",
    "                if profit_real < 0:\n",
    "                    reward += profit_real * penalty_factor\n",
    "                else:\n",
    "                    reward += profit_real\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit_real,\n",
    "                    'reward_real': reward_real\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar as recompensas reais\n",
    "        self.reward_real += reward_real\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.14\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_reward_real = 0  # Total da recompensa real\n",
    "    total_profit_real = 0  # Total do lucro real\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Atualizar recompensas e lucros reais\n",
    "        total_reward_real = env.reward_real\n",
    "        total_profit_real = env.profit_real\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, \"\n",
    "          f\"Total Real Reward: {total_reward_real:.2f}, Total Real Profit: {total_profit_real:.2f}, \"\n",
    "          f\"Win Rate: {win_rate:.2f}, Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'total_reward_real': total_reward_real,\n",
    "        'total_profit_real': total_profit_real,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Total Real Reward: {ep['total_reward_real']:.2f}, Total Real Profit: {ep['total_profit_real']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
