{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -1129.75, Win Rate: 0.49, Wins: 1458, Losses: 1499, Epsilon: 0.4950, Steps: 36754, Time: 123.78s\n",
      "Ações: Manter=10256, Comprar=12963, Vender=13535\n",
      "Ganhos Totais: 38084.50, Perdas Totais: -39214.25\n",
      "Modelo e log do episódio 1 salvos em: 4.8\\model_episode_1.pth e 4.8\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: 2085.75, Win Rate: 0.52, Wins: 1467, Losses: 1367, Epsilon: 0.4900, Steps: 36754, Time: 125.40s\n",
      "Ações: Manter=9243, Comprar=12381, Vender=15130\n",
      "Ganhos Totais: 39303.00, Perdas Totais: -37217.25\n",
      "Modelo e log do episódio 2 salvos em: 4.8\\model_episode_2.pth e 4.8\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: 817.00, Win Rate: 0.52, Wins: 1469, Losses: 1346, Epsilon: 0.4851, Steps: 36754, Time: 123.17s\n",
      "Ações: Manter=9800, Comprar=12642, Vender=14312\n",
      "Ganhos Totais: 38721.50, Perdas Totais: -37904.50\n",
      "Modelo e log do episódio 3 salvos em: 4.8\\model_episode_3.pth e 4.8\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 1127.00, Win Rate: 0.52, Wins: 1468, Losses: 1337, Epsilon: 0.4803, Steps: 36754, Time: 123.20s\n",
      "Ações: Manter=10309, Comprar=12471, Vender=13974\n",
      "Ganhos Totais: 39353.75, Perdas Totais: -38226.75\n",
      "Modelo e log do episódio 4 salvos em: 4.8\\model_episode_4.pth e 4.8\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: 2045.50, Win Rate: 0.53, Wins: 1433, Losses: 1295, Epsilon: 0.4755, Steps: 36754, Time: 124.51s\n",
      "Ações: Manter=11568, Comprar=13498, Vender=11688\n",
      "Ganhos Totais: 38448.25, Perdas Totais: -36402.75\n",
      "Modelo e log do episódio 5 salvos em: 4.8\\model_episode_5.pth e 4.8\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 1598.25, Win Rate: 0.52, Wins: 1449, Losses: 1343, Epsilon: 0.4707, Steps: 36754, Time: 123.66s\n",
      "Ações: Manter=11971, Comprar=12963, Vender=11820\n",
      "Ganhos Totais: 38243.00, Perdas Totais: -36644.75\n",
      "Modelo e log do episódio 6 salvos em: 4.8\\model_episode_6.pth e 4.8\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: 742.00, Win Rate: 0.53, Wins: 1414, Losses: 1265, Epsilon: 0.4660, Steps: 36754, Time: 123.08s\n",
      "Ações: Manter=12442, Comprar=11787, Vender=12525\n",
      "Ganhos Totais: 37407.50, Perdas Totais: -36665.50\n",
      "Modelo e log do episódio 7 salvos em: 4.8\\model_episode_7.pth e 4.8\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -1114.25, Win Rate: 0.51, Wins: 1435, Losses: 1371, Epsilon: 0.4614, Steps: 36754, Time: 123.29s\n",
      "Ações: Manter=11518, Comprar=12231, Vender=13005\n",
      "Ganhos Totais: 37018.75, Perdas Totais: -38133.00\n",
      "Modelo e log do episódio 8 salvos em: 4.8\\model_episode_8.pth e 4.8\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -190.50, Win Rate: 0.52, Wins: 1404, Losses: 1300, Epsilon: 0.4568, Steps: 36754, Time: 123.46s\n",
      "Ações: Manter=11821, Comprar=12740, Vender=12193\n",
      "Ganhos Totais: 38150.50, Perdas Totais: -38341.00\n",
      "Modelo e log do episódio 9 salvos em: 4.8\\model_episode_9.pth e 4.8\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -165.00, Win Rate: 0.50, Wins: 1277, Losses: 1260, Epsilon: 0.4522, Steps: 36754, Time: 123.32s\n",
      "Ações: Manter=12381, Comprar=11794, Vender=12579\n",
      "Ganhos Totais: 36237.25, Perdas Totais: -36402.25\n",
      "Modelo e log do episódio 10 salvos em: 4.8\\model_episode_10.pth e 4.8\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 1738.75, Win Rate: 0.53, Wins: 1452, Losses: 1304, Epsilon: 0.4477, Steps: 36754, Time: 123.45s\n",
      "Ações: Manter=11877, Comprar=13471, Vender=11406\n",
      "Ganhos Totais: 38491.25, Perdas Totais: -36752.50\n",
      "Modelo e log do episódio 11 salvos em: 4.8\\model_episode_11.pth e 4.8\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -2228.25, Win Rate: 0.51, Wins: 1358, Losses: 1306, Epsilon: 0.4432, Steps: 36754, Time: 124.09s\n",
      "Ações: Manter=12625, Comprar=11652, Vender=12477\n",
      "Ganhos Totais: 37040.75, Perdas Totais: -39269.00\n",
      "Episode 13/100, Total Reward: 161.75, Win Rate: 0.52, Wins: 1321, Losses: 1239, Epsilon: 0.4388, Steps: 36754, Time: 123.83s\n",
      "Ações: Manter=13452, Comprar=12568, Vender=10734\n",
      "Ganhos Totais: 36330.00, Perdas Totais: -36168.25\n",
      "Modelo e log do episódio 13 salvos em: 4.8\\model_episode_13.pth e 4.8\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -1746.25, Win Rate: 0.52, Wins: 1343, Losses: 1260, Epsilon: 0.4344, Steps: 36754, Time: 123.64s\n",
      "Ações: Manter=14325, Comprar=11605, Vender=10824\n",
      "Ganhos Totais: 36276.25, Perdas Totais: -38022.50\n",
      "Episode 15/100, Total Reward: 61.50, Win Rate: 0.51, Wins: 1231, Losses: 1164, Epsilon: 0.4300, Steps: 36754, Time: 124.45s\n",
      "Ações: Manter=15623, Comprar=9589, Vender=11542\n",
      "Ganhos Totais: 35224.25, Perdas Totais: -35162.75\n",
      "Modelo e log do episódio 15 salvos em: 4.8\\model_episode_15.pth e 4.8\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -541.00, Win Rate: 0.52, Wins: 1334, Losses: 1214, Epsilon: 0.4257, Steps: 36754, Time: 124.15s\n",
      "Ações: Manter=12521, Comprar=12947, Vender=11286\n",
      "Ganhos Totais: 36831.50, Perdas Totais: -37372.50\n",
      "Episode 17/100, Total Reward: -947.00, Win Rate: 0.54, Wins: 1349, Losses: 1171, Epsilon: 0.4215, Steps: 36754, Time: 124.68s\n",
      "Ações: Manter=13098, Comprar=13118, Vender=10538\n",
      "Ganhos Totais: 37039.25, Perdas Totais: -37986.25\n",
      "Episode 18/100, Total Reward: -163.25, Win Rate: 0.51, Wins: 1262, Losses: 1232, Epsilon: 0.4173, Steps: 36754, Time: 124.86s\n",
      "Ações: Manter=13349, Comprar=12456, Vender=10949\n",
      "Ganhos Totais: 36547.00, Perdas Totais: -36710.25\n",
      "Modelo e log do episódio 18 salvos em: 4.8\\model_episode_18.pth e 4.8\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -95.75, Win Rate: 0.53, Wins: 1356, Losses: 1213, Epsilon: 0.4131, Steps: 36754, Time: 124.70s\n",
      "Ações: Manter=12492, Comprar=12861, Vender=11401\n",
      "Ganhos Totais: 36095.75, Perdas Totais: -36191.50\n",
      "Modelo e log do episódio 19 salvos em: 4.8\\model_episode_19.pth e 4.8\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 1708.25, Win Rate: 0.54, Wins: 1462, Losses: 1231, Epsilon: 0.4090, Steps: 36754, Time: 124.50s\n",
      "Ações: Manter=11596, Comprar=13939, Vender=11219\n",
      "Ganhos Totais: 38754.75, Perdas Totais: -37046.50\n",
      "Modelo e log do episódio 20 salvos em: 4.8\\model_episode_20.pth e 4.8\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: 2845.50, Win Rate: 0.54, Wins: 1401, Losses: 1206, Epsilon: 0.4049, Steps: 36754, Time: 124.99s\n",
      "Ações: Manter=14060, Comprar=11533, Vender=11161\n",
      "Ganhos Totais: 39259.75, Perdas Totais: -36414.25\n",
      "Modelo e log do episódio 21 salvos em: 4.8\\model_episode_21.pth e 4.8\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -583.25, Win Rate: 0.54, Wins: 1464, Losses: 1246, Epsilon: 0.4008, Steps: 36754, Time: 120.73s\n",
      "Ações: Manter=10799, Comprar=12923, Vender=13032\n",
      "Ganhos Totais: 38439.25, Perdas Totais: -39022.50\n",
      "Episode 23/100, Total Reward: 112.50, Win Rate: 0.51, Wins: 1254, Losses: 1186, Epsilon: 0.3968, Steps: 36754, Time: 111.25s\n",
      "Ações: Manter=13317, Comprar=11279, Vender=12158\n",
      "Ganhos Totais: 37162.25, Perdas Totais: -37049.75\n",
      "Episode 24/100, Total Reward: 2841.00, Win Rate: 0.54, Wins: 1458, Losses: 1260, Epsilon: 0.3928, Steps: 36754, Time: 108.96s\n",
      "Ações: Manter=10953, Comprar=12745, Vender=13056\n",
      "Ganhos Totais: 38925.50, Perdas Totais: -36084.50\n",
      "Modelo e log do episódio 24 salvos em: 4.8\\model_episode_24.pth e 4.8\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 856.75, Win Rate: 0.52, Wins: 1356, Losses: 1262, Epsilon: 0.3889, Steps: 36754, Time: 110.68s\n",
      "Ações: Manter=11401, Comprar=13593, Vender=11760\n",
      "Ganhos Totais: 37774.50, Perdas Totais: -36917.75\n",
      "Modelo e log do episódio 25 salvos em: 4.8\\model_episode_25.pth e 4.8\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: 3438.00, Win Rate: 0.54, Wins: 1278, Losses: 1087, Epsilon: 0.3850, Steps: 36754, Time: 108.99s\n",
      "Ações: Manter=13762, Comprar=11504, Vender=11488\n",
      "Ganhos Totais: 36290.75, Perdas Totais: -32852.75\n",
      "Modelo e log do episódio 26 salvos em: 4.8\\model_episode_26.pth e 4.8\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -1559.00, Win Rate: 0.51, Wins: 1315, Losses: 1243, Epsilon: 0.3812, Steps: 36754, Time: 109.25s\n",
      "Ações: Manter=11385, Comprar=13569, Vender=11800\n",
      "Ganhos Totais: 37294.00, Perdas Totais: -38853.00\n",
      "Episode 28/100, Total Reward: -2310.00, Win Rate: 0.51, Wins: 1340, Losses: 1272, Epsilon: 0.3774, Steps: 36754, Time: 109.64s\n",
      "Ações: Manter=10823, Comprar=14297, Vender=11634\n",
      "Ganhos Totais: 35402.75, Perdas Totais: -37712.75\n",
      "Episode 29/100, Total Reward: -1090.75, Win Rate: 0.52, Wins: 1312, Losses: 1194, Epsilon: 0.3736, Steps: 36754, Time: 109.10s\n",
      "Ações: Manter=13036, Comprar=12690, Vender=11028\n",
      "Ganhos Totais: 35970.25, Perdas Totais: -37061.00\n",
      "Episode 30/100, Total Reward: 850.50, Win Rate: 0.53, Wins: 1372, Losses: 1231, Epsilon: 0.3699, Steps: 36754, Time: 109.34s\n",
      "Ações: Manter=12964, Comprar=12949, Vender=10841\n",
      "Ganhos Totais: 38367.75, Perdas Totais: -37517.25\n",
      "Episode 31/100, Total Reward: 1511.00, Win Rate: 0.54, Wins: 1392, Losses: 1188, Epsilon: 0.3662, Steps: 36754, Time: 109.11s\n",
      "Ações: Manter=11359, Comprar=13947, Vender=11448\n",
      "Ganhos Totais: 37375.75, Perdas Totais: -35864.75\n",
      "Modelo e log do episódio 31 salvos em: 4.8\\model_episode_31.pth e 4.8\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 2163.50, Win Rate: 0.52, Wins: 1346, Losses: 1221, Epsilon: 0.3625, Steps: 36754, Time: 109.64s\n",
      "Ações: Manter=11579, Comprar=11808, Vender=13367\n",
      "Ganhos Totais: 37963.50, Perdas Totais: -35800.00\n",
      "Modelo e log do episódio 32 salvos em: 4.8\\model_episode_32.pth e 4.8\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: -312.25, Win Rate: 0.52, Wins: 1246, Losses: 1131, Epsilon: 0.3589, Steps: 36754, Time: 110.83s\n",
      "Ações: Manter=13751, Comprar=12308, Vender=10695\n",
      "Ganhos Totais: 35894.75, Perdas Totais: -36207.00\n",
      "Episode 34/100, Total Reward: 750.25, Win Rate: 0.53, Wins: 1343, Losses: 1188, Epsilon: 0.3553, Steps: 36754, Time: 109.74s\n",
      "Ações: Manter=11319, Comprar=12876, Vender=12559\n",
      "Ganhos Totais: 38050.75, Perdas Totais: -37300.50\n",
      "Episode 35/100, Total Reward: -2662.25, Win Rate: 0.51, Wins: 1209, Losses: 1159, Epsilon: 0.3517, Steps: 36754, Time: 109.71s\n",
      "Ações: Manter=13496, Comprar=10549, Vender=12709\n",
      "Ganhos Totais: 34069.75, Perdas Totais: -36732.00\n",
      "Episode 36/100, Total Reward: -875.00, Win Rate: 0.53, Wins: 1342, Losses: 1179, Epsilon: 0.3482, Steps: 36754, Time: 109.63s\n",
      "Ações: Manter=11521, Comprar=14345, Vender=10888\n",
      "Ganhos Totais: 36167.75, Perdas Totais: -37042.75\n",
      "Episode 37/100, Total Reward: -281.00, Win Rate: 0.51, Wins: 1183, Losses: 1141, Epsilon: 0.3447, Steps: 36754, Time: 109.67s\n",
      "Ações: Manter=15217, Comprar=11425, Vender=10112\n",
      "Ganhos Totais: 34717.25, Perdas Totais: -34998.25\n",
      "Episode 38/100, Total Reward: -2861.00, Win Rate: 0.52, Wins: 1230, Losses: 1158, Epsilon: 0.3413, Steps: 36754, Time: 109.60s\n",
      "Ações: Manter=14323, Comprar=11367, Vender=11064\n",
      "Ganhos Totais: 35618.00, Perdas Totais: -38479.00\n",
      "Episode 39/100, Total Reward: -4916.75, Win Rate: 0.53, Wins: 1262, Losses: 1131, Epsilon: 0.3379, Steps: 36754, Time: 109.57s\n",
      "Ações: Manter=14855, Comprar=10952, Vender=10947\n",
      "Ganhos Totais: 33349.50, Perdas Totais: -38266.25\n",
      "Episode 40/100, Total Reward: 759.00, Win Rate: 0.54, Wins: 1346, Losses: 1150, Epsilon: 0.3345, Steps: 36754, Time: 109.67s\n",
      "Ações: Manter=13192, Comprar=11668, Vender=11894\n",
      "Ganhos Totais: 36867.25, Perdas Totais: -36108.25\n",
      "Episode 41/100, Total Reward: 2356.50, Win Rate: 0.52, Wins: 1331, Losses: 1227, Epsilon: 0.3311, Steps: 36754, Time: 109.91s\n",
      "Ações: Manter=11213, Comprar=13361, Vender=12180\n",
      "Ganhos Totais: 38018.50, Perdas Totais: -35662.00\n",
      "Modelo e log do episódio 41 salvos em: 4.8\\model_episode_41.pth e 4.8\\log_episode_41.csv\n",
      "\n",
      "Episode 42/100, Total Reward: -1911.50, Win Rate: 0.52, Wins: 1397, Losses: 1288, Epsilon: 0.3278, Steps: 36754, Time: 110.06s\n",
      "Ações: Manter=8038, Comprar=16551, Vender=12165\n",
      "Ganhos Totais: 36945.25, Perdas Totais: -38856.75\n",
      "Episode 43/100, Total Reward: -2429.25, Win Rate: 0.52, Wins: 1275, Losses: 1181, Epsilon: 0.3246, Steps: 36754, Time: 110.04s\n",
      "Ações: Manter=13039, Comprar=12263, Vender=11452\n",
      "Ganhos Totais: 34306.25, Perdas Totais: -36735.50\n",
      "Episode 44/100, Total Reward: -206.25, Win Rate: 0.51, Wins: 1262, Losses: 1200, Epsilon: 0.3213, Steps: 36754, Time: 109.94s\n",
      "Ações: Manter=12284, Comprar=12797, Vender=11673\n",
      "Ganhos Totais: 35852.25, Perdas Totais: -36058.50\n",
      "Episode 45/100, Total Reward: 1108.25, Win Rate: 0.54, Wins: 1358, Losses: 1160, Epsilon: 0.3181, Steps: 36754, Time: 110.03s\n",
      "Ações: Manter=10377, Comprar=15160, Vender=11217\n",
      "Ganhos Totais: 37434.75, Perdas Totais: -36326.50\n",
      "Episode 46/100, Total Reward: 3935.50, Win Rate: 0.54, Wins: 1343, Losses: 1153, Epsilon: 0.3149, Steps: 36754, Time: 109.87s\n",
      "Ações: Manter=12166, Comprar=11464, Vender=13124\n",
      "Ganhos Totais: 37894.25, Perdas Totais: -33958.75\n",
      "Modelo e log do episódio 46 salvos em: 4.8\\model_episode_46.pth e 4.8\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: -1999.25, Win Rate: 0.53, Wins: 1387, Losses: 1229, Epsilon: 0.3118, Steps: 36754, Time: 109.80s\n",
      "Ações: Manter=10266, Comprar=13582, Vender=12906\n",
      "Ganhos Totais: 36342.25, Perdas Totais: -38341.50\n",
      "Episode 48/100, Total Reward: -5899.75, Win Rate: 0.51, Wins: 1226, Losses: 1163, Epsilon: 0.3086, Steps: 36754, Time: 110.37s\n",
      "Ações: Manter=12977, Comprar=10939, Vender=12838\n",
      "Ganhos Totais: 33075.75, Perdas Totais: -38975.50\n",
      "Episode 49/100, Total Reward: -4277.75, Win Rate: 0.51, Wins: 1179, Losses: 1142, Epsilon: 0.3056, Steps: 36754, Time: 110.13s\n",
      "Ações: Manter=14808, Comprar=11027, Vender=10919\n",
      "Ganhos Totais: 32425.50, Perdas Totais: -36703.25\n",
      "Episode 50/100, Total Reward: -595.25, Win Rate: 0.54, Wins: 1370, Losses: 1163, Epsilon: 0.3025, Steps: 36754, Time: 110.29s\n",
      "Ações: Manter=14135, Comprar=10163, Vender=12456\n",
      "Ganhos Totais: 36129.25, Perdas Totais: -36724.50\n",
      "Episode 51/100, Total Reward: -2030.50, Win Rate: 0.54, Wins: 1275, Losses: 1071, Epsilon: 0.2995, Steps: 36754, Time: 110.61s\n",
      "Ações: Manter=14804, Comprar=10919, Vender=11031\n",
      "Ganhos Totais: 34437.75, Perdas Totais: -36468.25\n",
      "Episode 52/100, Total Reward: -4754.50, Win Rate: 0.52, Wins: 1358, Losses: 1275, Epsilon: 0.2965, Steps: 36754, Time: 110.58s\n",
      "Ações: Manter=10554, Comprar=12259, Vender=13941\n",
      "Ganhos Totais: 35362.50, Perdas Totais: -40117.00\n",
      "Episode 53/100, Total Reward: -3386.50, Win Rate: 0.53, Wins: 1392, Losses: 1212, Epsilon: 0.2935, Steps: 36754, Time: 110.56s\n",
      "Ações: Manter=10541, Comprar=12160, Vender=14053\n",
      "Ganhos Totais: 35142.25, Perdas Totais: -38528.75\n",
      "Episode 54/100, Total Reward: -4510.50, Win Rate: 0.51, Wins: 1238, Losses: 1176, Epsilon: 0.2906, Steps: 36754, Time: 110.27s\n",
      "Ações: Manter=12017, Comprar=11959, Vender=12778\n",
      "Ganhos Totais: 34268.50, Perdas Totais: -38779.00\n",
      "Episode 55/100, Total Reward: -4403.00, Win Rate: 0.51, Wins: 1209, Losses: 1174, Epsilon: 0.2877, Steps: 36754, Time: 110.46s\n",
      "Ações: Manter=11734, Comprar=12681, Vender=12339\n",
      "Ganhos Totais: 35643.75, Perdas Totais: -40046.75\n",
      "Episode 56/100, Total Reward: -2569.75, Win Rate: 0.52, Wins: 1189, Losses: 1107, Epsilon: 0.2848, Steps: 36754, Time: 110.74s\n",
      "Ações: Manter=13643, Comprar=10677, Vender=12434\n",
      "Ganhos Totais: 33865.25, Perdas Totais: -36435.00\n",
      "Episode 57/100, Total Reward: -2233.75, Win Rate: 0.51, Wins: 1197, Losses: 1154, Epsilon: 0.2820, Steps: 36754, Time: 110.63s\n",
      "Ações: Manter=13554, Comprar=11463, Vender=11737\n",
      "Ganhos Totais: 35277.75, Perdas Totais: -37511.50\n",
      "Episode 58/100, Total Reward: -6385.00, Win Rate: 0.52, Wins: 1043, Losses: 965, Epsilon: 0.2791, Steps: 36754, Time: 111.25s\n",
      "Ações: Manter=16361, Comprar=8223, Vender=12170\n",
      "Ganhos Totais: 30635.00, Perdas Totais: -37020.00\n",
      "Episode 59/100, Total Reward: 209.75, Win Rate: 0.54, Wins: 1394, Losses: 1199, Epsilon: 0.2763, Steps: 36754, Time: 110.91s\n",
      "Ações: Manter=11568, Comprar=12063, Vender=13123\n",
      "Ganhos Totais: 37192.00, Perdas Totais: -36982.25\n",
      "Episode 60/100, Total Reward: -1628.75, Win Rate: 0.55, Wins: 1391, Losses: 1160, Epsilon: 0.2736, Steps: 36754, Time: 111.01s\n",
      "Ações: Manter=12732, Comprar=11330, Vender=12692\n",
      "Ganhos Totais: 36372.75, Perdas Totais: -38001.50\n",
      "Episode 61/100, Total Reward: -5281.00, Win Rate: 0.53, Wins: 1227, Losses: 1096, Epsilon: 0.2708, Steps: 36754, Time: 110.85s\n",
      "Ações: Manter=10390, Comprar=13590, Vender=12774\n",
      "Ganhos Totais: 33984.50, Perdas Totais: -39265.50\n",
      "Episode 62/100, Total Reward: -6073.00, Win Rate: 0.52, Wins: 1270, Losses: 1167, Epsilon: 0.2681, Steps: 36754, Time: 111.02s\n",
      "Ações: Manter=10518, Comprar=13221, Vender=13015\n",
      "Ganhos Totais: 33374.00, Perdas Totais: -39447.00\n",
      "Episode 63/100, Total Reward: -826.75, Win Rate: 0.55, Wins: 1341, Losses: 1095, Epsilon: 0.2655, Steps: 36754, Time: 110.84s\n",
      "Ações: Manter=10976, Comprar=15321, Vender=10457\n",
      "Ganhos Totais: 36899.50, Perdas Totais: -37726.25\n",
      "Episode 64/100, Total Reward: -4243.75, Win Rate: 0.53, Wins: 1201, Losses: 1051, Epsilon: 0.2628, Steps: 36754, Time: 110.58s\n",
      "Ações: Manter=13293, Comprar=11882, Vender=11579\n",
      "Ganhos Totais: 33784.75, Perdas Totais: -38028.50\n",
      "Episode 65/100, Total Reward: 73.00, Win Rate: 0.53, Wins: 1315, Losses: 1189, Epsilon: 0.2602, Steps: 36754, Time: 111.01s\n",
      "Ações: Manter=11545, Comprar=11824, Vender=13385\n",
      "Ganhos Totais: 36679.75, Perdas Totais: -36606.75\n",
      "Episode 66/100, Total Reward: -2885.75, Win Rate: 0.54, Wins: 1388, Losses: 1195, Epsilon: 0.2576, Steps: 36754, Time: 111.31s\n",
      "Ações: Manter=11381, Comprar=15081, Vender=10292\n",
      "Ganhos Totais: 35252.25, Perdas Totais: -38138.00\n",
      "Episode 67/100, Total Reward: 79.25, Win Rate: 0.53, Wins: 1454, Losses: 1278, Epsilon: 0.2550, Steps: 36754, Time: 111.10s\n",
      "Ações: Manter=11510, Comprar=12655, Vender=12589\n",
      "Ganhos Totais: 38216.75, Perdas Totais: -38137.50\n",
      "Episode 68/100, Total Reward: -1599.25, Win Rate: 0.54, Wins: 1420, Losses: 1205, Epsilon: 0.2524, Steps: 36754, Time: 111.07s\n",
      "Ações: Manter=12895, Comprar=10381, Vender=13478\n",
      "Ganhos Totais: 36624.25, Perdas Totais: -38223.50\n",
      "Episode 69/100, Total Reward: -1747.75, Win Rate: 0.54, Wins: 1178, Losses: 1017, Epsilon: 0.2499, Steps: 36754, Time: 111.11s\n",
      "Ações: Manter=13649, Comprar=13457, Vender=9648\n",
      "Ganhos Totais: 34682.25, Perdas Totais: -36430.00\n",
      "Episode 70/100, Total Reward: -2333.75, Win Rate: 0.54, Wins: 1211, Losses: 1034, Epsilon: 0.2474, Steps: 36754, Time: 111.58s\n",
      "Ações: Manter=13517, Comprar=8437, Vender=14800\n",
      "Ganhos Totais: 34734.00, Perdas Totais: -37067.75\n",
      "Episode 71/100, Total Reward: -249.75, Win Rate: 0.55, Wins: 1410, Losses: 1155, Epsilon: 0.2449, Steps: 36754, Time: 111.20s\n",
      "Ações: Manter=11818, Comprar=12108, Vender=12828\n",
      "Ganhos Totais: 36219.75, Perdas Totais: -36469.50\n",
      "Episode 72/100, Total Reward: -7374.25, Win Rate: 0.51, Wins: 1435, Losses: 1380, Epsilon: 0.2425, Steps: 36754, Time: 111.38s\n",
      "Ações: Manter=10376, Comprar=12683, Vender=13695\n",
      "Ganhos Totais: 33293.75, Perdas Totais: -40668.00\n",
      "Episode 73/100, Total Reward: -2588.25, Win Rate: 0.54, Wins: 1280, Losses: 1090, Epsilon: 0.2401, Steps: 36754, Time: 111.18s\n",
      "Ações: Manter=13832, Comprar=11402, Vender=11520\n",
      "Ganhos Totais: 34560.00, Perdas Totais: -37148.25\n",
      "Episode 74/100, Total Reward: -4110.25, Win Rate: 0.53, Wins: 1417, Losses: 1244, Epsilon: 0.2377, Steps: 36754, Time: 111.07s\n",
      "Ações: Manter=10429, Comprar=11662, Vender=14663\n",
      "Ganhos Totais: 34964.75, Perdas Totais: -39075.00\n",
      "Episode 75/100, Total Reward: -2935.25, Win Rate: 0.54, Wins: 1337, Losses: 1161, Epsilon: 0.2353, Steps: 36754, Time: 111.46s\n",
      "Ações: Manter=12858, Comprar=11212, Vender=12684\n",
      "Ganhos Totais: 35455.50, Perdas Totais: -38390.75\n",
      "Episode 76/100, Total Reward: -3271.00, Win Rate: 0.53, Wins: 1186, Losses: 1065, Epsilon: 0.2329, Steps: 36754, Time: 111.44s\n",
      "Ações: Manter=14599, Comprar=10012, Vender=12143\n",
      "Ganhos Totais: 33471.25, Perdas Totais: -36742.25\n",
      "Episode 77/100, Total Reward: -4469.50, Win Rate: 0.52, Wins: 959, Losses: 884, Epsilon: 0.2306, Steps: 36754, Time: 111.48s\n",
      "Ações: Manter=18056, Comprar=6371, Vender=12327\n",
      "Ganhos Totais: 31554.25, Perdas Totais: -36023.75\n",
      "Episode 78/100, Total Reward: -5348.25, Win Rate: 0.52, Wins: 1201, Losses: 1109, Epsilon: 0.2283, Steps: 36754, Time: 111.53s\n",
      "Ações: Manter=14337, Comprar=10339, Vender=12078\n",
      "Ganhos Totais: 33258.75, Perdas Totais: -38607.00\n",
      "Episode 79/100, Total Reward: -4597.75, Win Rate: 0.55, Wins: 1328, Losses: 1101, Epsilon: 0.2260, Steps: 36754, Time: 111.84s\n",
      "Ações: Manter=12265, Comprar=10619, Vender=13870\n",
      "Ganhos Totais: 34217.25, Perdas Totais: -38815.00\n",
      "Episode 80/100, Total Reward: -6541.00, Win Rate: 0.53, Wins: 1309, Losses: 1140, Epsilon: 0.2238, Steps: 36754, Time: 111.98s\n",
      "Ações: Manter=11838, Comprar=12835, Vender=12081\n",
      "Ganhos Totais: 33716.00, Perdas Totais: -40257.00\n",
      "Episode 81/100, Total Reward: -4371.50, Win Rate: 0.51, Wins: 1201, Losses: 1145, Epsilon: 0.2215, Steps: 36754, Time: 111.56s\n",
      "Ações: Manter=15815, Comprar=9054, Vender=11885\n",
      "Ganhos Totais: 32960.00, Perdas Totais: -37331.50\n",
      "Episode 82/100, Total Reward: -4561.50, Win Rate: 0.55, Wins: 1346, Losses: 1083, Epsilon: 0.2193, Steps: 36754, Time: 111.77s\n",
      "Ações: Manter=13992, Comprar=9832, Vender=12930\n",
      "Ganhos Totais: 34156.75, Perdas Totais: -38718.25\n",
      "Episode 83/100, Total Reward: -1970.25, Win Rate: 0.55, Wins: 1234, Losses: 1030, Epsilon: 0.2171, Steps: 36754, Time: 112.11s\n",
      "Ações: Manter=15025, Comprar=10564, Vender=11165\n",
      "Ganhos Totais: 33209.00, Perdas Totais: -35179.25\n",
      "Episode 84/100, Total Reward: -2865.75, Win Rate: 0.54, Wins: 1117, Losses: 966, Epsilon: 0.2149, Steps: 36754, Time: 111.77s\n",
      "Ações: Manter=14458, Comprar=10803, Vender=11493\n",
      "Ganhos Totais: 32586.75, Perdas Totais: -35452.50\n",
      "Episode 85/100, Total Reward: 707.00, Win Rate: 0.55, Wins: 1323, Losses: 1094, Epsilon: 0.2128, Steps: 36754, Time: 111.72s\n",
      "Ações: Manter=14767, Comprar=10004, Vender=11983\n",
      "Ganhos Totais: 35556.75, Perdas Totais: -34849.75\n",
      "Episode 86/100, Total Reward: -3711.75, Win Rate: 0.54, Wins: 1278, Losses: 1107, Epsilon: 0.2107, Steps: 36754, Time: 111.76s\n",
      "Ações: Manter=13336, Comprar=10476, Vender=12942\n",
      "Ganhos Totais: 34298.75, Perdas Totais: -38010.50\n",
      "Episode 87/100, Total Reward: -2565.25, Win Rate: 0.55, Wins: 1284, Losses: 1058, Epsilon: 0.2086, Steps: 36754, Time: 111.96s\n",
      "Ações: Manter=15879, Comprar=10532, Vender=10343\n",
      "Ganhos Totais: 33229.25, Perdas Totais: -35794.50\n",
      "Episode 88/100, Total Reward: -5542.75, Win Rate: 0.52, Wins: 1119, Losses: 1034, Epsilon: 0.2065, Steps: 36754, Time: 111.83s\n",
      "Ações: Manter=13807, Comprar=9745, Vender=13202\n",
      "Ganhos Totais: 31405.50, Perdas Totais: -36948.25\n",
      "Episode 89/100, Total Reward: -4462.50, Win Rate: 0.53, Wins: 1267, Losses: 1108, Epsilon: 0.2044, Steps: 36754, Time: 112.09s\n",
      "Ações: Manter=10888, Comprar=11551, Vender=14315\n",
      "Ganhos Totais: 33281.00, Perdas Totais: -37743.50\n",
      "Episode 90/100, Total Reward: -2192.00, Win Rate: 0.55, Wins: 1376, Losses: 1113, Epsilon: 0.2024, Steps: 36754, Time: 112.85s\n",
      "Ações: Manter=14079, Comprar=11123, Vender=11552\n",
      "Ganhos Totais: 34623.50, Perdas Totais: -36815.50\n",
      "Episode 91/100, Total Reward: -3290.50, Win Rate: 0.55, Wins: 1361, Losses: 1130, Epsilon: 0.2003, Steps: 36754, Time: 112.10s\n",
      "Ações: Manter=11517, Comprar=12408, Vender=12829\n",
      "Ganhos Totais: 34788.00, Perdas Totais: -38078.50\n",
      "Episode 92/100, Total Reward: -4647.50, Win Rate: 0.55, Wins: 1256, Losses: 1040, Epsilon: 0.1983, Steps: 36754, Time: 112.46s\n",
      "Ações: Manter=13678, Comprar=10544, Vender=12532\n",
      "Ganhos Totais: 32193.25, Perdas Totais: -36840.75\n",
      "Episode 93/100, Total Reward: -4675.25, Win Rate: 0.55, Wins: 1135, Losses: 942, Epsilon: 0.1964, Steps: 36754, Time: 112.19s\n",
      "Ações: Manter=16469, Comprar=8278, Vender=12007\n",
      "Ganhos Totais: 31071.25, Perdas Totais: -35746.50\n",
      "Episode 94/100, Total Reward: -5923.00, Win Rate: 0.53, Wins: 1249, Losses: 1095, Epsilon: 0.1944, Steps: 36754, Time: 112.24s\n",
      "Ações: Manter=14522, Comprar=9853, Vender=12379\n",
      "Ganhos Totais: 32331.00, Perdas Totais: -38254.00\n",
      "Episode 95/100, Total Reward: -4587.25, Win Rate: 0.53, Wins: 1091, Losses: 974, Epsilon: 0.1924, Steps: 36754, Time: 111.86s\n",
      "Ações: Manter=16181, Comprar=8810, Vender=11763\n",
      "Ganhos Totais: 31530.50, Perdas Totais: -36117.75\n",
      "Episode 96/100, Total Reward: -5995.75, Win Rate: 0.54, Wins: 1271, Losses: 1098, Epsilon: 0.1905, Steps: 36754, Time: 112.12s\n",
      "Ações: Manter=15322, Comprar=11406, Vender=10026\n",
      "Ganhos Totais: 32935.75, Perdas Totais: -38931.50\n",
      "Episode 97/100, Total Reward: -1891.00, Win Rate: 0.55, Wins: 1347, Losses: 1087, Epsilon: 0.1886, Steps: 36754, Time: 112.17s\n",
      "Ações: Manter=11824, Comprar=11808, Vender=13122\n",
      "Ganhos Totais: 35203.75, Perdas Totais: -37094.75\n",
      "Episode 98/100, Total Reward: -4039.00, Win Rate: 0.55, Wins: 1137, Losses: 929, Epsilon: 0.1867, Steps: 36754, Time: 111.98s\n",
      "Ações: Manter=16552, Comprar=9526, Vender=10676\n",
      "Ganhos Totais: 30878.50, Perdas Totais: -34917.50\n",
      "Episode 99/100, Total Reward: -4312.50, Win Rate: 0.55, Wins: 1291, Losses: 1071, Epsilon: 0.1849, Steps: 36754, Time: 112.33s\n",
      "Ações: Manter=13990, Comprar=10973, Vender=11791\n",
      "Ganhos Totais: 32831.00, Perdas Totais: -37143.50\n",
      "Episode 100/100, Total Reward: -3818.00, Win Rate: 0.54, Wins: 1287, Losses: 1088, Epsilon: 0.1830, Steps: 36754, Time: 105.49s\n",
      "Ações: Manter=12969, Comprar=12412, Vender=11373\n",
      "Ganhos Totais: 32731.25, Perdas Totais: -36549.25\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 46, Total Reward: 3935.50, Win Rate: 0.54, Wins: 1343, Losses: 1153, Ações: {0: 12166, 1: 11464, 2: 13124}, Steps: 36754, Time: 109.87s\n",
      "Rank 2: Episode 26, Total Reward: 3438.00, Win Rate: 0.54, Wins: 1278, Losses: 1087, Ações: {0: 13762, 1: 11504, 2: 11488}, Steps: 36754, Time: 108.99s\n",
      "Rank 3: Episode 21, Total Reward: 2845.50, Win Rate: 0.54, Wins: 1401, Losses: 1206, Ações: {0: 14060, 1: 11533, 2: 11161}, Steps: 36754, Time: 124.99s\n",
      "Rank 4: Episode 24, Total Reward: 2841.00, Win Rate: 0.54, Wins: 1458, Losses: 1260, Ações: {0: 10953, 1: 12745, 2: 13056}, Steps: 36754, Time: 108.96s\n",
      "Rank 5: Episode 41, Total Reward: 2356.50, Win Rate: 0.52, Wins: 1331, Losses: 1227, Ações: {0: 11213, 1: 13361, 2: 12180}, Steps: 36754, Time: 109.91s\n",
      "Rank 6: Episode 32, Total Reward: 2163.50, Win Rate: 0.52, Wins: 1346, Losses: 1221, Ações: {0: 11579, 1: 11808, 2: 13367}, Steps: 36754, Time: 109.64s\n",
      "Rank 7: Episode 2, Total Reward: 2085.75, Win Rate: 0.52, Wins: 1467, Losses: 1367, Ações: {0: 9243, 1: 12381, 2: 15130}, Steps: 36754, Time: 125.40s\n",
      "Rank 8: Episode 5, Total Reward: 2045.50, Win Rate: 0.53, Wins: 1433, Losses: 1295, Ações: {0: 11568, 1: 13498, 2: 11688}, Steps: 36754, Time: 124.51s\n",
      "Rank 9: Episode 11, Total Reward: 1738.75, Win Rate: 0.53, Wins: 1452, Losses: 1304, Ações: {0: 11877, 1: 13471, 2: 11406}, Steps: 36754, Time: 123.45s\n",
      "Rank 10: Episode 20, Total Reward: 1708.25, Win Rate: 0.54, Wins: 1462, Losses: 1231, Ações: {0: 11596, 1: 13939, 2: 11219}, Steps: 36754, Time: 124.50s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL','Hour','Minute'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.8\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
