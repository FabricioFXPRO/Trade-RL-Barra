{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -571.50, Win Rate: 0.51, Wins: 1315, Losses: 1251, Epsilon: 0.4950, Steps: 36754, Time: 134.13s\n",
      "Ações: Manter=12581, Comprar=12672, Vender=11501\n",
      "Ganhos Totais: 36621.25, Perdas Totais: -37192.75\n",
      "Modelo e log do episódio 1 salvos em: 4.8.4\\model_episode_1.pth e 4.8.4\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: 1988.00, Win Rate: 0.51, Wins: 1417, Losses: 1351, Epsilon: 0.4900, Steps: 36754, Time: 137.80s\n",
      "Ações: Manter=12170, Comprar=12546, Vender=12038\n",
      "Ganhos Totais: 38424.75, Perdas Totais: -36436.75\n",
      "Modelo e log do episódio 2 salvos em: 4.8.4\\model_episode_2.pth e 4.8.4\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1742.50, Win Rate: 0.50, Wins: 1452, Losses: 1437, Epsilon: 0.4851, Steps: 36754, Time: 134.36s\n",
      "Ações: Manter=10481, Comprar=13059, Vender=13214\n",
      "Ganhos Totais: 37778.50, Perdas Totais: -39521.00\n",
      "Modelo e log do episódio 3 salvos em: 4.8.4\\model_episode_3.pth e 4.8.4\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -4029.50, Win Rate: 0.50, Wins: 1370, Losses: 1372, Epsilon: 0.4803, Steps: 36754, Time: 132.94s\n",
      "Ações: Manter=12006, Comprar=11631, Vender=13117\n",
      "Ganhos Totais: 36093.50, Perdas Totais: -40123.00\n",
      "Modelo e log do episódio 4 salvos em: 4.8.4\\model_episode_4.pth e 4.8.4\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -4746.50, Win Rate: 0.51, Wins: 1476, Losses: 1395, Epsilon: 0.4755, Steps: 36754, Time: 134.94s\n",
      "Ações: Manter=11371, Comprar=11103, Vender=14280\n",
      "Ganhos Totais: 35207.75, Perdas Totais: -39954.25\n",
      "Modelo e log do episódio 5 salvos em: 4.8.4\\model_episode_5.pth e 4.8.4\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -3866.00, Win Rate: 0.52, Wins: 1467, Losses: 1376, Epsilon: 0.4707, Steps: 36754, Time: 136.68s\n",
      "Ações: Manter=11211, Comprar=12335, Vender=13208\n",
      "Ganhos Totais: 35996.00, Perdas Totais: -39862.00\n",
      "Modelo e log do episódio 6 salvos em: 4.8.4\\model_episode_6.pth e 4.8.4\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -3499.00, Win Rate: 0.52, Wins: 1499, Losses: 1408, Epsilon: 0.4660, Steps: 36754, Time: 132.57s\n",
      "Ações: Manter=12637, Comprar=11265, Vender=12852\n",
      "Ganhos Totais: 36927.00, Perdas Totais: -40426.00\n",
      "Modelo e log do episódio 7 salvos em: 4.8.4\\model_episode_7.pth e 4.8.4\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -1179.50, Win Rate: 0.51, Wins: 1520, Losses: 1433, Epsilon: 0.4614, Steps: 36754, Time: 136.41s\n",
      "Ações: Manter=11211, Comprar=12900, Vender=12643\n",
      "Ganhos Totais: 37975.00, Perdas Totais: -39154.50\n",
      "Modelo e log do episódio 8 salvos em: 4.8.4\\model_episode_8.pth e 4.8.4\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -3569.50, Win Rate: 0.51, Wins: 1310, Losses: 1262, Epsilon: 0.4568, Steps: 36754, Time: 134.65s\n",
      "Ações: Manter=12788, Comprar=11823, Vender=12143\n",
      "Ganhos Totais: 35378.00, Perdas Totais: -38947.50\n",
      "Modelo e log do episódio 9 salvos em: 4.8.4\\model_episode_9.pth e 4.8.4\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -1508.50, Win Rate: 0.52, Wins: 1346, Losses: 1241, Epsilon: 0.4522, Steps: 36754, Time: 135.60s\n",
      "Ações: Manter=12311, Comprar=11996, Vender=12447\n",
      "Ganhos Totais: 36658.75, Perdas Totais: -38167.25\n",
      "Modelo e log do episódio 10 salvos em: 4.8.4\\model_episode_10.pth e 4.8.4\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -4614.50, Win Rate: 0.52, Wins: 1426, Losses: 1321, Epsilon: 0.4477, Steps: 36754, Time: 137.15s\n",
      "Ações: Manter=13423, Comprar=11083, Vender=12248\n",
      "Ganhos Totais: 35081.25, Perdas Totais: -39695.75\n",
      "Modelo e log do episódio 11 salvos em: 4.8.4\\model_episode_11.pth e 4.8.4\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -4420.75, Win Rate: 0.52, Wins: 1387, Losses: 1300, Epsilon: 0.4432, Steps: 36754, Time: 137.10s\n",
      "Ações: Manter=12981, Comprar=11989, Vender=11784\n",
      "Ganhos Totais: 33953.50, Perdas Totais: -38374.25\n",
      "Modelo e log do episódio 12 salvos em: 4.8.4\\model_episode_12.pth e 4.8.4\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -3484.75, Win Rate: 0.51, Wins: 1347, Losses: 1271, Epsilon: 0.4388, Steps: 36754, Time: 136.56s\n",
      "Ações: Manter=11539, Comprar=12397, Vender=12818\n",
      "Ganhos Totais: 35772.50, Perdas Totais: -39257.25\n",
      "Modelo e log do episódio 13 salvos em: 4.8.4\\model_episode_13.pth e 4.8.4\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -1399.00, Win Rate: 0.52, Wins: 1412, Losses: 1316, Epsilon: 0.4344, Steps: 36754, Time: 136.70s\n",
      "Ações: Manter=11681, Comprar=12172, Vender=12901\n",
      "Ganhos Totais: 36975.50, Perdas Totais: -38374.50\n",
      "Modelo e log do episódio 14 salvos em: 4.8.4\\model_episode_14.pth e 4.8.4\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -495.00, Win Rate: 0.51, Wins: 1357, Losses: 1286, Epsilon: 0.4300, Steps: 36754, Time: 133.54s\n",
      "Ações: Manter=12531, Comprar=11905, Vender=12318\n",
      "Ganhos Totais: 37098.25, Perdas Totais: -37593.25\n",
      "Modelo e log do episódio 15 salvos em: 4.8.4\\model_episode_15.pth e 4.8.4\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -5239.25, Win Rate: 0.49, Wins: 1312, Losses: 1339, Epsilon: 0.4257, Steps: 36754, Time: 136.37s\n",
      "Ações: Manter=12339, Comprar=12260, Vender=12155\n",
      "Ganhos Totais: 34581.50, Perdas Totais: -39820.75\n",
      "Episode 17/100, Total Reward: -4811.50, Win Rate: 0.52, Wins: 1377, Losses: 1252, Epsilon: 0.4215, Steps: 36754, Time: 138.49s\n",
      "Ações: Manter=12395, Comprar=11524, Vender=12835\n",
      "Ganhos Totais: 34499.50, Perdas Totais: -39311.00\n",
      "Episode 18/100, Total Reward: -4010.00, Win Rate: 0.51, Wins: 1374, Losses: 1295, Epsilon: 0.4173, Steps: 36754, Time: 137.16s\n",
      "Ações: Manter=12215, Comprar=12421, Vender=12118\n",
      "Ganhos Totais: 36500.50, Perdas Totais: -40510.50\n",
      "Episode 19/100, Total Reward: -4990.50, Win Rate: 0.52, Wins: 1394, Losses: 1301, Epsilon: 0.4131, Steps: 36754, Time: 137.53s\n",
      "Ações: Manter=13067, Comprar=11472, Vender=12215\n",
      "Ganhos Totais: 34268.50, Perdas Totais: -39259.00\n",
      "Episode 20/100, Total Reward: -3049.25, Win Rate: 0.51, Wins: 1402, Losses: 1343, Epsilon: 0.4090, Steps: 36754, Time: 138.67s\n",
      "Ações: Manter=13196, Comprar=12168, Vender=11390\n",
      "Ganhos Totais: 36250.50, Perdas Totais: -39299.75\n",
      "Modelo e log do episódio 20 salvos em: 4.8.4\\model_episode_20.pth e 4.8.4\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -1844.75, Win Rate: 0.55, Wins: 1612, Losses: 1336, Epsilon: 0.4049, Steps: 36754, Time: 137.08s\n",
      "Ações: Manter=11579, Comprar=12846, Vender=12329\n",
      "Ganhos Totais: 37454.75, Perdas Totais: -39299.50\n",
      "Modelo e log do episódio 21 salvos em: 4.8.4\\model_episode_21.pth e 4.8.4\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -807.75, Win Rate: 0.54, Wins: 1418, Losses: 1229, Epsilon: 0.4008, Steps: 36754, Time: 137.97s\n",
      "Ações: Manter=11886, Comprar=12420, Vender=12448\n",
      "Ganhos Totais: 36161.25, Perdas Totais: -36969.00\n",
      "Modelo e log do episódio 22 salvos em: 4.8.4\\model_episode_22.pth e 4.8.4\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: 855.00, Win Rate: 0.53, Wins: 1367, Losses: 1199, Epsilon: 0.3968, Steps: 36754, Time: 138.44s\n",
      "Ações: Manter=12132, Comprar=13691, Vender=10931\n",
      "Ganhos Totais: 37922.50, Perdas Totais: -37067.50\n",
      "Modelo e log do episódio 23 salvos em: 4.8.4\\model_episode_23.pth e 4.8.4\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: -4731.25, Win Rate: 0.53, Wins: 1325, Losses: 1192, Epsilon: 0.3928, Steps: 36754, Time: 135.57s\n",
      "Ações: Manter=13431, Comprar=12307, Vender=11016\n",
      "Ganhos Totais: 33581.75, Perdas Totais: -38313.00\n",
      "Episode 25/100, Total Reward: -4798.50, Win Rate: 0.52, Wins: 1317, Losses: 1237, Epsilon: 0.3889, Steps: 36754, Time: 136.36s\n",
      "Ações: Manter=13015, Comprar=11271, Vender=12468\n",
      "Ganhos Totais: 34299.25, Perdas Totais: -39097.75\n",
      "Episode 26/100, Total Reward: -2860.75, Win Rate: 0.52, Wins: 1396, Losses: 1290, Epsilon: 0.3850, Steps: 36754, Time: 138.11s\n",
      "Ações: Manter=12912, Comprar=11388, Vender=12454\n",
      "Ganhos Totais: 35835.50, Perdas Totais: -38696.25\n",
      "Episode 27/100, Total Reward: -741.75, Win Rate: 0.53, Wins: 1316, Losses: 1159, Epsilon: 0.3812, Steps: 36754, Time: 135.28s\n",
      "Ações: Manter=13590, Comprar=11729, Vender=11435\n",
      "Ganhos Totais: 35912.50, Perdas Totais: -36654.25\n",
      "Modelo e log do episódio 27 salvos em: 4.8.4\\model_episode_27.pth e 4.8.4\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: -3449.75, Win Rate: 0.52, Wins: 1329, Losses: 1224, Epsilon: 0.3774, Steps: 36754, Time: 137.74s\n",
      "Ações: Manter=13728, Comprar=11466, Vender=11560\n",
      "Ganhos Totais: 35311.25, Perdas Totais: -38761.00\n",
      "Episode 29/100, Total Reward: -2291.25, Win Rate: 0.53, Wins: 1407, Losses: 1249, Epsilon: 0.3736, Steps: 36754, Time: 137.81s\n",
      "Ações: Manter=13991, Comprar=11599, Vender=11164\n",
      "Ganhos Totais: 35917.75, Perdas Totais: -38209.00\n",
      "Episode 30/100, Total Reward: -37.50, Win Rate: 0.52, Wins: 1360, Losses: 1249, Epsilon: 0.3699, Steps: 36754, Time: 138.50s\n",
      "Ações: Manter=13183, Comprar=12052, Vender=11519\n",
      "Ganhos Totais: 36179.75, Perdas Totais: -36217.25\n",
      "Modelo e log do episódio 30 salvos em: 4.8.4\\model_episode_30.pth e 4.8.4\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -1541.25, Win Rate: 0.55, Wins: 1493, Losses: 1209, Epsilon: 0.3662, Steps: 36754, Time: 139.19s\n",
      "Ações: Manter=11899, Comprar=12511, Vender=12344\n",
      "Ganhos Totais: 37272.50, Perdas Totais: -38813.75\n",
      "Episode 32/100, Total Reward: -4.50, Win Rate: 0.54, Wins: 1391, Losses: 1189, Epsilon: 0.3625, Steps: 36754, Time: 137.90s\n",
      "Ações: Manter=13345, Comprar=12703, Vender=10706\n",
      "Ganhos Totais: 37282.50, Perdas Totais: -37287.00\n",
      "Modelo e log do episódio 32 salvos em: 4.8.4\\model_episode_32.pth e 4.8.4\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 1451.50, Win Rate: 0.54, Wins: 1365, Losses: 1181, Epsilon: 0.3589, Steps: 36754, Time: 138.89s\n",
      "Ações: Manter=13937, Comprar=12716, Vender=10101\n",
      "Ganhos Totais: 37208.50, Perdas Totais: -35757.00\n",
      "Modelo e log do episódio 33 salvos em: 4.8.4\\model_episode_33.pth e 4.8.4\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: 2002.50, Win Rate: 0.54, Wins: 1436, Losses: 1214, Epsilon: 0.3553, Steps: 36754, Time: 139.83s\n",
      "Ações: Manter=12522, Comprar=13637, Vender=10595\n",
      "Ganhos Totais: 39573.75, Perdas Totais: -37571.25\n",
      "Modelo e log do episódio 34 salvos em: 4.8.4\\model_episode_34.pth e 4.8.4\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: -820.00, Win Rate: 0.54, Wins: 1442, Losses: 1231, Epsilon: 0.3517, Steps: 36754, Time: 135.46s\n",
      "Ações: Manter=12334, Comprar=13709, Vender=10711\n",
      "Ganhos Totais: 37414.25, Perdas Totais: -38234.25\n",
      "Episode 36/100, Total Reward: 1168.75, Win Rate: 0.54, Wins: 1416, Losses: 1186, Epsilon: 0.3482, Steps: 36754, Time: 139.62s\n",
      "Ações: Manter=13222, Comprar=13347, Vender=10185\n",
      "Ganhos Totais: 38170.25, Perdas Totais: -37001.50\n",
      "Modelo e log do episódio 36 salvos em: 4.8.4\\model_episode_36.pth e 4.8.4\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: 1041.00, Win Rate: 0.54, Wins: 1364, Losses: 1146, Epsilon: 0.3447, Steps: 36754, Time: 139.02s\n",
      "Ações: Manter=14776, Comprar=12130, Vender=9848\n",
      "Ganhos Totais: 37045.50, Perdas Totais: -36004.50\n",
      "Modelo e log do episódio 37 salvos em: 4.8.4\\model_episode_37.pth e 4.8.4\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: -2528.50, Win Rate: 0.54, Wins: 1295, Losses: 1100, Epsilon: 0.3413, Steps: 36754, Time: 138.84s\n",
      "Ações: Manter=15467, Comprar=11741, Vender=9546\n",
      "Ganhos Totais: 34250.50, Perdas Totais: -36779.00\n",
      "Episode 39/100, Total Reward: -3388.75, Win Rate: 0.52, Wins: 1349, Losses: 1231, Epsilon: 0.3379, Steps: 36754, Time: 140.16s\n",
      "Ações: Manter=12321, Comprar=12902, Vender=11531\n",
      "Ganhos Totais: 34467.50, Perdas Totais: -37856.25\n",
      "Episode 40/100, Total Reward: -208.00, Win Rate: 0.54, Wins: 1422, Losses: 1209, Epsilon: 0.3345, Steps: 36754, Time: 138.62s\n",
      "Ações: Manter=12615, Comprar=14007, Vender=10132\n",
      "Ganhos Totais: 37586.25, Perdas Totais: -37794.25\n",
      "Modelo e log do episódio 40 salvos em: 4.8.4\\model_episode_40.pth e 4.8.4\\log_episode_40.csv\n",
      "\n",
      "Episode 41/100, Total Reward: -495.00, Win Rate: 0.53, Wins: 1420, Losses: 1247, Epsilon: 0.3311, Steps: 36754, Time: 139.55s\n",
      "Ações: Manter=12438, Comprar=13612, Vender=10704\n",
      "Ganhos Totais: 37112.75, Perdas Totais: -37607.75\n",
      "Episode 42/100, Total Reward: 528.75, Win Rate: 0.54, Wins: 1457, Losses: 1221, Epsilon: 0.3278, Steps: 36754, Time: 140.23s\n",
      "Ações: Manter=11814, Comprar=12968, Vender=11972\n",
      "Ganhos Totais: 38068.25, Perdas Totais: -37539.50\n",
      "Modelo e log do episódio 42 salvos em: 4.8.4\\model_episode_42.pth e 4.8.4\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: 1822.75, Win Rate: 0.56, Wins: 1419, Losses: 1133, Epsilon: 0.3246, Steps: 36754, Time: 139.06s\n",
      "Ações: Manter=13408, Comprar=12204, Vender=11142\n",
      "Ganhos Totais: 37336.25, Perdas Totais: -35513.50\n",
      "Modelo e log do episódio 43 salvos em: 4.8.4\\model_episode_43.pth e 4.8.4\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -4569.50, Win Rate: 0.54, Wins: 1376, Losses: 1160, Epsilon: 0.3213, Steps: 36754, Time: 140.21s\n",
      "Ações: Manter=12656, Comprar=12516, Vender=11582\n",
      "Ganhos Totais: 33395.25, Perdas Totais: -37964.75\n",
      "Episode 45/100, Total Reward: -4153.50, Win Rate: 0.53, Wins: 1292, Losses: 1142, Epsilon: 0.3181, Steps: 36754, Time: 139.70s\n",
      "Ações: Manter=13528, Comprar=11245, Vender=11981\n",
      "Ganhos Totais: 33805.25, Perdas Totais: -37958.75\n",
      "Episode 46/100, Total Reward: -2065.50, Win Rate: 0.53, Wins: 1297, Losses: 1137, Epsilon: 0.3149, Steps: 36754, Time: 138.74s\n",
      "Ações: Manter=13706, Comprar=12113, Vender=10935\n",
      "Ganhos Totais: 34644.50, Perdas Totais: -36710.00\n",
      "Episode 47/100, Total Reward: -4166.00, Win Rate: 0.54, Wins: 1342, Losses: 1164, Epsilon: 0.3118, Steps: 36754, Time: 140.18s\n",
      "Ações: Manter=12945, Comprar=11095, Vender=12714\n",
      "Ganhos Totais: 35022.50, Perdas Totais: -39188.50\n",
      "Episode 48/100, Total Reward: -5794.00, Win Rate: 0.53, Wins: 1294, Losses: 1153, Epsilon: 0.3086, Steps: 36754, Time: 139.43s\n",
      "Ações: Manter=12763, Comprar=11453, Vender=12538\n",
      "Ganhos Totais: 33086.25, Perdas Totais: -38880.25\n",
      "Episode 49/100, Total Reward: -1144.75, Win Rate: 0.53, Wins: 1285, Losses: 1140, Epsilon: 0.3056, Steps: 36754, Time: 139.53s\n",
      "Ações: Manter=14181, Comprar=11140, Vender=11433\n",
      "Ganhos Totais: 35334.50, Perdas Totais: -36479.25\n",
      "Episode 50/100, Total Reward: -8072.00, Win Rate: 0.52, Wins: 1213, Losses: 1110, Epsilon: 0.3025, Steps: 36754, Time: 137.04s\n",
      "Ações: Manter=12676, Comprar=11743, Vender=12335\n",
      "Ganhos Totais: 32381.75, Perdas Totais: -40453.75\n",
      "Episode 51/100, Total Reward: -7233.50, Win Rate: 0.53, Wins: 1212, Losses: 1076, Epsilon: 0.2995, Steps: 36754, Time: 137.11s\n",
      "Ações: Manter=13972, Comprar=11321, Vender=11461\n",
      "Ganhos Totais: 31788.00, Perdas Totais: -39021.50\n",
      "Episode 52/100, Total Reward: -1999.00, Win Rate: 0.55, Wins: 1368, Losses: 1121, Epsilon: 0.2965, Steps: 36754, Time: 140.43s\n",
      "Ações: Manter=12149, Comprar=14336, Vender=10269\n",
      "Ganhos Totais: 34862.50, Perdas Totais: -36861.50\n",
      "Episode 53/100, Total Reward: 1411.75, Win Rate: 0.55, Wins: 1346, Losses: 1100, Epsilon: 0.2935, Steps: 36754, Time: 138.41s\n",
      "Ações: Manter=14431, Comprar=11510, Vender=10813\n",
      "Ganhos Totais: 36696.25, Perdas Totais: -35284.50\n",
      "Modelo e log do episódio 53 salvos em: 4.8.4\\model_episode_53.pth e 4.8.4\\log_episode_53.csv\n",
      "\n",
      "Episode 54/100, Total Reward: 915.00, Win Rate: 0.57, Wins: 1384, Losses: 1051, Epsilon: 0.2906, Steps: 36754, Time: 138.90s\n",
      "Ações: Manter=15284, Comprar=12803, Vender=8667\n",
      "Ganhos Totais: 35559.75, Perdas Totais: -34644.75\n",
      "Modelo e log do episódio 54 salvos em: 4.8.4\\model_episode_54.pth e 4.8.4\\log_episode_54.csv\n",
      "\n",
      "Episode 55/100, Total Reward: 1410.00, Win Rate: 0.57, Wins: 1368, Losses: 1033, Epsilon: 0.2877, Steps: 36754, Time: 136.80s\n",
      "Ações: Manter=14203, Comprar=12536, Vender=10015\n",
      "Ganhos Totais: 36470.25, Perdas Totais: -35060.25\n",
      "Modelo e log do episódio 55 salvos em: 4.8.4\\model_episode_55.pth e 4.8.4\\log_episode_55.csv\n",
      "\n",
      "Episode 56/100, Total Reward: -1317.75, Win Rate: 0.55, Wins: 1503, Losses: 1246, Epsilon: 0.2848, Steps: 36754, Time: 136.99s\n",
      "Ações: Manter=12127, Comprar=14389, Vender=10238\n",
      "Ganhos Totais: 36210.50, Perdas Totais: -37528.25\n",
      "Episode 57/100, Total Reward: -2230.50, Win Rate: 0.55, Wins: 1257, Losses: 1028, Epsilon: 0.2820, Steps: 36754, Time: 139.95s\n",
      "Ações: Manter=12974, Comprar=13136, Vender=10644\n",
      "Ganhos Totais: 33409.50, Perdas Totais: -35640.00\n",
      "Episode 58/100, Total Reward: -2501.25, Win Rate: 0.55, Wins: 1422, Losses: 1155, Epsilon: 0.2791, Steps: 36754, Time: 140.55s\n",
      "Ações: Manter=13130, Comprar=13647, Vender=9977\n",
      "Ganhos Totais: 35383.00, Perdas Totais: -37884.25\n",
      "Episode 59/100, Total Reward: -121.50, Win Rate: 0.56, Wins: 1341, Losses: 1033, Epsilon: 0.2763, Steps: 36754, Time: 139.30s\n",
      "Ações: Manter=12450, Comprar=13846, Vender=10458\n",
      "Ganhos Totais: 35722.00, Perdas Totais: -35843.50\n",
      "Episode 60/100, Total Reward: 2949.75, Win Rate: 0.55, Wins: 1413, Losses: 1154, Epsilon: 0.2736, Steps: 36754, Time: 140.20s\n",
      "Ações: Manter=13610, Comprar=13931, Vender=9213\n",
      "Ganhos Totais: 37934.00, Perdas Totais: -34984.25\n",
      "Modelo e log do episódio 60 salvos em: 4.8.4\\model_episode_60.pth e 4.8.4\\log_episode_60.csv\n",
      "\n",
      "Episode 61/100, Total Reward: -2103.50, Win Rate: 0.54, Wins: 1307, Losses: 1111, Epsilon: 0.2708, Steps: 36754, Time: 141.17s\n",
      "Ações: Manter=12116, Comprar=15510, Vender=9128\n",
      "Ganhos Totais: 35471.75, Perdas Totais: -37575.25\n",
      "Episode 62/100, Total Reward: -2540.25, Win Rate: 0.53, Wins: 1343, Losses: 1179, Epsilon: 0.2681, Steps: 36754, Time: 139.82s\n",
      "Ações: Manter=10472, Comprar=15332, Vender=10950\n",
      "Ganhos Totais: 35090.00, Perdas Totais: -37630.25\n",
      "Episode 63/100, Total Reward: -2007.25, Win Rate: 0.54, Wins: 1293, Losses: 1086, Epsilon: 0.2655, Steps: 36754, Time: 141.26s\n",
      "Ações: Manter=13001, Comprar=15610, Vender=8143\n",
      "Ganhos Totais: 35337.00, Perdas Totais: -37344.25\n",
      "Episode 64/100, Total Reward: 733.75, Win Rate: 0.54, Wins: 1260, Losses: 1063, Epsilon: 0.2628, Steps: 36754, Time: 140.41s\n",
      "Ações: Manter=13196, Comprar=14442, Vender=9116\n",
      "Ganhos Totais: 36470.50, Perdas Totais: -35736.75\n",
      "Episode 65/100, Total Reward: -3852.50, Win Rate: 0.54, Wins: 1185, Losses: 1016, Epsilon: 0.2602, Steps: 36754, Time: 140.00s\n",
      "Ações: Manter=12771, Comprar=14207, Vender=9776\n",
      "Ganhos Totais: 34161.50, Perdas Totais: -38014.00\n",
      "Episode 66/100, Total Reward: 5347.50, Win Rate: 0.56, Wins: 1283, Losses: 1017, Epsilon: 0.2576, Steps: 36754, Time: 141.58s\n",
      "Ações: Manter=12587, Comprar=15123, Vender=9044\n",
      "Ganhos Totais: 39151.75, Perdas Totais: -33804.25\n",
      "Modelo e log do episódio 66 salvos em: 4.8.4\\model_episode_66.pth e 4.8.4\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: -2925.50, Win Rate: 0.55, Wins: 1222, Losses: 995, Epsilon: 0.2550, Steps: 36754, Time: 140.37s\n",
      "Ações: Manter=12797, Comprar=14470, Vender=9487\n",
      "Ganhos Totais: 34885.25, Perdas Totais: -37810.75\n",
      "Episode 68/100, Total Reward: 186.25, Win Rate: 0.54, Wins: 1294, Losses: 1090, Epsilon: 0.2524, Steps: 36754, Time: 140.95s\n",
      "Ações: Manter=10753, Comprar=16465, Vender=9536\n",
      "Ganhos Totais: 35835.00, Perdas Totais: -35648.75\n",
      "Episode 69/100, Total Reward: 2189.50, Win Rate: 0.56, Wins: 1284, Losses: 1008, Epsilon: 0.2499, Steps: 36754, Time: 141.67s\n",
      "Ações: Manter=9452, Comprar=18235, Vender=9067\n",
      "Ganhos Totais: 38055.50, Perdas Totais: -35866.00\n",
      "Modelo e log do episódio 69 salvos em: 4.8.4\\model_episode_69.pth e 4.8.4\\log_episode_69.csv\n",
      "\n",
      "Episode 70/100, Total Reward: 2081.00, Win Rate: 0.55, Wins: 1307, Losses: 1059, Epsilon: 0.2474, Steps: 36754, Time: 140.25s\n",
      "Ações: Manter=11796, Comprar=13558, Vender=11400\n",
      "Ganhos Totais: 37098.50, Perdas Totais: -35017.50\n",
      "Modelo e log do episódio 70 salvos em: 4.8.4\\model_episode_70.pth e 4.8.4\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: 961.00, Win Rate: 0.56, Wins: 1240, Losses: 986, Epsilon: 0.2449, Steps: 36754, Time: 141.37s\n",
      "Ações: Manter=12744, Comprar=14860, Vender=9150\n",
      "Ganhos Totais: 36578.75, Perdas Totais: -35617.75\n",
      "Episode 72/100, Total Reward: 1729.75, Win Rate: 0.55, Wins: 1237, Losses: 993, Epsilon: 0.2425, Steps: 36754, Time: 140.65s\n",
      "Ações: Manter=11416, Comprar=16152, Vender=9186\n",
      "Ganhos Totais: 37159.25, Perdas Totais: -35429.50\n",
      "Modelo e log do episódio 72 salvos em: 4.8.4\\model_episode_72.pth e 4.8.4\\log_episode_72.csv\n",
      "\n",
      "Episode 73/100, Total Reward: 2166.00, Win Rate: 0.55, Wins: 1194, Losses: 985, Epsilon: 0.2401, Steps: 36754, Time: 140.32s\n",
      "Ações: Manter=13822, Comprar=13516, Vender=9416\n",
      "Ganhos Totais: 36830.50, Perdas Totais: -34664.50\n",
      "Modelo e log do episódio 73 salvos em: 4.8.4\\model_episode_73.pth e 4.8.4\\log_episode_73.csv\n",
      "\n",
      "Episode 74/100, Total Reward: -2074.50, Win Rate: 0.53, Wins: 1036, Losses: 914, Epsilon: 0.2377, Steps: 36754, Time: 141.54s\n",
      "Ações: Manter=12855, Comprar=15840, Vender=8059\n",
      "Ganhos Totais: 33283.00, Perdas Totais: -35357.50\n",
      "Episode 75/100, Total Reward: 1607.25, Win Rate: 0.56, Wins: 1092, Losses: 875, Epsilon: 0.2353, Steps: 36754, Time: 140.50s\n",
      "Ações: Manter=11073, Comprar=16217, Vender=9464\n",
      "Ganhos Totais: 34232.75, Perdas Totais: -32625.50\n",
      "Modelo e log do episódio 75 salvos em: 4.8.4\\model_episode_75.pth e 4.8.4\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: 1552.75, Win Rate: 0.55, Wins: 1200, Losses: 993, Epsilon: 0.2329, Steps: 36754, Time: 140.88s\n",
      "Ações: Manter=10961, Comprar=16009, Vender=9784\n",
      "Ganhos Totais: 36237.75, Perdas Totais: -34685.00\n",
      "Episode 77/100, Total Reward: 881.00, Win Rate: 0.56, Wins: 1123, Losses: 877, Epsilon: 0.2306, Steps: 36754, Time: 141.75s\n",
      "Ações: Manter=13011, Comprar=14450, Vender=9293\n",
      "Ganhos Totais: 34798.25, Perdas Totais: -33917.25\n",
      "Episode 78/100, Total Reward: 2810.75, Win Rate: 0.57, Wins: 1200, Losses: 910, Epsilon: 0.2283, Steps: 36754, Time: 140.33s\n",
      "Ações: Manter=12558, Comprar=13963, Vender=10233\n",
      "Ganhos Totais: 36568.25, Perdas Totais: -33757.50\n",
      "Modelo e log do episódio 78 salvos em: 4.8.4\\model_episode_78.pth e 4.8.4\\log_episode_78.csv\n",
      "\n",
      "Episode 79/100, Total Reward: 1378.50, Win Rate: 0.56, Wins: 1135, Losses: 887, Epsilon: 0.2260, Steps: 36754, Time: 138.69s\n",
      "Ações: Manter=12220, Comprar=15024, Vender=9510\n",
      "Ganhos Totais: 35465.25, Perdas Totais: -34086.75\n",
      "Episode 80/100, Total Reward: 5012.50, Win Rate: 0.55, Wins: 1120, Losses: 912, Epsilon: 0.2238, Steps: 36754, Time: 141.60s\n",
      "Ações: Manter=10856, Comprar=16995, Vender=8903\n",
      "Ganhos Totais: 37204.75, Perdas Totais: -32192.25\n",
      "Modelo e log do episódio 80 salvos em: 4.8.4\\model_episode_80.pth e 4.8.4\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: 1729.25, Win Rate: 0.56, Wins: 1084, Losses: 852, Epsilon: 0.2215, Steps: 36754, Time: 140.41s\n",
      "Ações: Manter=13898, Comprar=14304, Vender=8552\n",
      "Ganhos Totais: 34651.75, Perdas Totais: -32922.50\n",
      "Episode 82/100, Total Reward: 2774.25, Win Rate: 0.57, Wins: 1119, Losses: 851, Epsilon: 0.2193, Steps: 36754, Time: 141.95s\n",
      "Ações: Manter=10911, Comprar=16933, Vender=8910\n",
      "Ganhos Totais: 36579.25, Perdas Totais: -33805.00\n",
      "Modelo e log do episódio 82 salvos em: 4.8.4\\model_episode_82.pth e 4.8.4\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: 2100.25, Win Rate: 0.55, Wins: 1145, Losses: 928, Epsilon: 0.2171, Steps: 36754, Time: 140.61s\n",
      "Ações: Manter=12257, Comprar=14895, Vender=9602\n",
      "Ganhos Totais: 35778.50, Perdas Totais: -33678.25\n",
      "Modelo e log do episódio 83 salvos em: 4.8.4\\model_episode_83.pth e 4.8.4\\log_episode_83.csv\n",
      "\n",
      "Episode 84/100, Total Reward: 950.25, Win Rate: 0.57, Wins: 1177, Losses: 873, Epsilon: 0.2149, Steps: 36754, Time: 141.52s\n",
      "Ações: Manter=11072, Comprar=15874, Vender=9808\n",
      "Ganhos Totais: 35164.50, Perdas Totais: -34214.25\n",
      "Episode 85/100, Total Reward: 4289.75, Win Rate: 0.57, Wins: 1178, Losses: 886, Epsilon: 0.2128, Steps: 36754, Time: 141.68s\n",
      "Ações: Manter=12219, Comprar=15094, Vender=9441\n",
      "Ganhos Totais: 37266.00, Perdas Totais: -32976.25\n",
      "Modelo e log do episódio 85 salvos em: 4.8.4\\model_episode_85.pth e 4.8.4\\log_episode_85.csv\n",
      "\n",
      "Episode 86/100, Total Reward: 3105.75, Win Rate: 0.57, Wins: 1080, Losses: 818, Epsilon: 0.2107, Steps: 36754, Time: 140.07s\n",
      "Ações: Manter=13786, Comprar=13912, Vender=9056\n",
      "Ganhos Totais: 36736.50, Perdas Totais: -33630.75\n",
      "Modelo e log do episódio 86 salvos em: 4.8.4\\model_episode_86.pth e 4.8.4\\log_episode_86.csv\n",
      "\n",
      "Episode 87/100, Total Reward: 2640.25, Win Rate: 0.57, Wins: 1224, Losses: 927, Epsilon: 0.2086, Steps: 36754, Time: 141.73s\n",
      "Ações: Manter=12988, Comprar=14565, Vender=9201\n",
      "Ganhos Totais: 37053.25, Perdas Totais: -34413.00\n",
      "Modelo e log do episódio 87 salvos em: 4.8.4\\model_episode_87.pth e 4.8.4\\log_episode_87.csv\n",
      "\n",
      "Episode 88/100, Total Reward: 2502.00, Win Rate: 0.57, Wins: 1118, Losses: 831, Epsilon: 0.2065, Steps: 36754, Time: 141.34s\n",
      "Ações: Manter=11178, Comprar=16330, Vender=9246\n",
      "Ganhos Totais: 36715.25, Perdas Totais: -34213.25\n",
      "Modelo e log do episódio 88 salvos em: 4.8.4\\model_episode_88.pth e 4.8.4\\log_episode_88.csv\n",
      "\n",
      "Episode 89/100, Total Reward: 1375.50, Win Rate: 0.56, Wins: 1102, Losses: 854, Epsilon: 0.2044, Steps: 36754, Time: 141.15s\n",
      "Ações: Manter=11785, Comprar=15868, Vender=9101\n",
      "Ganhos Totais: 35513.50, Perdas Totais: -34138.00\n",
      "Episode 90/100, Total Reward: 3827.75, Win Rate: 0.57, Wins: 1203, Losses: 895, Epsilon: 0.2024, Steps: 36754, Time: 138.21s\n",
      "Ações: Manter=12350, Comprar=14489, Vender=9915\n",
      "Ganhos Totais: 37329.75, Perdas Totais: -33502.00\n",
      "Modelo e log do episódio 90 salvos em: 4.8.4\\model_episode_90.pth e 4.8.4\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: 4536.75, Win Rate: 0.58, Wins: 1168, Losses: 841, Epsilon: 0.2003, Steps: 36754, Time: 140.97s\n",
      "Ações: Manter=12927, Comprar=14014, Vender=9813\n",
      "Ganhos Totais: 37497.00, Perdas Totais: -32960.25\n",
      "Modelo e log do episódio 91 salvos em: 4.8.4\\model_episode_91.pth e 4.8.4\\log_episode_91.csv\n",
      "\n",
      "Episode 92/100, Total Reward: 2024.50, Win Rate: 0.59, Wins: 1150, Losses: 808, Epsilon: 0.1983, Steps: 36754, Time: 141.26s\n",
      "Ações: Manter=12586, Comprar=14834, Vender=9334\n",
      "Ganhos Totais: 35222.75, Perdas Totais: -33198.25\n",
      "Episode 93/100, Total Reward: 4619.50, Win Rate: 0.59, Wins: 1245, Losses: 849, Epsilon: 0.1964, Steps: 36754, Time: 142.13s\n",
      "Ações: Manter=13380, Comprar=14306, Vender=9068\n",
      "Ganhos Totais: 38004.75, Perdas Totais: -33385.25\n",
      "Modelo e log do episódio 93 salvos em: 4.8.4\\model_episode_93.pth e 4.8.4\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: 3661.00, Win Rate: 0.58, Wins: 1179, Losses: 843, Epsilon: 0.1944, Steps: 36754, Time: 140.72s\n",
      "Ações: Manter=12111, Comprar=15824, Vender=8819\n",
      "Ganhos Totais: 37118.00, Perdas Totais: -33457.00\n",
      "Modelo e log do episódio 94 salvos em: 4.8.4\\model_episode_94.pth e 4.8.4\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: 2236.00, Win Rate: 0.57, Wins: 1135, Losses: 840, Epsilon: 0.1924, Steps: 36754, Time: 142.03s\n",
      "Ações: Manter=12260, Comprar=15948, Vender=8546\n",
      "Ganhos Totais: 36612.75, Perdas Totais: -34376.75\n",
      "Episode 96/100, Total Reward: 2675.50, Win Rate: 0.60, Wins: 1233, Losses: 837, Epsilon: 0.1905, Steps: 36754, Time: 141.16s\n",
      "Ações: Manter=13141, Comprar=13955, Vender=9658\n",
      "Ganhos Totais: 37213.25, Perdas Totais: -34537.75\n",
      "Episode 97/100, Total Reward: 4410.00, Win Rate: 0.60, Wins: 1203, Losses: 800, Epsilon: 0.1886, Steps: 36754, Time: 140.61s\n",
      "Ações: Manter=11568, Comprar=16255, Vender=8931\n",
      "Ganhos Totais: 37176.50, Perdas Totais: -32766.50\n",
      "Modelo e log do episódio 97 salvos em: 4.8.4\\model_episode_97.pth e 4.8.4\\log_episode_97.csv\n",
      "\n",
      "Episode 98/100, Total Reward: 3967.00, Win Rate: 0.58, Wins: 1148, Losses: 818, Epsilon: 0.1867, Steps: 36754, Time: 142.40s\n",
      "Ações: Manter=12007, Comprar=15305, Vender=9442\n",
      "Ganhos Totais: 36942.25, Perdas Totais: -32975.25\n",
      "Modelo e log do episódio 98 salvos em: 4.8.4\\model_episode_98.pth e 4.8.4\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: 2780.25, Win Rate: 0.59, Wins: 1238, Losses: 854, Epsilon: 0.1849, Steps: 36754, Time: 141.35s\n",
      "Ações: Manter=12143, Comprar=15073, Vender=9538\n",
      "Ganhos Totais: 35761.00, Perdas Totais: -32980.75\n",
      "Episode 100/100, Total Reward: 3073.75, Win Rate: 0.58, Wins: 1075, Losses: 770, Epsilon: 0.1830, Steps: 36754, Time: 141.80s\n",
      "Ações: Manter=12186, Comprar=15931, Vender=8637\n",
      "Ganhos Totais: 36074.50, Perdas Totais: -33000.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 66, Total Reward: 5347.50, Win Rate: 0.56, Wins: 1283, Losses: 1017, Ações: {0: 12587, 1: 15123, 2: 9044}, Steps: 36754, Time: 141.58s\n",
      "Rank 2: Episode 80, Total Reward: 5012.50, Win Rate: 0.55, Wins: 1120, Losses: 912, Ações: {0: 10856, 1: 16995, 2: 8903}, Steps: 36754, Time: 141.60s\n",
      "Rank 3: Episode 93, Total Reward: 4619.50, Win Rate: 0.59, Wins: 1245, Losses: 849, Ações: {0: 13380, 1: 14306, 2: 9068}, Steps: 36754, Time: 142.13s\n",
      "Rank 4: Episode 91, Total Reward: 4536.75, Win Rate: 0.58, Wins: 1168, Losses: 841, Ações: {0: 12927, 1: 14014, 2: 9813}, Steps: 36754, Time: 140.97s\n",
      "Rank 5: Episode 97, Total Reward: 4410.00, Win Rate: 0.60, Wins: 1203, Losses: 800, Ações: {0: 11568, 1: 16255, 2: 8931}, Steps: 36754, Time: 140.61s\n",
      "Rank 6: Episode 85, Total Reward: 4289.75, Win Rate: 0.57, Wins: 1178, Losses: 886, Ações: {0: 12219, 1: 15094, 2: 9441}, Steps: 36754, Time: 141.68s\n",
      "Rank 7: Episode 98, Total Reward: 3967.00, Win Rate: 0.58, Wins: 1148, Losses: 818, Ações: {0: 12007, 1: 15305, 2: 9442}, Steps: 36754, Time: 142.40s\n",
      "Rank 8: Episode 90, Total Reward: 3827.75, Win Rate: 0.57, Wins: 1203, Losses: 895, Ações: {0: 12350, 1: 14489, 2: 9915}, Steps: 36754, Time: 138.21s\n",
      "Rank 9: Episode 94, Total Reward: 3661.00, Win Rate: 0.58, Wins: 1179, Losses: 843, Ações: {0: 12111, 1: 15824, 2: 8819}, Steps: 36754, Time: 140.72s\n",
      "Rank 10: Episode 86, Total Reward: 3105.75, Win Rate: 0.57, Wins: 1080, Losses: 818, Ações: {0: 13786, 1: 13912, 2: 9056}, Steps: 36754, Time: 140.07s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.8.4\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
