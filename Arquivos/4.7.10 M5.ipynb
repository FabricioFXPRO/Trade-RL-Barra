{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -276.75, Win Rate: 0.51, Wins: 4062, Losses: 3926, Epsilon: 0.4950, Steps: 110609, Time: 350.74s\n",
      "Ações: Manter=35618, Comprar=37800, Vender=37191\n",
      "Ganhos Totais: 62111.75, Perdas Totais: -62388.50\n",
      "Modelo e log do episódio 1 salvos em: 4.7.10\\model_episode_1.pth e 4.7.10\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -2665.75, Win Rate: 0.50, Wins: 3778, Losses: 3757, Epsilon: 0.4900, Steps: 110609, Time: 339.18s\n",
      "Ações: Manter=34322, Comprar=38361, Vender=37926\n",
      "Ganhos Totais: 59588.50, Perdas Totais: -62254.25\n",
      "Modelo e log do episódio 2 salvos em: 4.7.10\\model_episode_2.pth e 4.7.10\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1468.75, Win Rate: 0.52, Wins: 3846, Losses: 3581, Epsilon: 0.4851, Steps: 110609, Time: 341.62s\n",
      "Ações: Manter=35729, Comprar=36861, Vender=38019\n",
      "Ganhos Totais: 61497.50, Perdas Totais: -62966.25\n",
      "Modelo e log do episódio 3 salvos em: 4.7.10\\model_episode_3.pth e 4.7.10\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: 1297.50, Win Rate: 0.52, Wins: 3792, Losses: 3500, Epsilon: 0.4803, Steps: 110609, Time: 346.03s\n",
      "Ações: Manter=35829, Comprar=36976, Vender=37804\n",
      "Ganhos Totais: 61404.25, Perdas Totais: -60106.75\n",
      "Modelo e log do episódio 4 salvos em: 4.7.10\\model_episode_4.pth e 4.7.10\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -4128.75, Win Rate: 0.51, Wins: 3696, Losses: 3518, Epsilon: 0.4755, Steps: 110609, Time: 377.60s\n",
      "Ações: Manter=36008, Comprar=36179, Vender=38422\n",
      "Ganhos Totais: 58276.50, Perdas Totais: -62405.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.10\\model_episode_5.pth e 4.7.10\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -1669.25, Win Rate: 0.51, Wins: 3598, Losses: 3441, Epsilon: 0.4707, Steps: 110609, Time: 376.90s\n",
      "Ações: Manter=36751, Comprar=36214, Vender=37644\n",
      "Ganhos Totais: 58008.50, Perdas Totais: -59677.75\n",
      "Modelo e log do episódio 6 salvos em: 4.7.10\\model_episode_6.pth e 4.7.10\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -669.25, Win Rate: 0.52, Wins: 3814, Losses: 3493, Epsilon: 0.4660, Steps: 110609, Time: 375.40s\n",
      "Ações: Manter=35086, Comprar=36834, Vender=38689\n",
      "Ganhos Totais: 61087.50, Perdas Totais: -61756.75\n",
      "Modelo e log do episódio 7 salvos em: 4.7.10\\model_episode_7.pth e 4.7.10\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 3704.75, Win Rate: 0.53, Wins: 3847, Losses: 3390, Epsilon: 0.4614, Steps: 110609, Time: 375.38s\n",
      "Ações: Manter=35695, Comprar=36621, Vender=38293\n",
      "Ganhos Totais: 62597.75, Perdas Totais: -58893.00\n",
      "Modelo e log do episódio 8 salvos em: 4.7.10\\model_episode_8.pth e 4.7.10\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -5428.75, Win Rate: 0.50, Wins: 3618, Losses: 3570, Epsilon: 0.4568, Steps: 110609, Time: 378.64s\n",
      "Ações: Manter=36242, Comprar=36526, Vender=37841\n",
      "Ganhos Totais: 58259.25, Perdas Totais: -63688.00\n",
      "Modelo e log do episódio 9 salvos em: 4.7.10\\model_episode_9.pth e 4.7.10\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -4828.25, Win Rate: 0.52, Wins: 3862, Losses: 3526, Epsilon: 0.4522, Steps: 110609, Time: 375.55s\n",
      "Ações: Manter=35032, Comprar=35851, Vender=39726\n",
      "Ganhos Totais: 58964.75, Perdas Totais: -63793.00\n",
      "Modelo e log do episódio 10 salvos em: 4.7.10\\model_episode_10.pth e 4.7.10\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -2412.75, Win Rate: 0.52, Wins: 3716, Losses: 3439, Epsilon: 0.4477, Steps: 110609, Time: 376.59s\n",
      "Ações: Manter=36271, Comprar=35959, Vender=38379\n",
      "Ganhos Totais: 58661.50, Perdas Totais: -61074.25\n",
      "Modelo e log do episódio 11 salvos em: 4.7.10\\model_episode_11.pth e 4.7.10\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -5294.25, Win Rate: 0.52, Wins: 3674, Losses: 3420, Epsilon: 0.4432, Steps: 110609, Time: 376.17s\n",
      "Ações: Manter=36834, Comprar=37083, Vender=36692\n",
      "Ganhos Totais: 56553.00, Perdas Totais: -61847.25\n",
      "Episode 13/100, Total Reward: -4229.75, Win Rate: 0.52, Wins: 3634, Losses: 3419, Epsilon: 0.4388, Steps: 110609, Time: 376.90s\n",
      "Ações: Manter=37149, Comprar=36937, Vender=36523\n",
      "Ganhos Totais: 56640.75, Perdas Totais: -60870.50\n",
      "Modelo e log do episódio 13 salvos em: 4.7.10\\model_episode_13.pth e 4.7.10\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -3773.75, Win Rate: 0.53, Wins: 3767, Losses: 3349, Epsilon: 0.4344, Steps: 110609, Time: 376.61s\n",
      "Ações: Manter=36666, Comprar=37836, Vender=36107\n",
      "Ganhos Totais: 57914.50, Perdas Totais: -61688.25\n",
      "Modelo e log do episódio 14 salvos em: 4.7.10\\model_episode_14.pth e 4.7.10\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -3000.75, Win Rate: 0.53, Wins: 3800, Losses: 3379, Epsilon: 0.4300, Steps: 110609, Time: 376.56s\n",
      "Ações: Manter=35688, Comprar=37205, Vender=37716\n",
      "Ganhos Totais: 57729.75, Perdas Totais: -60730.50\n",
      "Modelo e log do episódio 15 salvos em: 4.7.10\\model_episode_15.pth e 4.7.10\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -435.50, Win Rate: 0.53, Wins: 3793, Losses: 3385, Epsilon: 0.4257, Steps: 110609, Time: 376.92s\n",
      "Ações: Manter=35527, Comprar=37903, Vender=37179\n",
      "Ganhos Totais: 59197.50, Perdas Totais: -59633.00\n",
      "Modelo e log do episódio 16 salvos em: 4.7.10\\model_episode_16.pth e 4.7.10\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -2844.50, Win Rate: 0.53, Wins: 3723, Losses: 3352, Epsilon: 0.4215, Steps: 110609, Time: 379.17s\n",
      "Ações: Manter=35976, Comprar=38488, Vender=36145\n",
      "Ganhos Totais: 58007.00, Perdas Totais: -60851.50\n",
      "Modelo e log do episódio 17 salvos em: 4.7.10\\model_episode_17.pth e 4.7.10\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -3241.50, Win Rate: 0.53, Wins: 3669, Losses: 3292, Epsilon: 0.4173, Steps: 110609, Time: 379.13s\n",
      "Ações: Manter=37977, Comprar=36239, Vender=36393\n",
      "Ganhos Totais: 56760.50, Perdas Totais: -60002.00\n",
      "Episode 19/100, Total Reward: -4241.75, Win Rate: 0.52, Wins: 3470, Losses: 3195, Epsilon: 0.4131, Steps: 110609, Time: 377.77s\n",
      "Ações: Manter=35490, Comprar=37582, Vender=37537\n",
      "Ganhos Totais: 56510.25, Perdas Totais: -60752.00\n",
      "Episode 20/100, Total Reward: -805.75, Win Rate: 0.54, Wins: 3733, Losses: 3150, Epsilon: 0.4090, Steps: 110609, Time: 377.85s\n",
      "Ações: Manter=37716, Comprar=37137, Vender=35756\n",
      "Ganhos Totais: 58135.25, Perdas Totais: -58941.00\n",
      "Modelo e log do episódio 20 salvos em: 4.7.10\\model_episode_20.pth e 4.7.10\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -1231.50, Win Rate: 0.54, Wins: 3944, Losses: 3421, Epsilon: 0.4049, Steps: 110609, Time: 377.89s\n",
      "Ações: Manter=35227, Comprar=38831, Vender=36551\n",
      "Ganhos Totais: 58801.25, Perdas Totais: -60032.75\n",
      "Modelo e log do episódio 21 salvos em: 4.7.10\\model_episode_21.pth e 4.7.10\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: 1328.50, Win Rate: 0.54, Wins: 4100, Losses: 3454, Epsilon: 0.4008, Steps: 110609, Time: 377.42s\n",
      "Ações: Manter=33923, Comprar=38857, Vender=37829\n",
      "Ganhos Totais: 61435.50, Perdas Totais: -60107.00\n",
      "Modelo e log do episódio 22 salvos em: 4.7.10\\model_episode_22.pth e 4.7.10\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: 457.50, Win Rate: 0.53, Wins: 3621, Losses: 3175, Epsilon: 0.3968, Steps: 110609, Time: 377.20s\n",
      "Ações: Manter=35832, Comprar=37221, Vender=37556\n",
      "Ganhos Totais: 59297.75, Perdas Totais: -58840.25\n",
      "Modelo e log do episódio 23 salvos em: 4.7.10\\model_episode_23.pth e 4.7.10\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: 270.00, Win Rate: 0.54, Wins: 3590, Losses: 3093, Epsilon: 0.3928, Steps: 110609, Time: 377.94s\n",
      "Ações: Manter=37661, Comprar=37423, Vender=35525\n",
      "Ganhos Totais: 58601.00, Perdas Totais: -58331.00\n",
      "Modelo e log do episódio 24 salvos em: 4.7.10\\model_episode_24.pth e 4.7.10\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 29.50, Win Rate: 0.54, Wins: 3614, Losses: 3136, Epsilon: 0.3889, Steps: 110609, Time: 378.75s\n",
      "Ações: Manter=36898, Comprar=37228, Vender=36483\n",
      "Ganhos Totais: 58268.50, Perdas Totais: -58239.00\n",
      "Modelo e log do episódio 25 salvos em: 4.7.10\\model_episode_25.pth e 4.7.10\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: 678.75, Win Rate: 0.55, Wins: 3835, Losses: 3185, Epsilon: 0.3850, Steps: 110609, Time: 378.20s\n",
      "Ações: Manter=34736, Comprar=38291, Vender=37582\n",
      "Ganhos Totais: 59444.00, Perdas Totais: -58765.25\n",
      "Modelo e log do episódio 26 salvos em: 4.7.10\\model_episode_26.pth e 4.7.10\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -3224.00, Win Rate: 0.53, Wins: 3912, Losses: 3408, Epsilon: 0.3812, Steps: 110609, Time: 378.15s\n",
      "Ações: Manter=35247, Comprar=38105, Vender=37257\n",
      "Ganhos Totais: 57027.75, Perdas Totais: -60251.75\n",
      "Episode 28/100, Total Reward: -740.00, Win Rate: 0.55, Wins: 4082, Losses: 3367, Epsilon: 0.3774, Steps: 110609, Time: 382.41s\n",
      "Ações: Manter=35657, Comprar=37981, Vender=36971\n",
      "Ganhos Totais: 60482.00, Perdas Totais: -61222.00\n",
      "Episode 29/100, Total Reward: -3023.00, Win Rate: 0.55, Wins: 4188, Losses: 3493, Epsilon: 0.3736, Steps: 110609, Time: 377.74s\n",
      "Ações: Manter=37553, Comprar=37319, Vender=35737\n",
      "Ganhos Totais: 60311.25, Perdas Totais: -63334.25\n",
      "Episode 30/100, Total Reward: 1497.50, Win Rate: 0.56, Wins: 4292, Losses: 3411, Epsilon: 0.3699, Steps: 110609, Time: 380.10s\n",
      "Ações: Manter=37538, Comprar=36374, Vender=36697\n",
      "Ganhos Totais: 61303.50, Perdas Totais: -59806.00\n",
      "Modelo e log do episódio 30 salvos em: 4.7.10\\model_episode_30.pth e 4.7.10\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -872.75, Win Rate: 0.55, Wins: 4215, Losses: 3434, Epsilon: 0.3662, Steps: 110609, Time: 378.92s\n",
      "Ações: Manter=36756, Comprar=36823, Vender=37030\n",
      "Ganhos Totais: 60582.00, Perdas Totais: -61454.75\n",
      "Episode 32/100, Total Reward: -1793.25, Win Rate: 0.55, Wins: 4243, Losses: 3406, Epsilon: 0.3625, Steps: 110609, Time: 379.96s\n",
      "Ações: Manter=35250, Comprar=38118, Vender=37241\n",
      "Ganhos Totais: 59871.00, Perdas Totais: -61664.25\n",
      "Episode 33/100, Total Reward: -2473.00, Win Rate: 0.54, Wins: 4074, Losses: 3457, Epsilon: 0.3589, Steps: 110609, Time: 380.49s\n",
      "Ações: Manter=38079, Comprar=36600, Vender=35930\n",
      "Ganhos Totais: 59176.25, Perdas Totais: -61649.25\n",
      "Episode 34/100, Total Reward: 623.25, Win Rate: 0.56, Wins: 4261, Losses: 3391, Epsilon: 0.3553, Steps: 110609, Time: 380.00s\n",
      "Ações: Manter=35762, Comprar=39405, Vender=35442\n",
      "Ganhos Totais: 61622.50, Perdas Totais: -60999.25\n",
      "Modelo e log do episódio 34 salvos em: 4.7.10\\model_episode_34.pth e 4.7.10\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: -74.00, Win Rate: 0.56, Wins: 4410, Losses: 3511, Epsilon: 0.3517, Steps: 110609, Time: 381.59s\n",
      "Ações: Manter=35992, Comprar=38205, Vender=36412\n",
      "Ganhos Totais: 61377.50, Perdas Totais: -61451.50\n",
      "Modelo e log do episódio 35 salvos em: 4.7.10\\model_episode_35.pth e 4.7.10\\log_episode_35.csv\n",
      "\n",
      "Episode 36/100, Total Reward: -198.75, Win Rate: 0.56, Wins: 4390, Losses: 3505, Epsilon: 0.3482, Steps: 110609, Time: 381.48s\n",
      "Ações: Manter=36149, Comprar=36740, Vender=37720\n",
      "Ganhos Totais: 61499.00, Perdas Totais: -61697.75\n",
      "Episode 37/100, Total Reward: -384.50, Win Rate: 0.56, Wins: 4456, Losses: 3520, Epsilon: 0.3447, Steps: 110609, Time: 381.18s\n",
      "Ações: Manter=36984, Comprar=37060, Vender=36565\n",
      "Ganhos Totais: 60692.50, Perdas Totais: -61077.00\n",
      "Episode 38/100, Total Reward: -1147.25, Win Rate: 0.55, Wins: 4325, Losses: 3567, Epsilon: 0.3413, Steps: 110609, Time: 381.13s\n",
      "Ações: Manter=36152, Comprar=37388, Vender=37069\n",
      "Ganhos Totais: 60429.25, Perdas Totais: -61576.50\n",
      "Episode 39/100, Total Reward: -165.50, Win Rate: 0.56, Wins: 4464, Losses: 3448, Epsilon: 0.3379, Steps: 110609, Time: 380.55s\n",
      "Ações: Manter=35369, Comprar=39084, Vender=36156\n",
      "Ganhos Totais: 61805.75, Perdas Totais: -61971.25\n",
      "Episode 40/100, Total Reward: 333.00, Win Rate: 0.56, Wins: 4302, Losses: 3366, Epsilon: 0.3345, Steps: 110609, Time: 382.43s\n",
      "Ações: Manter=37955, Comprar=36886, Vender=35768\n",
      "Ganhos Totais: 60418.50, Perdas Totais: -60085.50\n",
      "Modelo e log do episódio 40 salvos em: 4.7.10\\model_episode_40.pth e 4.7.10\\log_episode_40.csv\n",
      "\n",
      "Episode 41/100, Total Reward: -2615.50, Win Rate: 0.56, Wins: 4216, Losses: 3301, Epsilon: 0.3311, Steps: 110609, Time: 381.81s\n",
      "Ações: Manter=37674, Comprar=36656, Vender=36279\n",
      "Ganhos Totais: 59234.50, Perdas Totais: -61850.00\n",
      "Episode 42/100, Total Reward: -1236.25, Win Rate: 0.56, Wins: 4335, Losses: 3423, Epsilon: 0.3278, Steps: 110609, Time: 381.85s\n",
      "Ações: Manter=39091, Comprar=36965, Vender=34553\n",
      "Ganhos Totais: 59989.75, Perdas Totais: -61226.00\n",
      "Episode 43/100, Total Reward: 154.25, Win Rate: 0.56, Wins: 4392, Losses: 3451, Epsilon: 0.3246, Steps: 110609, Time: 382.05s\n",
      "Ações: Manter=36692, Comprar=36748, Vender=37169\n",
      "Ganhos Totais: 60170.25, Perdas Totais: -60016.00\n",
      "Modelo e log do episódio 43 salvos em: 4.7.10\\model_episode_43.pth e 4.7.10\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -284.50, Win Rate: 0.57, Wins: 4330, Losses: 3290, Epsilon: 0.3213, Steps: 110609, Time: 382.15s\n",
      "Ações: Manter=36876, Comprar=33712, Vender=40021\n",
      "Ganhos Totais: 60002.25, Perdas Totais: -60286.75\n",
      "Episode 45/100, Total Reward: -1536.50, Win Rate: 0.56, Wins: 4178, Losses: 3303, Epsilon: 0.3181, Steps: 110609, Time: 382.81s\n",
      "Ações: Manter=36930, Comprar=34362, Vender=39317\n",
      "Ganhos Totais: 58883.50, Perdas Totais: -60420.00\n",
      "Episode 46/100, Total Reward: -1240.50, Win Rate: 0.56, Wins: 4136, Losses: 3259, Epsilon: 0.3149, Steps: 110609, Time: 383.34s\n",
      "Ações: Manter=39794, Comprar=35926, Vender=34889\n",
      "Ganhos Totais: 58669.00, Perdas Totais: -59909.50\n",
      "Episode 47/100, Total Reward: -3609.25, Win Rate: 0.55, Wins: 4179, Losses: 3374, Epsilon: 0.3118, Steps: 110609, Time: 384.14s\n",
      "Ações: Manter=40591, Comprar=34400, Vender=35618\n",
      "Ganhos Totais: 57273.00, Perdas Totais: -60882.25\n",
      "Episode 48/100, Total Reward: 165.25, Win Rate: 0.56, Wins: 4357, Losses: 3382, Epsilon: 0.3086, Steps: 110609, Time: 383.42s\n",
      "Ações: Manter=38830, Comprar=35766, Vender=36013\n",
      "Ganhos Totais: 59893.25, Perdas Totais: -59728.00\n",
      "Modelo e log do episódio 48 salvos em: 4.7.10\\model_episode_48.pth e 4.7.10\\log_episode_48.csv\n",
      "\n",
      "Episode 49/100, Total Reward: -2523.25, Win Rate: 0.56, Wins: 4233, Losses: 3325, Epsilon: 0.3056, Steps: 110609, Time: 383.47s\n",
      "Ações: Manter=40689, Comprar=37109, Vender=32811\n",
      "Ganhos Totais: 57023.00, Perdas Totais: -59546.25\n",
      "Episode 50/100, Total Reward: -2511.25, Win Rate: 0.56, Wins: 4105, Losses: 3237, Epsilon: 0.3025, Steps: 110609, Time: 382.51s\n",
      "Ações: Manter=41627, Comprar=33454, Vender=35528\n",
      "Ganhos Totais: 57475.75, Perdas Totais: -59987.00\n",
      "Episode 51/100, Total Reward: -2882.75, Win Rate: 0.55, Wins: 4201, Losses: 3438, Epsilon: 0.2995, Steps: 110609, Time: 382.41s\n",
      "Ações: Manter=40881, Comprar=33485, Vender=36243\n",
      "Ganhos Totais: 57746.75, Perdas Totais: -60629.50\n",
      "Episode 52/100, Total Reward: -520.00, Win Rate: 0.56, Wins: 4279, Losses: 3350, Epsilon: 0.2965, Steps: 110609, Time: 382.55s\n",
      "Ações: Manter=40140, Comprar=36609, Vender=33860\n",
      "Ganhos Totais: 59348.25, Perdas Totais: -59868.25\n",
      "Episode 53/100, Total Reward: -1123.00, Win Rate: 0.56, Wins: 4448, Losses: 3497, Epsilon: 0.2935, Steps: 110609, Time: 383.60s\n",
      "Ações: Manter=38712, Comprar=37324, Vender=34573\n",
      "Ganhos Totais: 59370.50, Perdas Totais: -60493.50\n",
      "Episode 54/100, Total Reward: -4064.00, Win Rate: 0.56, Wins: 4323, Losses: 3438, Epsilon: 0.2906, Steps: 110609, Time: 384.13s\n",
      "Ações: Manter=39634, Comprar=36474, Vender=34501\n",
      "Ganhos Totais: 58094.00, Perdas Totais: -62158.00\n",
      "Episode 55/100, Total Reward: -2703.25, Win Rate: 0.56, Wins: 4323, Losses: 3358, Epsilon: 0.2877, Steps: 110609, Time: 384.27s\n",
      "Ações: Manter=40912, Comprar=36836, Vender=32861\n",
      "Ganhos Totais: 57370.50, Perdas Totais: -60073.75\n",
      "Episode 56/100, Total Reward: -2038.75, Win Rate: 0.55, Wins: 4206, Losses: 3435, Epsilon: 0.2848, Steps: 110609, Time: 385.87s\n",
      "Ações: Manter=40250, Comprar=34650, Vender=35709\n",
      "Ganhos Totais: 57814.25, Perdas Totais: -59853.00\n",
      "Episode 57/100, Total Reward: 382.00, Win Rate: 0.57, Wins: 4464, Losses: 3407, Epsilon: 0.2820, Steps: 110609, Time: 386.49s\n",
      "Ações: Manter=37804, Comprar=36650, Vender=36155\n",
      "Ganhos Totais: 59941.00, Perdas Totais: -59559.00\n",
      "Modelo e log do episódio 57 salvos em: 4.7.10\\model_episode_57.pth e 4.7.10\\log_episode_57.csv\n",
      "\n",
      "Episode 58/100, Total Reward: -1224.75, Win Rate: 0.56, Wins: 4279, Losses: 3318, Epsilon: 0.2791, Steps: 110609, Time: 386.01s\n",
      "Ações: Manter=40845, Comprar=35507, Vender=34257\n",
      "Ganhos Totais: 57291.50, Perdas Totais: -58516.25\n",
      "Episode 59/100, Total Reward: -2891.50, Win Rate: 0.56, Wins: 4286, Losses: 3318, Epsilon: 0.2763, Steps: 110609, Time: 385.36s\n",
      "Ações: Manter=40815, Comprar=35825, Vender=33969\n",
      "Ganhos Totais: 56677.25, Perdas Totais: -59568.75\n",
      "Episode 60/100, Total Reward: -4530.75, Win Rate: 0.56, Wins: 4119, Losses: 3228, Epsilon: 0.2736, Steps: 110609, Time: 385.74s\n",
      "Ações: Manter=40156, Comprar=38538, Vender=31915\n",
      "Ganhos Totais: 55727.75, Perdas Totais: -60258.50\n",
      "Episode 61/100, Total Reward: -3553.50, Win Rate: 0.56, Wins: 4209, Losses: 3266, Epsilon: 0.2708, Steps: 110609, Time: 386.08s\n",
      "Ações: Manter=41806, Comprar=37016, Vender=31787\n",
      "Ganhos Totais: 56163.50, Perdas Totais: -59717.00\n",
      "Episode 62/100, Total Reward: 1176.75, Win Rate: 0.57, Wins: 4544, Losses: 3377, Epsilon: 0.2681, Steps: 110609, Time: 388.19s\n",
      "Ações: Manter=36511, Comprar=40977, Vender=33121\n",
      "Ganhos Totais: 61034.75, Perdas Totais: -59858.00\n",
      "Modelo e log do episódio 62 salvos em: 4.7.10\\model_episode_62.pth e 4.7.10\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: 1558.50, Win Rate: 0.57, Wins: 4439, Losses: 3363, Epsilon: 0.2655, Steps: 110609, Time: 388.05s\n",
      "Ações: Manter=36991, Comprar=42090, Vender=31528\n",
      "Ganhos Totais: 60797.50, Perdas Totais: -59239.00\n",
      "Modelo e log do episódio 63 salvos em: 4.7.10\\model_episode_63.pth e 4.7.10\\log_episode_63.csv\n",
      "\n",
      "Episode 64/100, Total Reward: -2294.00, Win Rate: 0.57, Wins: 4446, Losses: 3412, Epsilon: 0.2628, Steps: 110609, Time: 386.93s\n",
      "Ações: Manter=37189, Comprar=43594, Vender=29826\n",
      "Ganhos Totais: 57841.50, Perdas Totais: -60135.50\n",
      "Episode 65/100, Total Reward: -114.00, Win Rate: 0.56, Wins: 4374, Losses: 3376, Epsilon: 0.2602, Steps: 110609, Time: 386.56s\n",
      "Ações: Manter=37884, Comprar=42647, Vender=30078\n",
      "Ganhos Totais: 58589.00, Perdas Totais: -58703.00\n",
      "Episode 66/100, Total Reward: 500.75, Win Rate: 0.56, Wins: 4173, Losses: 3257, Epsilon: 0.2576, Steps: 110609, Time: 387.24s\n",
      "Ações: Manter=40200, Comprar=41050, Vender=29359\n",
      "Ganhos Totais: 58347.25, Perdas Totais: -57846.50\n",
      "Modelo e log do episódio 66 salvos em: 4.7.10\\model_episode_66.pth e 4.7.10\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: -2153.50, Win Rate: 0.57, Wins: 4229, Losses: 3238, Epsilon: 0.2550, Steps: 110609, Time: 387.05s\n",
      "Ações: Manter=37705, Comprar=43857, Vender=29047\n",
      "Ganhos Totais: 56684.00, Perdas Totais: -58837.50\n",
      "Episode 68/100, Total Reward: -367.75, Win Rate: 0.57, Wins: 4485, Losses: 3317, Epsilon: 0.2524, Steps: 110609, Time: 386.86s\n",
      "Ações: Manter=36579, Comprar=41927, Vender=32103\n",
      "Ganhos Totais: 58436.50, Perdas Totais: -58804.25\n",
      "Episode 69/100, Total Reward: -755.00, Win Rate: 0.58, Wins: 4482, Losses: 3268, Epsilon: 0.2499, Steps: 110609, Time: 388.75s\n",
      "Ações: Manter=38998, Comprar=41243, Vender=30368\n",
      "Ganhos Totais: 58584.50, Perdas Totais: -59339.50\n",
      "Episode 70/100, Total Reward: 1430.25, Win Rate: 0.58, Wins: 4700, Losses: 3389, Epsilon: 0.2474, Steps: 110609, Time: 387.96s\n",
      "Ações: Manter=36334, Comprar=43947, Vender=30328\n",
      "Ganhos Totais: 60474.50, Perdas Totais: -59044.25\n",
      "Modelo e log do episódio 70 salvos em: 4.7.10\\model_episode_70.pth e 4.7.10\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: -1149.00, Win Rate: 0.57, Wins: 4411, Losses: 3294, Epsilon: 0.2449, Steps: 110609, Time: 388.45s\n",
      "Ações: Manter=36622, Comprar=44008, Vender=29979\n",
      "Ganhos Totais: 59000.50, Perdas Totais: -60149.50\n",
      "Episode 72/100, Total Reward: -1882.00, Win Rate: 0.58, Wins: 4290, Losses: 3139, Epsilon: 0.2425, Steps: 110609, Time: 388.94s\n",
      "Ações: Manter=41390, Comprar=36995, Vender=32224\n",
      "Ganhos Totais: 56247.00, Perdas Totais: -58129.00\n",
      "Episode 73/100, Total Reward: -3719.75, Win Rate: 0.57, Wins: 4121, Losses: 3169, Epsilon: 0.2401, Steps: 110609, Time: 388.40s\n",
      "Ações: Manter=42181, Comprar=36820, Vender=31608\n",
      "Ganhos Totais: 53525.50, Perdas Totais: -57245.25\n",
      "Episode 74/100, Total Reward: -1315.25, Win Rate: 0.57, Wins: 4334, Losses: 3327, Epsilon: 0.2377, Steps: 110609, Time: 388.09s\n",
      "Ações: Manter=39426, Comprar=39282, Vender=31901\n",
      "Ganhos Totais: 57298.50, Perdas Totais: -58613.75\n",
      "Episode 75/100, Total Reward: 1343.25, Win Rate: 0.57, Wins: 4247, Losses: 3243, Epsilon: 0.2353, Steps: 110609, Time: 388.41s\n",
      "Ações: Manter=41911, Comprar=37058, Vender=31640\n",
      "Ganhos Totais: 57696.25, Perdas Totais: -56353.00\n",
      "Modelo e log do episódio 75 salvos em: 4.7.10\\model_episode_75.pth e 4.7.10\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: 595.50, Win Rate: 0.57, Wins: 4314, Losses: 3290, Epsilon: 0.2329, Steps: 110609, Time: 386.78s\n",
      "Ações: Manter=40410, Comprar=39749, Vender=30450\n",
      "Ganhos Totais: 57622.25, Perdas Totais: -57026.75\n",
      "Episode 77/100, Total Reward: -1527.25, Win Rate: 0.57, Wins: 4315, Losses: 3246, Epsilon: 0.2306, Steps: 110609, Time: 386.93s\n",
      "Ações: Manter=41570, Comprar=37680, Vender=31359\n",
      "Ganhos Totais: 56605.50, Perdas Totais: -58132.75\n",
      "Episode 78/100, Total Reward: -477.25, Win Rate: 0.57, Wins: 4158, Losses: 3130, Epsilon: 0.2283, Steps: 110609, Time: 386.11s\n",
      "Ações: Manter=43845, Comprar=37014, Vender=29750\n",
      "Ganhos Totais: 54468.50, Perdas Totais: -54945.75\n",
      "Episode 79/100, Total Reward: 57.00, Win Rate: 0.57, Wins: 4258, Losses: 3217, Epsilon: 0.2260, Steps: 110609, Time: 386.73s\n",
      "Ações: Manter=40502, Comprar=35657, Vender=34450\n",
      "Ganhos Totais: 58490.75, Perdas Totais: -58433.75\n",
      "Episode 80/100, Total Reward: -1067.75, Win Rate: 0.57, Wins: 4261, Losses: 3165, Epsilon: 0.2238, Steps: 110609, Time: 387.99s\n",
      "Ações: Manter=41731, Comprar=37197, Vender=31681\n",
      "Ganhos Totais: 57659.50, Perdas Totais: -58727.25\n",
      "Episode 81/100, Total Reward: -3001.00, Win Rate: 0.56, Wins: 3942, Losses: 3073, Epsilon: 0.2215, Steps: 110609, Time: 386.82s\n",
      "Ações: Manter=45143, Comprar=35196, Vender=30270\n",
      "Ganhos Totais: 53772.75, Perdas Totais: -56773.75\n",
      "Episode 82/100, Total Reward: 2673.75, Win Rate: 0.58, Wins: 4397, Losses: 3210, Epsilon: 0.2193, Steps: 110609, Time: 387.45s\n",
      "Ações: Manter=41350, Comprar=37741, Vender=31518\n",
      "Ganhos Totais: 58662.25, Perdas Totais: -55988.50\n",
      "Modelo e log do episódio 82 salvos em: 4.7.10\\model_episode_82.pth e 4.7.10\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: -1374.75, Win Rate: 0.57, Wins: 4194, Losses: 3133, Epsilon: 0.2171, Steps: 110609, Time: 381.81s\n",
      "Ações: Manter=42286, Comprar=38721, Vender=29602\n",
      "Ganhos Totais: 56199.50, Perdas Totais: -57574.25\n",
      "Episode 84/100, Total Reward: -821.25, Win Rate: 0.57, Wins: 4043, Losses: 3021, Epsilon: 0.2149, Steps: 110609, Time: 369.68s\n",
      "Ações: Manter=41623, Comprar=37503, Vender=31483\n",
      "Ganhos Totais: 55317.25, Perdas Totais: -56138.50\n",
      "Episode 85/100, Total Reward: -695.75, Win Rate: 0.58, Wins: 4111, Losses: 3029, Epsilon: 0.2128, Steps: 110609, Time: 368.32s\n",
      "Ações: Manter=42030, Comprar=39318, Vender=29261\n",
      "Ganhos Totais: 55822.25, Perdas Totais: -56518.00\n",
      "Episode 86/100, Total Reward: 3049.50, Win Rate: 0.58, Wins: 4275, Losses: 3044, Epsilon: 0.2107, Steps: 110609, Time: 352.29s\n",
      "Ações: Manter=41477, Comprar=38798, Vender=30334\n",
      "Ganhos Totais: 58120.75, Perdas Totais: -55071.25\n",
      "Modelo e log do episódio 86 salvos em: 4.7.10\\model_episode_86.pth e 4.7.10\\log_episode_86.csv\n",
      "\n",
      "Episode 87/100, Total Reward: -3079.00, Win Rate: 0.57, Wins: 3992, Losses: 2988, Epsilon: 0.2086, Steps: 110609, Time: 340.87s\n",
      "Ações: Manter=42343, Comprar=39794, Vender=28472\n",
      "Ganhos Totais: 54248.25, Perdas Totais: -57327.25\n",
      "Episode 88/100, Total Reward: -1959.25, Win Rate: 0.58, Wins: 4061, Losses: 2944, Epsilon: 0.2065, Steps: 110609, Time: 342.34s\n",
      "Ações: Manter=41992, Comprar=38033, Vender=30584\n",
      "Ganhos Totais: 53927.50, Perdas Totais: -55886.75\n",
      "Episode 89/100, Total Reward: -2115.00, Win Rate: 0.58, Wins: 3902, Losses: 2864, Epsilon: 0.2044, Steps: 110609, Time: 342.17s\n",
      "Ações: Manter=43280, Comprar=34506, Vender=32823\n",
      "Ganhos Totais: 54993.25, Perdas Totais: -57108.25\n",
      "Episode 90/100, Total Reward: -538.75, Win Rate: 0.59, Wins: 4094, Losses: 2866, Epsilon: 0.2024, Steps: 110609, Time: 344.03s\n",
      "Ações: Manter=39277, Comprar=41206, Vender=30126\n",
      "Ganhos Totais: 55436.25, Perdas Totais: -55975.00\n",
      "Episode 91/100, Total Reward: -1210.50, Win Rate: 0.57, Wins: 3808, Losses: 2816, Epsilon: 0.2003, Steps: 110609, Time: 345.68s\n",
      "Ações: Manter=41967, Comprar=39610, Vender=29032\n",
      "Ganhos Totais: 53399.00, Perdas Totais: -54609.50\n",
      "Episode 92/100, Total Reward: -1152.50, Win Rate: 0.58, Wins: 3748, Losses: 2676, Epsilon: 0.1983, Steps: 110609, Time: 356.58s\n",
      "Ações: Manter=47768, Comprar=35304, Vender=27537\n",
      "Ganhos Totais: 51831.75, Perdas Totais: -52984.25\n",
      "Episode 93/100, Total Reward: -2065.75, Win Rate: 0.57, Wins: 3730, Losses: 2775, Epsilon: 0.1964, Steps: 110609, Time: 382.52s\n",
      "Ações: Manter=45485, Comprar=32855, Vender=32269\n",
      "Ganhos Totais: 53706.75, Perdas Totais: -55772.50\n",
      "Episode 94/100, Total Reward: 1861.00, Win Rate: 0.58, Wins: 3698, Losses: 2657, Epsilon: 0.1944, Steps: 110609, Time: 400.04s\n",
      "Ações: Manter=46711, Comprar=32198, Vender=31700\n",
      "Ganhos Totais: 55949.75, Perdas Totais: -54088.75\n",
      "Modelo e log do episódio 94 salvos em: 4.7.10\\model_episode_94.pth e 4.7.10\\log_episode_94.csv\n",
      "\n",
      "Episode 95/100, Total Reward: -4.75, Win Rate: 0.58, Wins: 3669, Losses: 2640, Epsilon: 0.1924, Steps: 110609, Time: 386.93s\n",
      "Ações: Manter=44061, Comprar=29831, Vender=36717\n",
      "Ganhos Totais: 54985.00, Perdas Totais: -54989.75\n",
      "Episode 96/100, Total Reward: 244.25, Win Rate: 0.59, Wins: 3769, Losses: 2608, Epsilon: 0.1905, Steps: 110609, Time: 392.72s\n",
      "Ações: Manter=43356, Comprar=31173, Vender=36080\n",
      "Ganhos Totais: 54308.00, Perdas Totais: -54063.75\n",
      "Episode 97/100, Total Reward: -697.25, Win Rate: 0.58, Wins: 3478, Losses: 2546, Epsilon: 0.1886, Steps: 110609, Time: 381.79s\n",
      "Ações: Manter=42985, Comprar=28689, Vender=38935\n",
      "Ganhos Totais: 53744.25, Perdas Totais: -54441.50\n",
      "Episode 98/100, Total Reward: 237.50, Win Rate: 0.58, Wins: 3606, Losses: 2594, Epsilon: 0.1867, Steps: 110609, Time: 376.55s\n",
      "Ações: Manter=46924, Comprar=30547, Vender=33138\n",
      "Ganhos Totais: 54477.00, Perdas Totais: -54239.50\n",
      "Episode 99/100, Total Reward: -1441.75, Win Rate: 0.58, Wins: 3431, Losses: 2446, Epsilon: 0.1849, Steps: 110609, Time: 361.24s\n",
      "Ações: Manter=48642, Comprar=31894, Vender=30073\n",
      "Ganhos Totais: 51967.25, Perdas Totais: -53409.00\n",
      "Episode 100/100, Total Reward: 788.25, Win Rate: 0.59, Wins: 3484, Losses: 2408, Epsilon: 0.1830, Steps: 110609, Time: 360.57s\n",
      "Ações: Manter=49680, Comprar=27175, Vender=33754\n",
      "Ganhos Totais: 54599.25, Perdas Totais: -53811.00\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 8, Total Reward: 3704.75, Win Rate: 0.53, Wins: 3847, Losses: 3390, Ações: {0: 35695, 1: 36621, 2: 38293}, Steps: 110609, Time: 375.38s\n",
      "Rank 2: Episode 86, Total Reward: 3049.50, Win Rate: 0.58, Wins: 4275, Losses: 3044, Ações: {0: 41477, 1: 38798, 2: 30334}, Steps: 110609, Time: 352.29s\n",
      "Rank 3: Episode 82, Total Reward: 2673.75, Win Rate: 0.58, Wins: 4397, Losses: 3210, Ações: {0: 41350, 1: 37741, 2: 31518}, Steps: 110609, Time: 387.45s\n",
      "Rank 4: Episode 94, Total Reward: 1861.00, Win Rate: 0.58, Wins: 3698, Losses: 2657, Ações: {0: 46711, 1: 32198, 2: 31700}, Steps: 110609, Time: 400.04s\n",
      "Rank 5: Episode 63, Total Reward: 1558.50, Win Rate: 0.57, Wins: 4439, Losses: 3363, Ações: {0: 36991, 1: 42090, 2: 31528}, Steps: 110609, Time: 388.05s\n",
      "Rank 6: Episode 30, Total Reward: 1497.50, Win Rate: 0.56, Wins: 4292, Losses: 3411, Ações: {0: 37538, 1: 36374, 2: 36697}, Steps: 110609, Time: 380.10s\n",
      "Rank 7: Episode 70, Total Reward: 1430.25, Win Rate: 0.58, Wins: 4700, Losses: 3389, Ações: {0: 36334, 1: 43947, 2: 30328}, Steps: 110609, Time: 387.96s\n",
      "Rank 8: Episode 75, Total Reward: 1343.25, Win Rate: 0.57, Wins: 4247, Losses: 3243, Ações: {0: 41911, 1: 37058, 2: 31640}, Steps: 110609, Time: 388.41s\n",
      "Rank 9: Episode 22, Total Reward: 1328.50, Win Rate: 0.54, Wins: 4100, Losses: 3454, Ações: {0: 33923, 1: 38857, 2: 37829}, Steps: 110609, Time: 377.42s\n",
      "Rank 10: Episode 4, Total Reward: 1297.50, Win Rate: 0.52, Wins: 3792, Losses: 3500, Ações: {0: 35829, 1: 36976, 2: 37804}, Steps: 110609, Time: 346.03s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M5_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.10\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
