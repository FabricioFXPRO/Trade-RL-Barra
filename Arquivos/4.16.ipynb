{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -5897.75, Win Rate: 0.48, Wins: 1240, Losses: 1354, Epsilon: 0.4950, Steps: 36754, Time: 170.27s\n",
      "Ações: Manter=12250, Comprar=11620, Vender=12884\n",
      "Ganhos Totais: 33589.50, Perdas Totais: -38836.50\n",
      "Modelo e log do episódio 1 salvos em: 4.16\\model_episode_1.pth e 4.16\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -5625.50, Win Rate: 0.49, Wins: 1242, Losses: 1291, Epsilon: 0.4900, Steps: 36754, Time: 169.02s\n",
      "Ações: Manter=13316, Comprar=10671, Vender=12767\n",
      "Ganhos Totais: 33149.25, Perdas Totais: -38139.50\n",
      "Modelo e log do episódio 2 salvos em: 4.16\\model_episode_2.pth e 4.16\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -2422.00, Win Rate: 0.50, Wins: 1333, Losses: 1320, Epsilon: 0.4851, Steps: 36754, Time: 160.64s\n",
      "Ações: Manter=11364, Comprar=12393, Vender=12997\n",
      "Ganhos Totais: 36986.75, Perdas Totais: -38742.50\n",
      "Modelo e log do episódio 3 salvos em: 4.16\\model_episode_3.pth e 4.16\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -479.75, Win Rate: 0.53, Wins: 1424, Losses: 1271, Epsilon: 0.4803, Steps: 36754, Time: 145.81s\n",
      "Ações: Manter=12027, Comprar=11692, Vender=13035\n",
      "Ganhos Totais: 37978.75, Perdas Totais: -37780.75\n",
      "Modelo e log do episódio 4 salvos em: 4.16\\model_episode_4.pth e 4.16\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -2321.25, Win Rate: 0.52, Wins: 1448, Losses: 1333, Epsilon: 0.4755, Steps: 36754, Time: 135.64s\n",
      "Ações: Manter=11529, Comprar=12139, Vender=13086\n",
      "Ganhos Totais: 36868.50, Perdas Totais: -38490.75\n",
      "Modelo e log do episódio 5 salvos em: 4.16\\model_episode_5.pth e 4.16\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -6436.25, Win Rate: 0.49, Wins: 1307, Losses: 1355, Epsilon: 0.4707, Steps: 36754, Time: 134.83s\n",
      "Ações: Manter=12176, Comprar=11450, Vender=13128\n",
      "Ganhos Totais: 34434.50, Perdas Totais: -40202.50\n",
      "Modelo e log do episódio 6 salvos em: 4.16\\model_episode_6.pth e 4.16\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -4831.25, Win Rate: 0.51, Wins: 1374, Losses: 1323, Epsilon: 0.4660, Steps: 36754, Time: 135.13s\n",
      "Ações: Manter=11901, Comprar=12074, Vender=12779\n",
      "Ganhos Totais: 34986.00, Perdas Totais: -39139.25\n",
      "Modelo e log do episódio 7 salvos em: 4.16\\model_episode_7.pth e 4.16\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -1474.50, Win Rate: 0.52, Wins: 1450, Losses: 1359, Epsilon: 0.4614, Steps: 36754, Time: 135.19s\n",
      "Ações: Manter=11520, Comprar=12605, Vender=12629\n",
      "Ganhos Totais: 38634.50, Perdas Totais: -39402.75\n",
      "Modelo e log do episódio 8 salvos em: 4.16\\model_episode_8.pth e 4.16\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -5419.75, Win Rate: 0.52, Wins: 1520, Losses: 1384, Epsilon: 0.4568, Steps: 36754, Time: 162.25s\n",
      "Ações: Manter=11413, Comprar=11656, Vender=13685\n",
      "Ganhos Totais: 35709.50, Perdas Totais: -40400.25\n",
      "Modelo e log do episódio 9 salvos em: 4.16\\model_episode_9.pth e 4.16\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -2162.75, Win Rate: 0.52, Wins: 1486, Losses: 1365, Epsilon: 0.4522, Steps: 36754, Time: 166.89s\n",
      "Ações: Manter=11667, Comprar=11632, Vender=13455\n",
      "Ganhos Totais: 36527.75, Perdas Totais: -37973.50\n",
      "Modelo e log do episódio 10 salvos em: 4.16\\model_episode_10.pth e 4.16\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -5072.00, Win Rate: 0.51, Wins: 1384, Losses: 1329, Epsilon: 0.4477, Steps: 36754, Time: 152.28s\n",
      "Ações: Manter=11984, Comprar=10960, Vender=13810\n",
      "Ganhos Totais: 35861.50, Perdas Totais: -40253.00\n",
      "Modelo e log do episódio 11 salvos em: 4.16\\model_episode_11.pth e 4.16\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -3958.00, Win Rate: 0.53, Wins: 1472, Losses: 1317, Epsilon: 0.4432, Steps: 36754, Time: 151.52s\n",
      "Ações: Manter=12875, Comprar=11821, Vender=12058\n",
      "Ganhos Totais: 35836.50, Perdas Totais: -39092.75\n",
      "Modelo e log do episódio 12 salvos em: 4.16\\model_episode_12.pth e 4.16\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: -2561.50, Win Rate: 0.52, Wins: 1529, Losses: 1420, Epsilon: 0.4388, Steps: 36754, Time: 151.18s\n",
      "Ações: Manter=11927, Comprar=11918, Vender=12909\n",
      "Ganhos Totais: 37496.25, Perdas Totais: -39316.50\n",
      "Modelo e log do episódio 13 salvos em: 4.16\\model_episode_13.pth e 4.16\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -3607.00, Win Rate: 0.51, Wins: 1464, Losses: 1400, Epsilon: 0.4344, Steps: 36754, Time: 151.30s\n",
      "Ações: Manter=11995, Comprar=12144, Vender=12615\n",
      "Ganhos Totais: 36736.75, Perdas Totais: -39620.25\n",
      "Modelo e log do episódio 14 salvos em: 4.16\\model_episode_14.pth e 4.16\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -4334.00, Win Rate: 0.50, Wins: 1478, Losses: 1461, Epsilon: 0.4300, Steps: 36754, Time: 151.05s\n",
      "Ações: Manter=13194, Comprar=10713, Vender=12847\n",
      "Ganhos Totais: 36105.25, Perdas Totais: -39700.75\n",
      "Modelo e log do episódio 15 salvos em: 4.16\\model_episode_15.pth e 4.16\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -4216.75, Win Rate: 0.52, Wins: 1525, Losses: 1436, Epsilon: 0.4257, Steps: 36754, Time: 151.47s\n",
      "Ações: Manter=12268, Comprar=10788, Vender=13698\n",
      "Ganhos Totais: 37570.25, Perdas Totais: -41042.75\n",
      "Modelo e log do episódio 16 salvos em: 4.16\\model_episode_16.pth e 4.16\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -2147.00, Win Rate: 0.52, Wins: 1598, Losses: 1454, Epsilon: 0.4215, Steps: 36754, Time: 151.72s\n",
      "Ações: Manter=11911, Comprar=11618, Vender=13225\n",
      "Ganhos Totais: 37937.50, Perdas Totais: -39317.00\n",
      "Modelo e log do episódio 17 salvos em: 4.16\\model_episode_17.pth e 4.16\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: -1488.75, Win Rate: 0.51, Wins: 1538, Losses: 1455, Epsilon: 0.4173, Steps: 36754, Time: 151.82s\n",
      "Ações: Manter=12803, Comprar=11304, Vender=12647\n",
      "Ganhos Totais: 37705.50, Perdas Totais: -38443.25\n",
      "Modelo e log do episódio 18 salvos em: 4.16\\model_episode_18.pth e 4.16\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -4459.50, Win Rate: 0.52, Wins: 1538, Losses: 1439, Epsilon: 0.4131, Steps: 36754, Time: 151.83s\n",
      "Ações: Manter=12686, Comprar=11574, Vender=12494\n",
      "Ganhos Totais: 35731.25, Perdas Totais: -39441.25\n",
      "Episode 20/100, Total Reward: -3242.75, Win Rate: 0.52, Wins: 1533, Losses: 1406, Epsilon: 0.4090, Steps: 36754, Time: 151.62s\n",
      "Ações: Manter=13184, Comprar=10929, Vender=12641\n",
      "Ganhos Totais: 37310.75, Perdas Totais: -39815.25\n",
      "Modelo e log do episódio 20 salvos em: 4.16\\model_episode_20.pth e 4.16\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -1965.50, Win Rate: 0.53, Wins: 1530, Losses: 1331, Epsilon: 0.4049, Steps: 36754, Time: 152.76s\n",
      "Ações: Manter=13616, Comprar=11531, Vender=11607\n",
      "Ganhos Totais: 36907.50, Perdas Totais: -38154.25\n",
      "Modelo e log do episódio 21 salvos em: 4.16\\model_episode_21.pth e 4.16\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -4883.00, Win Rate: 0.52, Wins: 1481, Losses: 1352, Epsilon: 0.4008, Steps: 36754, Time: 152.38s\n",
      "Ações: Manter=12051, Comprar=11907, Vender=12796\n",
      "Ganhos Totais: 35129.75, Perdas Totais: -39299.75\n",
      "Episode 23/100, Total Reward: -3707.25, Win Rate: 0.53, Wins: 1399, Losses: 1226, Epsilon: 0.3968, Steps: 36754, Time: 152.80s\n",
      "Ações: Manter=13345, Comprar=10943, Vender=12466\n",
      "Ganhos Totais: 36488.75, Perdas Totais: -39537.25\n",
      "Episode 24/100, Total Reward: -1885.75, Win Rate: 0.53, Wins: 1458, Losses: 1277, Epsilon: 0.3928, Steps: 36754, Time: 152.55s\n",
      "Ações: Manter=12867, Comprar=11283, Vender=12604\n",
      "Ganhos Totais: 36543.50, Perdas Totais: -37741.50\n",
      "Modelo e log do episódio 24 salvos em: 4.16\\model_episode_24.pth e 4.16\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -3458.25, Win Rate: 0.53, Wins: 1372, Losses: 1199, Epsilon: 0.3889, Steps: 36754, Time: 152.35s\n",
      "Ações: Manter=13502, Comprar=11370, Vender=11882\n",
      "Ganhos Totais: 35595.00, Perdas Totais: -38407.50\n",
      "Episode 26/100, Total Reward: 298.75, Win Rate: 0.53, Wins: 1457, Losses: 1272, Epsilon: 0.3850, Steps: 36754, Time: 152.02s\n",
      "Ações: Manter=12507, Comprar=12202, Vender=12045\n",
      "Ganhos Totais: 38771.25, Perdas Totais: -37786.25\n",
      "Modelo e log do episódio 26 salvos em: 4.16\\model_episode_26.pth e 4.16\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -5081.75, Win Rate: 0.51, Wins: 1364, Losses: 1307, Epsilon: 0.3812, Steps: 36754, Time: 152.70s\n",
      "Ações: Manter=12206, Comprar=10844, Vender=13704\n",
      "Ganhos Totais: 34864.25, Perdas Totais: -39275.75\n",
      "Episode 28/100, Total Reward: -1490.75, Win Rate: 0.54, Wins: 1427, Losses: 1232, Epsilon: 0.3774, Steps: 36754, Time: 152.73s\n",
      "Ações: Manter=12660, Comprar=11198, Vender=12896\n",
      "Ganhos Totais: 37913.50, Perdas Totais: -38735.50\n",
      "Modelo e log do episódio 28 salvos em: 4.16\\model_episode_28.pth e 4.16\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: -4465.00, Win Rate: 0.54, Wins: 1403, Losses: 1205, Epsilon: 0.3736, Steps: 36754, Time: 152.26s\n",
      "Ações: Manter=14555, Comprar=11159, Vender=11040\n",
      "Ganhos Totais: 35112.50, Perdas Totais: -38921.75\n",
      "Episode 30/100, Total Reward: -3586.75, Win Rate: 0.51, Wins: 1340, Losses: 1266, Epsilon: 0.3699, Steps: 36754, Time: 152.53s\n",
      "Ações: Manter=14035, Comprar=11028, Vender=11691\n",
      "Ganhos Totais: 34086.75, Perdas Totais: -37019.00\n",
      "Episode 31/100, Total Reward: 1796.00, Win Rate: 0.54, Wins: 1372, Losses: 1167, Epsilon: 0.3662, Steps: 36754, Time: 152.10s\n",
      "Ações: Manter=12728, Comprar=11737, Vender=12289\n",
      "Ganhos Totais: 38533.25, Perdas Totais: -36101.25\n",
      "Modelo e log do episódio 31 salvos em: 4.16\\model_episode_31.pth e 4.16\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 1075.50, Win Rate: 0.54, Wins: 1369, Losses: 1155, Epsilon: 0.3625, Steps: 36754, Time: 153.46s\n",
      "Ações: Manter=12669, Comprar=13317, Vender=10768\n",
      "Ganhos Totais: 37508.25, Perdas Totais: -35799.00\n",
      "Modelo e log do episódio 32 salvos em: 4.16\\model_episode_32.pth e 4.16\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 1964.50, Win Rate: 0.54, Wins: 1264, Losses: 1070, Epsilon: 0.3589, Steps: 36754, Time: 153.02s\n",
      "Ações: Manter=12664, Comprar=13907, Vender=10183\n",
      "Ganhos Totais: 37041.75, Perdas Totais: -34489.25\n",
      "Modelo e log do episódio 33 salvos em: 4.16\\model_episode_33.pth e 4.16\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: 3490.00, Win Rate: 0.55, Wins: 1363, Losses: 1096, Epsilon: 0.3553, Steps: 36754, Time: 153.11s\n",
      "Ações: Manter=11724, Comprar=13559, Vender=11471\n",
      "Ganhos Totais: 38487.00, Perdas Totais: -34380.25\n",
      "Modelo e log do episódio 34 salvos em: 4.16\\model_episode_34.pth e 4.16\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: 1627.75, Win Rate: 0.54, Wins: 1217, Losses: 1050, Epsilon: 0.3517, Steps: 36754, Time: 153.82s\n",
      "Ações: Manter=13872, Comprar=12753, Vender=10129\n",
      "Ganhos Totais: 36662.50, Perdas Totais: -34466.00\n",
      "Modelo e log do episódio 35 salvos em: 4.16\\model_episode_35.pth e 4.16\\log_episode_35.csv\n",
      "\n",
      "Episode 36/100, Total Reward: 466.75, Win Rate: 0.55, Wins: 1341, Losses: 1085, Epsilon: 0.3482, Steps: 36754, Time: 152.93s\n",
      "Ações: Manter=12216, Comprar=13860, Vender=10678\n",
      "Ganhos Totais: 37303.00, Perdas Totais: -36226.75\n",
      "Modelo e log do episódio 36 salvos em: 4.16\\model_episode_36.pth e 4.16\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: -1284.25, Win Rate: 0.53, Wins: 1219, Losses: 1095, Epsilon: 0.3447, Steps: 36754, Time: 153.56s\n",
      "Ações: Manter=12827, Comprar=13173, Vender=10754\n",
      "Ganhos Totais: 35023.50, Perdas Totais: -35725.25\n",
      "Modelo e log do episódio 37 salvos em: 4.16\\model_episode_37.pth e 4.16\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: -1691.25, Win Rate: 0.52, Wins: 1190, Losses: 1079, Epsilon: 0.3413, Steps: 36754, Time: 153.75s\n",
      "Ações: Manter=13522, Comprar=11813, Vender=11419\n",
      "Ganhos Totais: 34623.00, Perdas Totais: -35742.75\n",
      "Episode 39/100, Total Reward: 471.50, Win Rate: 0.55, Wins: 1271, Losses: 1043, Epsilon: 0.3379, Steps: 36754, Time: 153.41s\n",
      "Ações: Manter=13047, Comprar=13197, Vender=10510\n",
      "Ganhos Totais: 36561.75, Perdas Totais: -35506.75\n",
      "Modelo e log do episódio 39 salvos em: 4.16\\model_episode_39.pth e 4.16\\log_episode_39.csv\n",
      "\n",
      "Episode 40/100, Total Reward: -214.50, Win Rate: 0.54, Wins: 1282, Losses: 1097, Epsilon: 0.3345, Steps: 36754, Time: 154.11s\n",
      "Ações: Manter=13511, Comprar=12303, Vender=10940\n",
      "Ganhos Totais: 36301.00, Perdas Totais: -35918.00\n",
      "Modelo e log do episódio 40 salvos em: 4.16\\model_episode_40.pth e 4.16\\log_episode_40.csv\n",
      "\n",
      "Episode 41/100, Total Reward: -830.00, Win Rate: 0.53, Wins: 1285, Losses: 1133, Epsilon: 0.3311, Steps: 36754, Time: 154.01s\n",
      "Ações: Manter=13145, Comprar=13615, Vender=9994\n",
      "Ganhos Totais: 36866.25, Perdas Totais: -37087.50\n",
      "Episode 42/100, Total Reward: -92.25, Win Rate: 0.56, Wins: 1419, Losses: 1122, Epsilon: 0.3278, Steps: 36754, Time: 153.67s\n",
      "Ações: Manter=12590, Comprar=13040, Vender=11124\n",
      "Ganhos Totais: 36874.75, Perdas Totais: -36328.00\n",
      "Modelo e log do episódio 42 salvos em: 4.16\\model_episode_42.pth e 4.16\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: 191.75, Win Rate: 0.55, Wins: 1411, Losses: 1138, Epsilon: 0.3246, Steps: 36754, Time: 154.20s\n",
      "Ações: Manter=12591, Comprar=13358, Vender=10805\n",
      "Ganhos Totais: 38117.50, Perdas Totais: -37286.50\n",
      "Modelo e log do episódio 43 salvos em: 4.16\\model_episode_43.pth e 4.16\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: -3346.25, Win Rate: 0.53, Wins: 1247, Losses: 1098, Epsilon: 0.3213, Steps: 36754, Time: 153.66s\n",
      "Ações: Manter=13362, Comprar=11835, Vender=11557\n",
      "Ganhos Totais: 35334.25, Perdas Totais: -38092.75\n",
      "Episode 45/100, Total Reward: 1035.75, Win Rate: 0.56, Wins: 1387, Losses: 1072, Epsilon: 0.3181, Steps: 36754, Time: 154.44s\n",
      "Ações: Manter=13446, Comprar=11891, Vender=11417\n",
      "Ganhos Totais: 38219.50, Perdas Totais: -36565.25\n",
      "Modelo e log do episódio 45 salvos em: 4.16\\model_episode_45.pth e 4.16\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: 936.25, Win Rate: 0.55, Wins: 1319, Losses: 1091, Epsilon: 0.3149, Steps: 36754, Time: 154.18s\n",
      "Ações: Manter=12349, Comprar=13296, Vender=11109\n",
      "Ganhos Totais: 36683.75, Perdas Totais: -35142.00\n",
      "Modelo e log do episódio 46 salvos em: 4.16\\model_episode_46.pth e 4.16\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: -208.75, Win Rate: 0.56, Wins: 1384, Losses: 1108, Epsilon: 0.3118, Steps: 36754, Time: 165.42s\n",
      "Ações: Manter=13672, Comprar=13224, Vender=9858\n",
      "Ganhos Totais: 37150.25, Perdas Totais: -36734.50\n",
      "Episode 48/100, Total Reward: 78.75, Win Rate: 0.55, Wins: 1356, Losses: 1110, Epsilon: 0.3086, Steps: 36754, Time: 179.45s\n",
      "Ações: Manter=14510, Comprar=12470, Vender=9774\n",
      "Ganhos Totais: 36664.25, Perdas Totais: -35966.50\n",
      "Episode 49/100, Total Reward: -2490.50, Win Rate: 0.54, Wins: 1423, Losses: 1216, Epsilon: 0.3056, Steps: 36754, Time: 177.54s\n",
      "Ações: Manter=14607, Comprar=11100, Vender=11047\n",
      "Ganhos Totais: 35602.00, Perdas Totais: -37429.00\n",
      "Episode 50/100, Total Reward: -1374.00, Win Rate: 0.54, Wins: 1339, Losses: 1131, Epsilon: 0.3025, Steps: 36754, Time: 176.86s\n",
      "Ações: Manter=13969, Comprar=11501, Vender=11284\n",
      "Ganhos Totais: 36132.50, Perdas Totais: -36885.50\n",
      "Episode 51/100, Total Reward: -861.75, Win Rate: 0.54, Wins: 1349, Losses: 1153, Epsilon: 0.2995, Steps: 36754, Time: 177.16s\n",
      "Ações: Manter=13673, Comprar=10837, Vender=12244\n",
      "Ganhos Totais: 36874.50, Perdas Totais: -37109.00\n",
      "Episode 52/100, Total Reward: -3500.00, Win Rate: 0.51, Wins: 1118, Losses: 1056, Epsilon: 0.2965, Steps: 36754, Time: 177.51s\n",
      "Ações: Manter=13288, Comprar=11932, Vender=11534\n",
      "Ganhos Totais: 33326.25, Perdas Totais: -36280.25\n",
      "Episode 53/100, Total Reward: 146.00, Win Rate: 0.54, Wins: 1234, Losses: 1056, Epsilon: 0.2935, Steps: 36754, Time: 176.69s\n",
      "Ações: Manter=13340, Comprar=12511, Vender=10903\n",
      "Ganhos Totais: 36593.25, Perdas Totais: -35872.25\n",
      "Episode 54/100, Total Reward: -65.50, Win Rate: 0.55, Wins: 1307, Losses: 1066, Epsilon: 0.2906, Steps: 36754, Time: 177.01s\n",
      "Ações: Manter=13825, Comprar=12199, Vender=10730\n",
      "Ganhos Totais: 36925.25, Perdas Totais: -36393.50\n",
      "Episode 55/100, Total Reward: 1776.75, Win Rate: 0.54, Wins: 1255, Losses: 1057, Epsilon: 0.2877, Steps: 36754, Time: 177.33s\n",
      "Ações: Manter=12732, Comprar=13061, Vender=10961\n",
      "Ganhos Totais: 37718.00, Perdas Totais: -35361.75\n",
      "Modelo e log do episódio 55 salvos em: 4.16\\model_episode_55.pth e 4.16\\log_episode_55.csv\n",
      "\n",
      "Episode 56/100, Total Reward: -3087.25, Win Rate: 0.55, Wins: 1273, Losses: 1059, Epsilon: 0.2848, Steps: 36754, Time: 178.09s\n",
      "Ações: Manter=12680, Comprar=13282, Vender=10792\n",
      "Ganhos Totais: 33896.00, Perdas Totais: -36398.25\n",
      "Episode 57/100, Total Reward: -684.75, Win Rate: 0.55, Wins: 1338, Losses: 1112, Epsilon: 0.2820, Steps: 36754, Time: 178.56s\n",
      "Ações: Manter=12766, Comprar=13261, Vender=10727\n",
      "Ganhos Totais: 37000.00, Perdas Totais: -37069.00\n",
      "Episode 58/100, Total Reward: 129.00, Win Rate: 0.54, Wins: 1242, Losses: 1061, Epsilon: 0.2791, Steps: 36754, Time: 177.64s\n",
      "Ações: Manter=14030, Comprar=12356, Vender=10368\n",
      "Ganhos Totais: 35172.25, Perdas Totais: -34464.75\n",
      "Episode 59/100, Total Reward: -2135.50, Win Rate: 0.55, Wins: 1148, Losses: 933, Epsilon: 0.2763, Steps: 36754, Time: 178.64s\n",
      "Ações: Manter=13117, Comprar=14086, Vender=9551\n",
      "Ganhos Totais: 34788.50, Perdas Totais: -36402.75\n",
      "Episode 60/100, Total Reward: 271.75, Win Rate: 0.55, Wins: 1287, Losses: 1071, Epsilon: 0.2736, Steps: 36754, Time: 179.55s\n",
      "Ações: Manter=12368, Comprar=14095, Vender=10291\n",
      "Ganhos Totais: 35291.75, Perdas Totais: -34428.25\n",
      "Episode 61/100, Total Reward: 4128.00, Win Rate: 0.56, Wins: 1384, Losses: 1100, Epsilon: 0.2708, Steps: 36754, Time: 156.89s\n",
      "Ações: Manter=12152, Comprar=14331, Vender=10271\n",
      "Ganhos Totais: 40060.50, Perdas Totais: -35307.25\n",
      "Modelo e log do episódio 61 salvos em: 4.16\\model_episode_61.pth e 4.16\\log_episode_61.csv\n",
      "\n",
      "Episode 62/100, Total Reward: -1232.25, Win Rate: 0.53, Wins: 1151, Losses: 1007, Epsilon: 0.2681, Steps: 36754, Time: 158.38s\n",
      "Ações: Manter=12250, Comprar=12427, Vender=12077\n",
      "Ganhos Totais: 34244.50, Perdas Totais: -34935.25\n",
      "Episode 63/100, Total Reward: -2384.50, Win Rate: 0.54, Wins: 1126, Losses: 973, Epsilon: 0.2655, Steps: 36754, Time: 157.95s\n",
      "Ações: Manter=11714, Comprar=14126, Vender=10914\n",
      "Ganhos Totais: 34546.50, Perdas Totais: -36404.75\n",
      "Episode 64/100, Total Reward: -2525.25, Win Rate: 0.53, Wins: 1156, Losses: 1011, Epsilon: 0.2628, Steps: 36754, Time: 156.70s\n",
      "Ações: Manter=10876, Comprar=13698, Vender=12180\n",
      "Ganhos Totais: 35035.25, Perdas Totais: -37016.50\n",
      "Episode 65/100, Total Reward: -1907.75, Win Rate: 0.55, Wins: 1251, Losses: 1029, Epsilon: 0.2602, Steps: 36754, Time: 159.51s\n",
      "Ações: Manter=11726, Comprar=12954, Vender=12074\n",
      "Ganhos Totais: 36014.50, Perdas Totais: -37350.75\n",
      "Episode 66/100, Total Reward: 3333.00, Win Rate: 0.56, Wins: 1191, Losses: 954, Epsilon: 0.2576, Steps: 36754, Time: 155.77s\n",
      "Ações: Manter=11940, Comprar=13958, Vender=10856\n",
      "Ganhos Totais: 37989.75, Perdas Totais: -34118.75\n",
      "Modelo e log do episódio 66 salvos em: 4.16\\model_episode_66.pth e 4.16\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: 3895.00, Win Rate: 0.55, Wins: 1314, Losses: 1060, Epsilon: 0.2550, Steps: 36754, Time: 171.10s\n",
      "Ações: Manter=11327, Comprar=13538, Vender=11889\n",
      "Ganhos Totais: 39662.75, Perdas Totais: -35171.50\n",
      "Modelo e log do episódio 67 salvos em: 4.16\\model_episode_67.pth e 4.16\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: 110.50, Win Rate: 0.56, Wins: 1228, Losses: 984, Epsilon: 0.2524, Steps: 36754, Time: 147.61s\n",
      "Ações: Manter=11185, Comprar=13307, Vender=12262\n",
      "Ganhos Totais: 35981.25, Perdas Totais: -35315.00\n",
      "Episode 69/100, Total Reward: 1770.00, Win Rate: 0.56, Wins: 1178, Losses: 927, Epsilon: 0.2499, Steps: 36754, Time: 158.74s\n",
      "Ações: Manter=12522, Comprar=12747, Vender=11485\n",
      "Ganhos Totais: 36374.50, Perdas Totais: -34076.50\n",
      "Modelo e log do episódio 69 salvos em: 4.16\\model_episode_69.pth e 4.16\\log_episode_69.csv\n",
      "\n",
      "Episode 70/100, Total Reward: -2809.75, Win Rate: 0.55, Wins: 1115, Losses: 896, Epsilon: 0.2474, Steps: 36754, Time: 149.99s\n",
      "Ações: Manter=13267, Comprar=12167, Vender=11320\n",
      "Ganhos Totais: 33331.75, Perdas Totais: -35637.00\n",
      "Episode 71/100, Total Reward: -11.50, Win Rate: 0.54, Wins: 1039, Losses: 896, Epsilon: 0.2449, Steps: 36754, Time: 150.35s\n",
      "Ações: Manter=13941, Comprar=11738, Vender=11075\n",
      "Ganhos Totais: 34184.75, Perdas Totais: -33709.50\n",
      "Episode 72/100, Total Reward: 363.50, Win Rate: 0.57, Wins: 1125, Losses: 866, Epsilon: 0.2425, Steps: 36754, Time: 157.86s\n",
      "Ações: Manter=12004, Comprar=13918, Vender=10832\n",
      "Ganhos Totais: 35656.25, Perdas Totais: -34793.75\n",
      "Episode 73/100, Total Reward: 1646.00, Win Rate: 0.56, Wins: 1181, Losses: 941, Epsilon: 0.2401, Steps: 36754, Time: 153.89s\n",
      "Ações: Manter=10197, Comprar=15646, Vender=10911\n",
      "Ganhos Totais: 37608.50, Perdas Totais: -35430.25\n",
      "Modelo e log do episódio 73 salvos em: 4.16\\model_episode_73.pth e 4.16\\log_episode_73.csv\n",
      "\n",
      "Episode 74/100, Total Reward: -642.00, Win Rate: 0.54, Wins: 1086, Losses: 937, Epsilon: 0.2377, Steps: 36754, Time: 141.41s\n",
      "Ações: Manter=10324, Comprar=14762, Vender=11668\n",
      "Ganhos Totais: 34875.50, Perdas Totais: -35010.00\n",
      "Episode 75/100, Total Reward: -1544.50, Win Rate: 0.55, Wins: 1065, Losses: 889, Epsilon: 0.2353, Steps: 36754, Time: 141.22s\n",
      "Ações: Manter=11588, Comprar=14641, Vender=10525\n",
      "Ganhos Totais: 33543.50, Perdas Totais: -34597.00\n",
      "Episode 76/100, Total Reward: 786.75, Win Rate: 0.55, Wins: 1098, Losses: 912, Epsilon: 0.2329, Steps: 36754, Time: 148.16s\n",
      "Ações: Manter=9942, Comprar=14504, Vender=12308\n",
      "Ganhos Totais: 35551.00, Perdas Totais: -34258.75\n",
      "Episode 77/100, Total Reward: 439.50, Win Rate: 0.53, Wins: 1020, Losses: 895, Epsilon: 0.2306, Steps: 36754, Time: 147.27s\n",
      "Ações: Manter=10491, Comprar=14580, Vender=11683\n",
      "Ganhos Totais: 35128.75, Perdas Totais: -34207.75\n",
      "Episode 78/100, Total Reward: -1641.75, Win Rate: 0.55, Wins: 1062, Losses: 854, Epsilon: 0.2283, Steps: 36754, Time: 145.62s\n",
      "Ações: Manter=10951, Comprar=15179, Vender=10624\n",
      "Ganhos Totais: 33214.25, Perdas Totais: -34375.25\n",
      "Episode 79/100, Total Reward: -2660.50, Win Rate: 0.55, Wins: 1077, Losses: 866, Epsilon: 0.2260, Steps: 36754, Time: 145.14s\n",
      "Ações: Manter=10791, Comprar=14949, Vender=11014\n",
      "Ganhos Totais: 34232.00, Perdas Totais: -36404.00\n",
      "Episode 80/100, Total Reward: -985.75, Win Rate: 0.53, Wins: 1032, Losses: 928, Epsilon: 0.2238, Steps: 36754, Time: 160.13s\n",
      "Ações: Manter=11126, Comprar=14059, Vender=11569\n",
      "Ganhos Totais: 33986.25, Perdas Totais: -34479.75\n",
      "Episode 81/100, Total Reward: -2490.50, Win Rate: 0.56, Wins: 1130, Losses: 897, Epsilon: 0.2215, Steps: 36754, Time: 134.15s\n",
      "Ações: Manter=9891, Comprar=13766, Vender=13097\n",
      "Ganhos Totais: 33970.50, Perdas Totais: -35952.00\n",
      "Episode 82/100, Total Reward: 1653.25, Win Rate: 0.56, Wins: 1076, Losses: 829, Epsilon: 0.2193, Steps: 36754, Time: 135.48s\n",
      "Ações: Manter=10861, Comprar=14229, Vender=11664\n",
      "Ganhos Totais: 36997.75, Perdas Totais: -34866.25\n",
      "Modelo e log do episódio 82 salvos em: 4.16\\model_episode_82.pth e 4.16\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: -491.25, Win Rate: 0.56, Wins: 1086, Losses: 838, Epsilon: 0.2171, Steps: 36754, Time: 143.74s\n",
      "Ações: Manter=10348, Comprar=15294, Vender=11112\n",
      "Ganhos Totais: 34203.25, Perdas Totais: -34209.50\n",
      "Episode 84/100, Total Reward: 3750.00, Win Rate: 0.56, Wins: 1098, Losses: 852, Epsilon: 0.2149, Steps: 36754, Time: 149.59s\n",
      "Ações: Manter=10747, Comprar=15304, Vender=10703\n",
      "Ganhos Totais: 38051.00, Perdas Totais: -33811.50\n",
      "Modelo e log do episódio 84 salvos em: 4.16\\model_episode_84.pth e 4.16\\log_episode_84.csv\n",
      "\n",
      "Episode 85/100, Total Reward: -922.00, Win Rate: 0.55, Wins: 1072, Losses: 868, Epsilon: 0.2128, Steps: 36754, Time: 138.05s\n",
      "Ações: Manter=9072, Comprar=15195, Vender=12487\n",
      "Ganhos Totais: 36128.25, Perdas Totais: -36563.25\n",
      "Episode 86/100, Total Reward: 411.25, Win Rate: 0.55, Wins: 1114, Losses: 900, Epsilon: 0.2107, Steps: 36754, Time: 138.94s\n",
      "Ações: Manter=9580, Comprar=15979, Vender=11195\n",
      "Ganhos Totais: 36559.75, Perdas Totais: -35643.25\n",
      "Episode 87/100, Total Reward: 2956.75, Win Rate: 0.56, Wins: 1123, Losses: 886, Epsilon: 0.2086, Steps: 36754, Time: 149.41s\n",
      "Ações: Manter=11396, Comprar=14032, Vender=11326\n",
      "Ganhos Totais: 37342.50, Perdas Totais: -33881.00\n",
      "Modelo e log do episódio 87 salvos em: 4.16\\model_episode_87.pth e 4.16\\log_episode_87.csv\n",
      "\n",
      "Episode 88/100, Total Reward: 11.50, Win Rate: 0.55, Wins: 1036, Losses: 838, Epsilon: 0.2065, Steps: 36754, Time: 138.18s\n",
      "Ações: Manter=9388, Comprar=14861, Vender=12505\n",
      "Ganhos Totais: 35138.50, Perdas Totais: -34656.50\n",
      "Episode 89/100, Total Reward: -857.00, Win Rate: 0.55, Wins: 1084, Losses: 892, Epsilon: 0.2044, Steps: 36754, Time: 138.02s\n",
      "Ações: Manter=11476, Comprar=13635, Vender=11643\n",
      "Ganhos Totais: 35735.75, Perdas Totais: -36095.25\n",
      "Episode 90/100, Total Reward: 1654.75, Win Rate: 0.54, Wins: 1002, Losses: 861, Epsilon: 0.2024, Steps: 36754, Time: 138.06s\n",
      "Ações: Manter=12395, Comprar=13189, Vender=11170\n",
      "Ganhos Totais: 35271.50, Perdas Totais: -33148.50\n",
      "Episode 91/100, Total Reward: -3227.75, Win Rate: 0.55, Wins: 1088, Losses: 898, Epsilon: 0.2003, Steps: 36754, Time: 137.84s\n",
      "Ações: Manter=10420, Comprar=15914, Vender=10420\n",
      "Ganhos Totais: 34105.00, Perdas Totais: -36834.50\n",
      "Episode 92/100, Total Reward: 3104.75, Win Rate: 0.56, Wins: 1084, Losses: 848, Epsilon: 0.1983, Steps: 36754, Time: 138.27s\n",
      "Ações: Manter=12198, Comprar=15662, Vender=8894\n",
      "Ganhos Totais: 36689.00, Perdas Totais: -33099.00\n",
      "Modelo e log do episódio 92 salvos em: 4.16\\model_episode_92.pth e 4.16\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: -1350.50, Win Rate: 0.56, Wins: 1011, Losses: 804, Epsilon: 0.1964, Steps: 36754, Time: 137.61s\n",
      "Ações: Manter=11197, Comprar=14923, Vender=10634\n",
      "Ganhos Totais: 35105.50, Perdas Totais: -36000.25\n",
      "Episode 94/100, Total Reward: -2860.00, Win Rate: 0.55, Wins: 1023, Losses: 837, Epsilon: 0.1944, Steps: 36754, Time: 137.58s\n",
      "Ações: Manter=9715, Comprar=16581, Vender=10458\n",
      "Ganhos Totais: 34089.00, Perdas Totais: -36482.50\n",
      "Episode 95/100, Total Reward: -966.50, Win Rate: 0.57, Wins: 1074, Losses: 799, Epsilon: 0.1924, Steps: 36754, Time: 138.07s\n",
      "Ações: Manter=10465, Comprar=16194, Vender=10095\n",
      "Ganhos Totais: 34468.00, Perdas Totais: -34963.75\n",
      "Episode 96/100, Total Reward: 218.25, Win Rate: 0.56, Wins: 1087, Losses: 843, Epsilon: 0.1905, Steps: 36754, Time: 142.24s\n",
      "Ações: Manter=10770, Comprar=14853, Vender=11131\n",
      "Ganhos Totais: 36228.75, Perdas Totais: -35525.00\n",
      "Episode 97/100, Total Reward: 2369.00, Win Rate: 0.57, Wins: 1048, Losses: 793, Epsilon: 0.1886, Steps: 36754, Time: 138.02s\n",
      "Ações: Manter=12067, Comprar=14121, Vender=10566\n",
      "Ganhos Totais: 36884.75, Perdas Totais: -34053.00\n",
      "Modelo e log do episódio 97 salvos em: 4.16\\model_episode_97.pth e 4.16\\log_episode_97.csv\n",
      "\n",
      "Episode 98/100, Total Reward: -696.75, Win Rate: 0.56, Wins: 1020, Losses: 792, Epsilon: 0.1867, Steps: 36754, Time: 137.56s\n",
      "Ações: Manter=10682, Comprar=15268, Vender=10804\n",
      "Ganhos Totais: 34907.75, Perdas Totais: -35149.75\n",
      "Episode 99/100, Total Reward: 266.75, Win Rate: 0.56, Wins: 1094, Losses: 869, Epsilon: 0.1849, Steps: 36754, Time: 132.01s\n",
      "Ações: Manter=10385, Comprar=15659, Vender=10710\n",
      "Ganhos Totais: 36438.50, Perdas Totais: -35678.50\n",
      "Episode 100/100, Total Reward: -1927.25, Win Rate: 0.55, Wins: 979, Losses: 812, Epsilon: 0.1830, Steps: 36754, Time: 126.72s\n",
      "Ações: Manter=11967, Comprar=14318, Vender=10469\n",
      "Ganhos Totais: 33663.75, Perdas Totais: -35142.25\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 61, Total Reward: 4128.00, Win Rate: 0.56, Wins: 1384, Losses: 1100, Ações: {0: 12152, 1: 14331, 2: 10271}, Steps: 36754, Time: 156.89s\n",
      "Rank 2: Episode 67, Total Reward: 3895.00, Win Rate: 0.55, Wins: 1314, Losses: 1060, Ações: {0: 11327, 1: 13538, 2: 11889}, Steps: 36754, Time: 171.10s\n",
      "Rank 3: Episode 84, Total Reward: 3750.00, Win Rate: 0.56, Wins: 1098, Losses: 852, Ações: {0: 10747, 1: 15304, 2: 10703}, Steps: 36754, Time: 149.59s\n",
      "Rank 4: Episode 34, Total Reward: 3490.00, Win Rate: 0.55, Wins: 1363, Losses: 1096, Ações: {0: 11724, 1: 13559, 2: 11471}, Steps: 36754, Time: 153.11s\n",
      "Rank 5: Episode 66, Total Reward: 3333.00, Win Rate: 0.56, Wins: 1191, Losses: 954, Ações: {0: 11940, 1: 13958, 2: 10856}, Steps: 36754, Time: 155.77s\n",
      "Rank 6: Episode 92, Total Reward: 3104.75, Win Rate: 0.56, Wins: 1084, Losses: 848, Ações: {0: 12198, 1: 15662, 2: 8894}, Steps: 36754, Time: 138.27s\n",
      "Rank 7: Episode 87, Total Reward: 2956.75, Win Rate: 0.56, Wins: 1123, Losses: 886, Ações: {0: 11396, 1: 14032, 2: 11326}, Steps: 36754, Time: 149.41s\n",
      "Rank 8: Episode 97, Total Reward: 2369.00, Win Rate: 0.57, Wins: 1048, Losses: 793, Ações: {0: 12067, 1: 14121, 2: 10566}, Steps: 36754, Time: 138.02s\n",
      "Rank 9: Episode 33, Total Reward: 1964.50, Win Rate: 0.54, Wins: 1264, Losses: 1070, Ações: {0: 12664, 1: 13907, 2: 10183}, Steps: 36754, Time: 153.02s\n",
      "Rank 10: Episode 31, Total Reward: 1796.00, Win Rate: 0.54, Wins: 1372, Losses: 1167, Ações: {0: 12728, 1: 11737, 2: 12289}, Steps: 36754, Time: 152.10s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V03_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28','dayO','dayH','dayL'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Definir o limite de stop loss\n",
    "        stop_loss = -200  # Limite de perda em pontos\n",
    "\n",
    "        # Verificar stop loss\n",
    "        stop_loss_triggered = False\n",
    "        if self.position != 0:\n",
    "            if self.position == 1:  # Posição comprada\n",
    "                # Calcular lucro não realizado\n",
    "                unrealized_profit = current_price - self.entry_price - 0.25\n",
    "            elif self.position == -1:  # Posição vendida\n",
    "                # Calcular lucro não realizado\n",
    "                unrealized_profit = self.entry_price - current_price - 0.25\n",
    "\n",
    "            # Se o lucro não realizado <= stop_loss, fechar a posição\n",
    "            if unrealized_profit <= stop_loss:\n",
    "                stop_loss_triggered = True\n",
    "                # Fechar a posição\n",
    "                self.exit_price = current_price\n",
    "                profit = unrealized_profit  # Lucro realizado é o lucro não realizado atual\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "\n",
    "                if self.position == 1:\n",
    "                    trade_type = 'close_long_stop_loss'\n",
    "                else:\n",
    "                    trade_type = 'close_short_stop_loss'\n",
    "\n",
    "                info['trade'] = {\n",
    "                    'type': trade_type,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy' if self.position == 1 else 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Processar a ação se a posição não foi fechada pelo stop loss\n",
    "        if not stop_loss_triggered:\n",
    "            # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "            if gatilho == 1:\n",
    "                if action == 1:  # Comprar\n",
    "                    if self.position == 0:\n",
    "                        self.position = 1  # Abrir posição comprada\n",
    "                        self.entry_price = current_price\n",
    "                        self.entry_step = self.current_step\n",
    "                        self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward -= 0.25  # Custo de operação\n",
    "                        info['trade'] = {\n",
    "                            'type': 'buy',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime\n",
    "                        }\n",
    "                    elif self.position == -1:\n",
    "                        # Fechar posição vendida\n",
    "                        self.exit_price = current_price\n",
    "                        profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                        self.exit_step = self.current_step\n",
    "                        self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward += profit\n",
    "                        info['trade'] = {\n",
    "                            'type': 'close_short',\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        }\n",
    "                        # Registrar a operação\n",
    "                        self.trades.append({\n",
    "                            'type': 'sell',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime,\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        })\n",
    "                        # Resetar posição\n",
    "                        self.position = 0\n",
    "                        self.entry_step = None\n",
    "                        self.entry_datetime = None\n",
    "                elif action == 2:  # Vender\n",
    "                    if self.position == 0:\n",
    "                        self.position = -1  # Abrir posição vendida\n",
    "                        self.entry_price = current_price\n",
    "                        self.entry_step = self.current_step\n",
    "                        self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward -= 0.25  # Custo de operação\n",
    "                        info['trade'] = {\n",
    "                            'type': 'sell',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime\n",
    "                        }\n",
    "                    elif self.position == 1:\n",
    "                        # Fechar posição comprada\n",
    "                        self.exit_price = current_price\n",
    "                        profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                        self.exit_step = self.current_step\n",
    "                        self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                        reward += profit\n",
    "                        info['trade'] = {\n",
    "                            'type': 'close_long',\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        }\n",
    "                        # Registrar a operação\n",
    "                        self.trades.append({\n",
    "                            'type': 'buy',\n",
    "                            'entry_step': self.entry_step,\n",
    "                            'entry_price': self.entry_price,\n",
    "                            'entry_datetime': self.entry_datetime,\n",
    "                            'exit_step': self.exit_step,\n",
    "                            'exit_price': self.exit_price,\n",
    "                            'exit_datetime': self.exit_datetime,\n",
    "                            'profit': profit\n",
    "                        })\n",
    "                        # Resetar posição\n",
    "                        self.position = 0\n",
    "                        self.entry_step = None\n",
    "                        self.entry_datetime = None\n",
    "                else:  # Manter\n",
    "                    pass  # Nenhuma ação necessária\n",
    "            else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "                # Se há uma posição aberta, fechá-la\n",
    "                if self.position == 1:  # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "                elif self.position == -1:  # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.16\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short', 'close_long_stop_loss', 'close_short_stop_loss']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
