{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -1249.75, Win Rate: 0.50, Wins: 1435, Losses: 1432, Epsilon: 0.4950, Steps: 36754, Time: 126.01s\n",
      "Ações: Manter=10235, Comprar=13452, Vender=13067\n",
      "Ganhos Totais: 37430.25, Perdas Totais: -38680.00\n",
      "Modelo e log do episódio 1 salvos em: 4.7.4\\model_episode_1.pth e 4.7.4\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -1488.25, Win Rate: 0.51, Wins: 1407, Losses: 1348, Epsilon: 0.4900, Steps: 36754, Time: 127.34s\n",
      "Ações: Manter=9605, Comprar=13272, Vender=13877\n",
      "Ganhos Totais: 37666.00, Perdas Totais: -39154.25\n",
      "Modelo e log do episódio 2 salvos em: 4.7.4\\model_episode_2.pth e 4.7.4\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -49.75, Win Rate: 0.52, Wins: 1447, Losses: 1354, Epsilon: 0.4851, Steps: 36754, Time: 130.58s\n",
      "Ações: Manter=10220, Comprar=13893, Vender=12641\n",
      "Ganhos Totais: 38505.50, Perdas Totais: -38555.25\n",
      "Modelo e log do episódio 3 salvos em: 4.7.4\\model_episode_3.pth e 4.7.4\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -2609.25, Win Rate: 0.51, Wins: 1356, Losses: 1278, Epsilon: 0.4803, Steps: 36754, Time: 116.85s\n",
      "Ações: Manter=11141, Comprar=13577, Vender=12036\n",
      "Ganhos Totais: 35847.25, Perdas Totais: -38456.50\n",
      "Modelo e log do episódio 4 salvos em: 4.7.4\\model_episode_4.pth e 4.7.4\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -2286.00, Win Rate: 0.50, Wins: 1269, Losses: 1245, Epsilon: 0.4755, Steps: 36754, Time: 108.38s\n",
      "Ações: Manter=12460, Comprar=11390, Vender=12904\n",
      "Ganhos Totais: 34683.00, Perdas Totais: -36969.00\n",
      "Modelo e log do episódio 5 salvos em: 4.7.4\\model_episode_5.pth e 4.7.4\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -3544.75, Win Rate: 0.52, Wins: 1398, Losses: 1305, Epsilon: 0.4707, Steps: 36754, Time: 108.74s\n",
      "Ações: Manter=10355, Comprar=13469, Vender=12930\n",
      "Ganhos Totais: 36872.50, Perdas Totais: -40417.25\n",
      "Modelo e log do episódio 6 salvos em: 4.7.4\\model_episode_6.pth e 4.7.4\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -1753.75, Win Rate: 0.53, Wins: 1396, Losses: 1235, Epsilon: 0.4660, Steps: 36754, Time: 106.75s\n",
      "Ações: Manter=11650, Comprar=12161, Vender=12943\n",
      "Ganhos Totais: 36827.25, Perdas Totais: -38581.00\n",
      "Modelo e log do episódio 7 salvos em: 4.7.4\\model_episode_7.pth e 4.7.4\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: 3959.25, Win Rate: 0.52, Wins: 1447, Losses: 1333, Epsilon: 0.4614, Steps: 36754, Time: 107.43s\n",
      "Ações: Manter=11733, Comprar=13193, Vender=11828\n",
      "Ganhos Totais: 38893.50, Perdas Totais: -34934.25\n",
      "Modelo e log do episódio 8 salvos em: 4.7.4\\model_episode_8.pth e 4.7.4\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -2866.50, Win Rate: 0.50, Wins: 1362, Losses: 1364, Epsilon: 0.4568, Steps: 36754, Time: 108.32s\n",
      "Ações: Manter=11409, Comprar=12997, Vender=12348\n",
      "Ganhos Totais: 37157.25, Perdas Totais: -40023.75\n",
      "Modelo e log do episódio 9 salvos em: 4.7.4\\model_episode_9.pth e 4.7.4\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -1589.25, Win Rate: 0.51, Wins: 1374, Losses: 1323, Epsilon: 0.4522, Steps: 36754, Time: 107.13s\n",
      "Ações: Manter=12174, Comprar=12396, Vender=12184\n",
      "Ganhos Totais: 36050.00, Perdas Totais: -37639.25\n",
      "Modelo e log do episódio 10 salvos em: 4.7.4\\model_episode_10.pth e 4.7.4\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -3948.50, Win Rate: 0.50, Wins: 1323, Losses: 1317, Epsilon: 0.4477, Steps: 36754, Time: 107.16s\n",
      "Ações: Manter=13590, Comprar=11254, Vender=11910\n",
      "Ganhos Totais: 34554.50, Perdas Totais: -38503.00\n",
      "Episode 12/100, Total Reward: -3894.25, Win Rate: 0.51, Wins: 1402, Losses: 1327, Epsilon: 0.4432, Steps: 36754, Time: 107.71s\n",
      "Ações: Manter=11384, Comprar=12398, Vender=12972\n",
      "Ganhos Totais: 35541.25, Perdas Totais: -39435.50\n",
      "Episode 13/100, Total Reward: 1965.25, Win Rate: 0.52, Wins: 1310, Losses: 1222, Epsilon: 0.4388, Steps: 36754, Time: 108.55s\n",
      "Ações: Manter=12402, Comprar=12514, Vender=11838\n",
      "Ganhos Totais: 37362.00, Perdas Totais: -35396.75\n",
      "Modelo e log do episódio 13 salvos em: 4.7.4\\model_episode_13.pth e 4.7.4\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: 2252.75, Win Rate: 0.52, Wins: 1362, Losses: 1262, Epsilon: 0.4344, Steps: 36754, Time: 107.62s\n",
      "Ações: Manter=12002, Comprar=11654, Vender=13098\n",
      "Ganhos Totais: 38156.75, Perdas Totais: -35904.00\n",
      "Modelo e log do episódio 14 salvos em: 4.7.4\\model_episode_14.pth e 4.7.4\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -5318.25, Win Rate: 0.51, Wins: 1314, Losses: 1277, Epsilon: 0.4300, Steps: 36754, Time: 108.05s\n",
      "Ações: Manter=11871, Comprar=11214, Vender=13669\n",
      "Ganhos Totais: 34228.00, Perdas Totais: -39546.25\n",
      "Episode 16/100, Total Reward: 2192.00, Win Rate: 0.52, Wins: 1352, Losses: 1237, Epsilon: 0.4257, Steps: 36754, Time: 110.61s\n",
      "Ações: Manter=12176, Comprar=12525, Vender=12053\n",
      "Ganhos Totais: 38014.50, Perdas Totais: -35822.50\n",
      "Modelo e log do episódio 16 salvos em: 4.7.4\\model_episode_16.pth e 4.7.4\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -2405.00, Win Rate: 0.52, Wins: 1323, Losses: 1225, Epsilon: 0.4215, Steps: 36754, Time: 107.33s\n",
      "Ações: Manter=12545, Comprar=11777, Vender=12432\n",
      "Ganhos Totais: 36074.75, Perdas Totais: -38479.75\n",
      "Episode 18/100, Total Reward: 569.50, Win Rate: 0.52, Wins: 1350, Losses: 1224, Epsilon: 0.4173, Steps: 36754, Time: 107.81s\n",
      "Ações: Manter=12741, Comprar=12956, Vender=11057\n",
      "Ganhos Totais: 36955.50, Perdas Totais: -36386.00\n",
      "Modelo e log do episódio 18 salvos em: 4.7.4\\model_episode_18.pth e 4.7.4\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -1341.00, Win Rate: 0.52, Wins: 1336, Losses: 1241, Epsilon: 0.4131, Steps: 36754, Time: 109.32s\n",
      "Ações: Manter=13085, Comprar=11776, Vender=11893\n",
      "Ganhos Totais: 36433.50, Perdas Totais: -37774.50\n",
      "Modelo e log do episódio 19 salvos em: 4.7.4\\model_episode_19.pth e 4.7.4\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 294.75, Win Rate: 0.52, Wins: 1338, Losses: 1237, Epsilon: 0.4090, Steps: 36754, Time: 109.37s\n",
      "Ações: Manter=11119, Comprar=12662, Vender=12973\n",
      "Ganhos Totais: 36710.00, Perdas Totais: -36415.25\n",
      "Modelo e log do episódio 20 salvos em: 4.7.4\\model_episode_20.pth e 4.7.4\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -1744.75, Win Rate: 0.53, Wins: 1275, Losses: 1145, Epsilon: 0.4049, Steps: 36754, Time: 108.05s\n",
      "Ações: Manter=12520, Comprar=12576, Vender=11658\n",
      "Ganhos Totais: 34964.00, Perdas Totais: -36708.75\n",
      "Episode 22/100, Total Reward: -6367.75, Win Rate: 0.51, Wins: 1292, Losses: 1237, Epsilon: 0.4008, Steps: 36754, Time: 108.90s\n",
      "Ações: Manter=11765, Comprar=12401, Vender=12588\n",
      "Ganhos Totais: 33051.25, Perdas Totais: -39419.00\n",
      "Episode 23/100, Total Reward: -5780.75, Win Rate: 0.51, Wins: 1204, Losses: 1179, Epsilon: 0.3968, Steps: 36754, Time: 109.04s\n",
      "Ações: Manter=13935, Comprar=11292, Vender=11527\n",
      "Ganhos Totais: 32616.75, Perdas Totais: -38397.50\n",
      "Episode 24/100, Total Reward: -576.75, Win Rate: 0.52, Wins: 1278, Losses: 1200, Epsilon: 0.3928, Steps: 36754, Time: 108.83s\n",
      "Ações: Manter=11437, Comprar=13569, Vender=11748\n",
      "Ganhos Totais: 37552.50, Perdas Totais: -38129.25\n",
      "Modelo e log do episódio 24 salvos em: 4.7.4\\model_episode_24.pth e 4.7.4\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: 1793.25, Win Rate: 0.53, Wins: 1321, Losses: 1192, Epsilon: 0.3889, Steps: 36754, Time: 108.13s\n",
      "Ações: Manter=12901, Comprar=12176, Vender=11677\n",
      "Ganhos Totais: 37293.25, Perdas Totais: -35500.00\n",
      "Modelo e log do episódio 25 salvos em: 4.7.4\\model_episode_25.pth e 4.7.4\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: 2972.00, Win Rate: 0.53, Wins: 1361, Losses: 1217, Epsilon: 0.3850, Steps: 36754, Time: 109.20s\n",
      "Ações: Manter=12157, Comprar=13499, Vender=11098\n",
      "Ganhos Totais: 38206.75, Perdas Totais: -35234.75\n",
      "Modelo e log do episódio 26 salvos em: 4.7.4\\model_episode_26.pth e 4.7.4\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: -1682.25, Win Rate: 0.51, Wins: 1249, Losses: 1210, Epsilon: 0.3812, Steps: 36754, Time: 108.90s\n",
      "Ações: Manter=12146, Comprar=14188, Vender=10420\n",
      "Ganhos Totais: 36219.00, Perdas Totais: -37901.25\n",
      "Episode 28/100, Total Reward: -3093.75, Win Rate: 0.51, Wins: 1238, Losses: 1178, Epsilon: 0.3774, Steps: 36754, Time: 108.47s\n",
      "Ações: Manter=14070, Comprar=11606, Vender=11078\n",
      "Ganhos Totais: 34271.00, Perdas Totais: -37364.75\n",
      "Episode 29/100, Total Reward: 912.50, Win Rate: 0.53, Wins: 1312, Losses: 1175, Epsilon: 0.3736, Steps: 36754, Time: 109.19s\n",
      "Ações: Manter=12372, Comprar=13510, Vender=10872\n",
      "Ganhos Totais: 37479.50, Perdas Totais: -36567.00\n",
      "Modelo e log do episódio 29 salvos em: 4.7.4\\model_episode_29.pth e 4.7.4\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: 376.00, Win Rate: 0.52, Wins: 1333, Losses: 1226, Epsilon: 0.3699, Steps: 36754, Time: 109.69s\n",
      "Ações: Manter=12188, Comprar=13302, Vender=11264\n",
      "Ganhos Totais: 36203.00, Perdas Totais: -35827.00\n",
      "Modelo e log do episódio 30 salvos em: 4.7.4\\model_episode_30.pth e 4.7.4\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: 1898.75, Win Rate: 0.53, Wins: 1454, Losses: 1305, Epsilon: 0.3662, Steps: 36754, Time: 108.71s\n",
      "Ações: Manter=12199, Comprar=12121, Vender=12434\n",
      "Ganhos Totais: 39219.50, Perdas Totais: -37320.75\n",
      "Modelo e log do episódio 31 salvos em: 4.7.4\\model_episode_31.pth e 4.7.4\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 4141.75, Win Rate: 0.52, Wins: 1332, Losses: 1218, Epsilon: 0.3625, Steps: 36754, Time: 108.65s\n",
      "Ações: Manter=11360, Comprar=12896, Vender=12498\n",
      "Ganhos Totais: 39966.00, Perdas Totais: -35824.25\n",
      "Modelo e log do episódio 32 salvos em: 4.7.4\\model_episode_32.pth e 4.7.4\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 95.50, Win Rate: 0.52, Wins: 1223, Losses: 1119, Epsilon: 0.3589, Steps: 36754, Time: 109.74s\n",
      "Ações: Manter=13631, Comprar=11163, Vender=11960\n",
      "Ganhos Totais: 36665.50, Perdas Totais: -36570.00\n",
      "Episode 34/100, Total Reward: -3000.25, Win Rate: 0.53, Wins: 1210, Losses: 1076, Epsilon: 0.3553, Steps: 36754, Time: 114.49s\n",
      "Ações: Manter=14851, Comprar=10501, Vender=11402\n",
      "Ganhos Totais: 32492.00, Perdas Totais: -35492.25\n",
      "Episode 35/100, Total Reward: -271.00, Win Rate: 0.51, Wins: 1204, Losses: 1153, Epsilon: 0.3517, Steps: 36754, Time: 109.44s\n",
      "Ações: Manter=13051, Comprar=11910, Vender=11793\n",
      "Ganhos Totais: 35067.25, Perdas Totais: -35338.25\n",
      "Episode 36/100, Total Reward: 1059.25, Win Rate: 0.53, Wins: 1313, Losses: 1143, Epsilon: 0.3482, Steps: 36754, Time: 109.65s\n",
      "Ações: Manter=12166, Comprar=13615, Vender=10973\n",
      "Ganhos Totais: 35507.00, Perdas Totais: -34447.75\n",
      "Modelo e log do episódio 36 salvos em: 4.7.4\\model_episode_36.pth e 4.7.4\\log_episode_36.csv\n",
      "\n",
      "Episode 37/100, Total Reward: 3363.25, Win Rate: 0.52, Wins: 1264, Losses: 1165, Epsilon: 0.3447, Steps: 36754, Time: 109.88s\n",
      "Ações: Manter=12496, Comprar=13060, Vender=11198\n",
      "Ganhos Totais: 37752.50, Perdas Totais: -34389.25\n",
      "Modelo e log do episódio 37 salvos em: 4.7.4\\model_episode_37.pth e 4.7.4\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: 523.75, Win Rate: 0.53, Wins: 1274, Losses: 1122, Epsilon: 0.3413, Steps: 36754, Time: 109.56s\n",
      "Ações: Manter=14392, Comprar=11660, Vender=10702\n",
      "Ganhos Totais: 35891.25, Perdas Totais: -35367.50\n",
      "Episode 39/100, Total Reward: -3261.25, Win Rate: 0.52, Wins: 1179, Losses: 1090, Epsilon: 0.3379, Steps: 36754, Time: 110.27s\n",
      "Ações: Manter=14676, Comprar=11265, Vender=10813\n",
      "Ganhos Totais: 32650.75, Perdas Totais: -35912.00\n",
      "Episode 40/100, Total Reward: -1151.00, Win Rate: 0.51, Wins: 1206, Losses: 1148, Epsilon: 0.3345, Steps: 36754, Time: 110.06s\n",
      "Ações: Manter=13368, Comprar=11397, Vender=11989\n",
      "Ganhos Totais: 34170.75, Perdas Totais: -35321.75\n",
      "Episode 41/100, Total Reward: -640.00, Win Rate: 0.52, Wins: 1271, Losses: 1195, Epsilon: 0.3311, Steps: 36754, Time: 110.13s\n",
      "Ações: Manter=12406, Comprar=12840, Vender=11508\n",
      "Ganhos Totais: 36284.75, Perdas Totais: -36924.75\n",
      "Episode 42/100, Total Reward: -1643.00, Win Rate: 0.52, Wins: 1214, Losses: 1108, Epsilon: 0.3278, Steps: 36754, Time: 110.04s\n",
      "Ações: Manter=13021, Comprar=12394, Vender=11339\n",
      "Ganhos Totais: 35621.25, Perdas Totais: -37264.25\n",
      "Episode 43/100, Total Reward: 850.00, Win Rate: 0.53, Wins: 1298, Losses: 1171, Epsilon: 0.3246, Steps: 36754, Time: 110.65s\n",
      "Ações: Manter=12287, Comprar=12157, Vender=12310\n",
      "Ganhos Totais: 35828.25, Perdas Totais: -34978.25\n",
      "Episode 44/100, Total Reward: -356.50, Win Rate: 0.52, Wins: 1281, Losses: 1202, Epsilon: 0.3213, Steps: 36754, Time: 109.97s\n",
      "Ações: Manter=13442, Comprar=11911, Vender=11401\n",
      "Ganhos Totais: 35188.75, Perdas Totais: -35545.25\n",
      "Episode 45/100, Total Reward: -1255.00, Win Rate: 0.51, Wins: 1227, Losses: 1193, Epsilon: 0.3181, Steps: 36754, Time: 109.37s\n",
      "Ações: Manter=11640, Comprar=11996, Vender=13118\n",
      "Ganhos Totais: 35160.00, Perdas Totais: -36415.00\n",
      "Episode 46/100, Total Reward: -889.50, Win Rate: 0.53, Wins: 1406, Losses: 1240, Epsilon: 0.3149, Steps: 36754, Time: 110.69s\n",
      "Ações: Manter=11534, Comprar=12167, Vender=13053\n",
      "Ganhos Totais: 36163.75, Perdas Totais: -37053.25\n",
      "Episode 47/100, Total Reward: -4750.00, Win Rate: 0.52, Wins: 1174, Losses: 1090, Epsilon: 0.3118, Steps: 36754, Time: 110.28s\n",
      "Ações: Manter=12441, Comprar=11505, Vender=12808\n",
      "Ganhos Totais: 32190.00, Perdas Totais: -36940.00\n",
      "Episode 48/100, Total Reward: -108.50, Win Rate: 0.52, Wins: 1132, Losses: 1054, Epsilon: 0.3086, Steps: 36754, Time: 141.61s\n",
      "Ações: Manter=13495, Comprar=10155, Vender=13104\n",
      "Ganhos Totais: 33993.00, Perdas Totais: -34101.50\n",
      "Episode 49/100, Total Reward: -964.50, Win Rate: 0.52, Wins: 1316, Losses: 1211, Epsilon: 0.3056, Steps: 36754, Time: 146.66s\n",
      "Ações: Manter=12301, Comprar=13654, Vender=10799\n",
      "Ganhos Totais: 36480.25, Perdas Totais: -37444.75\n",
      "Episode 50/100, Total Reward: -1111.00, Win Rate: 0.50, Wins: 1090, Losses: 1069, Epsilon: 0.3025, Steps: 36754, Time: 139.43s\n",
      "Ações: Manter=13187, Comprar=12739, Vender=10828\n",
      "Ganhos Totais: 33648.00, Perdas Totais: -34759.00\n",
      "Episode 51/100, Total Reward: -3147.50, Win Rate: 0.51, Wins: 1131, Losses: 1100, Epsilon: 0.2995, Steps: 36754, Time: 154.30s\n",
      "Ações: Manter=11593, Comprar=11896, Vender=13265\n",
      "Ganhos Totais: 34397.25, Perdas Totais: -37544.75\n",
      "Episode 52/100, Total Reward: -2192.00, Win Rate: 0.52, Wins: 1195, Losses: 1100, Epsilon: 0.2965, Steps: 36754, Time: 181.87s\n",
      "Ações: Manter=14328, Comprar=11309, Vender=11117\n",
      "Ganhos Totais: 33315.50, Perdas Totais: -35507.50\n",
      "Episode 53/100, Total Reward: -346.25, Win Rate: 0.52, Wins: 1259, Losses: 1141, Epsilon: 0.2935, Steps: 36754, Time: 154.01s\n",
      "Ações: Manter=11705, Comprar=14303, Vender=10746\n",
      "Ganhos Totais: 37175.75, Perdas Totais: -37522.00\n",
      "Episode 54/100, Total Reward: -4958.00, Win Rate: 0.52, Wins: 1189, Losses: 1081, Epsilon: 0.2906, Steps: 36754, Time: 165.29s\n",
      "Ações: Manter=13118, Comprar=12143, Vender=11493\n",
      "Ganhos Totais: 32755.75, Perdas Totais: -37713.75\n",
      "Episode 55/100, Total Reward: 2974.50, Win Rate: 0.54, Wins: 1231, Losses: 1044, Epsilon: 0.2877, Steps: 36754, Time: 161.18s\n",
      "Ações: Manter=15658, Comprar=10627, Vender=10469\n",
      "Ganhos Totais: 35501.00, Perdas Totais: -32526.50\n",
      "Modelo e log do episódio 55 salvos em: 4.7.4\\model_episode_55.pth e 4.7.4\\log_episode_55.csv\n",
      "\n",
      "Episode 56/100, Total Reward: 3079.50, Win Rate: 0.54, Wins: 1329, Losses: 1127, Epsilon: 0.2848, Steps: 36754, Time: 159.42s\n",
      "Ações: Manter=13514, Comprar=13538, Vender=9702\n",
      "Ganhos Totais: 37248.75, Perdas Totais: -34169.25\n",
      "Modelo e log do episódio 56 salvos em: 4.7.4\\model_episode_56.pth e 4.7.4\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: 1300.75, Win Rate: 0.55, Wins: 1364, Losses: 1122, Epsilon: 0.2820, Steps: 36754, Time: 148.64s\n",
      "Ações: Manter=14201, Comprar=13223, Vender=9330\n",
      "Ganhos Totais: 36934.00, Perdas Totais: -35633.25\n",
      "Episode 58/100, Total Reward: -415.00, Win Rate: 0.54, Wins: 1266, Losses: 1087, Epsilon: 0.2791, Steps: 36754, Time: 164.49s\n",
      "Ações: Manter=13708, Comprar=12907, Vender=10139\n",
      "Ganhos Totais: 35004.75, Perdas Totais: -35419.75\n",
      "Episode 59/100, Total Reward: -1573.50, Win Rate: 0.53, Wins: 1423, Losses: 1266, Epsilon: 0.2763, Steps: 36754, Time: 161.30s\n",
      "Ações: Manter=11642, Comprar=15006, Vender=10106\n",
      "Ganhos Totais: 36444.25, Perdas Totais: -38017.75\n",
      "Episode 60/100, Total Reward: 277.50, Win Rate: 0.53, Wins: 1359, Losses: 1191, Epsilon: 0.2736, Steps: 36754, Time: 153.48s\n",
      "Ações: Manter=14021, Comprar=13118, Vender=9615\n",
      "Ganhos Totais: 36232.25, Perdas Totais: -35954.75\n",
      "Episode 61/100, Total Reward: 1419.25, Win Rate: 0.53, Wins: 1327, Losses: 1156, Epsilon: 0.2708, Steps: 36754, Time: 154.32s\n",
      "Ações: Manter=12435, Comprar=13114, Vender=11205\n",
      "Ganhos Totais: 36472.00, Perdas Totais: -35052.75\n",
      "Episode 62/100, Total Reward: 5071.00, Win Rate: 0.54, Wins: 1236, Losses: 1035, Epsilon: 0.2681, Steps: 36754, Time: 153.02s\n",
      "Ações: Manter=11981, Comprar=13409, Vender=11364\n",
      "Ganhos Totais: 37805.25, Perdas Totais: -32734.25\n",
      "Modelo e log do episódio 62 salvos em: 4.7.4\\model_episode_62.pth e 4.7.4\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: 485.25, Win Rate: 0.54, Wins: 1248, Losses: 1057, Epsilon: 0.2655, Steps: 36754, Time: 152.12s\n",
      "Ações: Manter=14688, Comprar=12711, Vender=9355\n",
      "Ganhos Totais: 35221.00, Perdas Totais: -34735.75\n",
      "Episode 64/100, Total Reward: 1982.75, Win Rate: 0.55, Wins: 1345, Losses: 1091, Epsilon: 0.2628, Steps: 36754, Time: 152.85s\n",
      "Ações: Manter=12729, Comprar=11443, Vender=12582\n",
      "Ganhos Totais: 37047.75, Perdas Totais: -35065.00\n",
      "Modelo e log do episódio 64 salvos em: 4.7.4\\model_episode_64.pth e 4.7.4\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: 1712.00, Win Rate: 0.54, Wins: 1385, Losses: 1176, Epsilon: 0.2602, Steps: 36754, Time: 151.04s\n",
      "Ações: Manter=11715, Comprar=13942, Vender=11097\n",
      "Ganhos Totais: 36367.50, Perdas Totais: -34655.50\n",
      "Episode 66/100, Total Reward: -19.25, Win Rate: 0.52, Wins: 1145, Losses: 1047, Epsilon: 0.2576, Steps: 36754, Time: 151.83s\n",
      "Ações: Manter=11231, Comprar=15303, Vender=10220\n",
      "Ganhos Totais: 33331.50, Perdas Totais: -33350.75\n",
      "Episode 67/100, Total Reward: 61.25, Win Rate: 0.53, Wins: 1148, Losses: 1012, Epsilon: 0.2550, Steps: 36754, Time: 150.91s\n",
      "Ações: Manter=13258, Comprar=11831, Vender=11665\n",
      "Ganhos Totais: 34802.50, Perdas Totais: -34741.25\n",
      "Episode 68/100, Total Reward: 653.50, Win Rate: 0.54, Wins: 1395, Losses: 1182, Epsilon: 0.2524, Steps: 36754, Time: 158.40s\n",
      "Ações: Manter=11032, Comprar=16218, Vender=9504\n",
      "Ganhos Totais: 37290.50, Perdas Totais: -36637.00\n",
      "Episode 69/100, Total Reward: 3459.75, Win Rate: 0.55, Wins: 1357, Losses: 1130, Epsilon: 0.2499, Steps: 36754, Time: 156.52s\n",
      "Ações: Manter=13333, Comprar=12717, Vender=10704\n",
      "Ganhos Totais: 37988.75, Perdas Totais: -34529.00\n",
      "Modelo e log do episódio 69 salvos em: 4.7.4\\model_episode_69.pth e 4.7.4\\log_episode_69.csv\n",
      "\n",
      "Episode 70/100, Total Reward: -1320.75, Win Rate: 0.52, Wins: 1246, Losses: 1157, Epsilon: 0.2474, Steps: 36754, Time: 139.55s\n",
      "Ações: Manter=11519, Comprar=14124, Vender=11111\n",
      "Ganhos Totais: 34443.75, Perdas Totais: -35764.50\n",
      "Episode 71/100, Total Reward: -2605.75, Win Rate: 0.53, Wins: 1104, Losses: 968, Epsilon: 0.2449, Steps: 36754, Time: 140.57s\n",
      "Ações: Manter=14192, Comprar=12269, Vender=10293\n",
      "Ganhos Totais: 32503.25, Perdas Totais: -35109.00\n",
      "Episode 72/100, Total Reward: -4491.00, Win Rate: 0.53, Wins: 1151, Losses: 1009, Epsilon: 0.2425, Steps: 36754, Time: 138.89s\n",
      "Ações: Manter=11205, Comprar=13693, Vender=11856\n",
      "Ganhos Totais: 32399.75, Perdas Totais: -36890.75\n",
      "Episode 73/100, Total Reward: 45.50, Win Rate: 0.54, Wins: 1268, Losses: 1102, Epsilon: 0.2401, Steps: 36754, Time: 144.39s\n",
      "Ações: Manter=9400, Comprar=15792, Vender=11562\n",
      "Ganhos Totais: 35361.00, Perdas Totais: -35315.50\n",
      "Episode 74/100, Total Reward: 4527.50, Win Rate: 0.56, Wins: 1215, Losses: 943, Epsilon: 0.2377, Steps: 36754, Time: 139.82s\n",
      "Ações: Manter=10984, Comprar=13895, Vender=11875\n",
      "Ganhos Totais: 38497.25, Perdas Totais: -33969.75\n",
      "Modelo e log do episódio 74 salvos em: 4.7.4\\model_episode_74.pth e 4.7.4\\log_episode_74.csv\n",
      "\n",
      "Episode 75/100, Total Reward: 732.50, Win Rate: 0.54, Wins: 1167, Losses: 989, Epsilon: 0.2353, Steps: 36754, Time: 139.07s\n",
      "Ações: Manter=10139, Comprar=14978, Vender=11637\n",
      "Ganhos Totais: 34863.75, Perdas Totais: -34131.25\n",
      "Episode 76/100, Total Reward: 366.50, Win Rate: 0.54, Wins: 1168, Losses: 1009, Epsilon: 0.2329, Steps: 36754, Time: 142.95s\n",
      "Ações: Manter=10501, Comprar=15697, Vender=10556\n",
      "Ganhos Totais: 35774.25, Perdas Totais: -35407.75\n",
      "Episode 77/100, Total Reward: -308.00, Win Rate: 0.55, Wins: 1156, Losses: 940, Epsilon: 0.2306, Steps: 36754, Time: 136.26s\n",
      "Ações: Manter=11319, Comprar=14406, Vender=11029\n",
      "Ganhos Totais: 34660.25, Perdas Totais: -34968.25\n",
      "Episode 78/100, Total Reward: -1231.50, Win Rate: 0.53, Wins: 1133, Losses: 1015, Epsilon: 0.2283, Steps: 36754, Time: 139.40s\n",
      "Ações: Manter=14585, Comprar=12225, Vender=9944\n",
      "Ganhos Totais: 33895.50, Perdas Totais: -35127.00\n",
      "Episode 79/100, Total Reward: 1100.75, Win Rate: 0.55, Wins: 1278, Losses: 1026, Epsilon: 0.2260, Steps: 36754, Time: 134.78s\n",
      "Ações: Manter=12747, Comprar=13424, Vender=10583\n",
      "Ganhos Totais: 36501.75, Perdas Totais: -35401.00\n",
      "Episode 80/100, Total Reward: -3679.25, Win Rate: 0.53, Wins: 1086, Losses: 948, Epsilon: 0.2238, Steps: 36754, Time: 127.52s\n",
      "Ações: Manter=14361, Comprar=13293, Vender=9100\n",
      "Ganhos Totais: 31917.25, Perdas Totais: -35596.50\n",
      "Episode 81/100, Total Reward: -4042.50, Win Rate: 0.53, Wins: 1035, Losses: 934, Epsilon: 0.2215, Steps: 36754, Time: 133.66s\n",
      "Ações: Manter=14212, Comprar=11272, Vender=11270\n",
      "Ganhos Totais: 30756.75, Perdas Totais: -34799.25\n",
      "Episode 82/100, Total Reward: 1043.00, Win Rate: 0.54, Wins: 1207, Losses: 1008, Epsilon: 0.2193, Steps: 36754, Time: 126.52s\n",
      "Ações: Manter=11254, Comprar=14479, Vender=11021\n",
      "Ganhos Totais: 35740.75, Perdas Totais: -34697.75\n",
      "Episode 83/100, Total Reward: 990.50, Win Rate: 0.55, Wins: 1373, Losses: 1121, Epsilon: 0.2171, Steps: 36754, Time: 126.41s\n",
      "Ações: Manter=11783, Comprar=15671, Vender=9300\n",
      "Ganhos Totais: 36313.50, Perdas Totais: -35323.00\n",
      "Episode 84/100, Total Reward: -689.00, Win Rate: 0.56, Wins: 1364, Losses: 1073, Epsilon: 0.2149, Steps: 36754, Time: 125.91s\n",
      "Ações: Manter=8879, Comprar=18124, Vender=9751\n",
      "Ganhos Totais: 35771.50, Perdas Totais: -36460.50\n",
      "Episode 85/100, Total Reward: 801.50, Win Rate: 0.55, Wins: 1258, Losses: 1030, Epsilon: 0.2128, Steps: 36754, Time: 125.98s\n",
      "Ações: Manter=8803, Comprar=19297, Vender=8654\n",
      "Ganhos Totais: 36011.50, Perdas Totais: -35210.00\n",
      "Episode 86/100, Total Reward: -1158.25, Win Rate: 0.54, Wins: 1098, Losses: 937, Epsilon: 0.2107, Steps: 36754, Time: 126.03s\n",
      "Ações: Manter=14124, Comprar=13087, Vender=9543\n",
      "Ganhos Totais: 31805.25, Perdas Totais: -32963.50\n",
      "Episode 87/100, Total Reward: -713.75, Win Rate: 0.57, Wins: 1230, Losses: 940, Epsilon: 0.2086, Steps: 36754, Time: 125.26s\n",
      "Ações: Manter=12278, Comprar=13753, Vender=10723\n",
      "Ganhos Totais: 33711.50, Perdas Totais: -34425.25\n",
      "Episode 88/100, Total Reward: 2091.00, Win Rate: 0.56, Wins: 1211, Losses: 960, Epsilon: 0.2065, Steps: 36754, Time: 125.49s\n",
      "Ações: Manter=11805, Comprar=15652, Vender=9297\n",
      "Ganhos Totais: 34747.50, Perdas Totais: -32656.50\n",
      "Episode 89/100, Total Reward: 2330.25, Win Rate: 0.56, Wins: 1243, Losses: 971, Epsilon: 0.2044, Steps: 36754, Time: 125.53s\n",
      "Ações: Manter=11471, Comprar=16824, Vender=8459\n",
      "Ganhos Totais: 35619.25, Perdas Totais: -33289.00\n",
      "Modelo e log do episódio 89 salvos em: 4.7.4\\model_episode_89.pth e 4.7.4\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: 774.00, Win Rate: 0.55, Wins: 1216, Losses: 1014, Epsilon: 0.2024, Steps: 36754, Time: 125.05s\n",
      "Ações: Manter=11580, Comprar=16195, Vender=8979\n",
      "Ganhos Totais: 35768.25, Perdas Totais: -34994.25\n",
      "Episode 91/100, Total Reward: -1830.50, Win Rate: 0.54, Wins: 1251, Losses: 1053, Epsilon: 0.2003, Steps: 36754, Time: 125.63s\n",
      "Ações: Manter=9973, Comprar=16592, Vender=10189\n",
      "Ganhos Totais: 32606.00, Perdas Totais: -34436.50\n",
      "Episode 92/100, Total Reward: 5555.25, Win Rate: 0.57, Wins: 1259, Losses: 935, Epsilon: 0.1983, Steps: 36754, Time: 125.75s\n",
      "Ações: Manter=9744, Comprar=17185, Vender=9825\n",
      "Ganhos Totais: 37540.75, Perdas Totais: -31985.50\n",
      "Modelo e log do episódio 92 salvos em: 4.7.4\\model_episode_92.pth e 4.7.4\\log_episode_92.csv\n",
      "\n",
      "Episode 93/100, Total Reward: -3035.00, Win Rate: 0.53, Wins: 1025, Losses: 905, Epsilon: 0.1964, Steps: 36754, Time: 125.04s\n",
      "Ações: Manter=11353, Comprar=16339, Vender=9062\n",
      "Ganhos Totais: 31459.00, Perdas Totais: -34494.00\n",
      "Episode 94/100, Total Reward: 545.50, Win Rate: 0.55, Wins: 1153, Losses: 939, Epsilon: 0.1944, Steps: 36754, Time: 125.45s\n",
      "Ações: Manter=8914, Comprar=19628, Vender=8212\n",
      "Ganhos Totais: 34076.50, Perdas Totais: -33531.00\n",
      "Episode 95/100, Total Reward: 729.00, Win Rate: 0.54, Wins: 1116, Losses: 945, Epsilon: 0.1924, Steps: 36754, Time: 125.48s\n",
      "Ações: Manter=10535, Comprar=18537, Vender=7682\n",
      "Ganhos Totais: 34073.75, Perdas Totais: -33344.75\n",
      "Episode 96/100, Total Reward: 2387.25, Win Rate: 0.56, Wins: 1208, Losses: 954, Epsilon: 0.1905, Steps: 36754, Time: 124.38s\n",
      "Ações: Manter=11046, Comprar=18205, Vender=7503\n",
      "Ganhos Totais: 35322.50, Perdas Totais: -32935.25\n",
      "Episode 97/100, Total Reward: 1026.75, Win Rate: 0.55, Wins: 1128, Losses: 918, Epsilon: 0.1886, Steps: 36754, Time: 126.79s\n",
      "Ações: Manter=11857, Comprar=16703, Vender=8194\n",
      "Ganhos Totais: 34974.25, Perdas Totais: -33947.50\n",
      "Episode 98/100, Total Reward: -927.25, Win Rate: 0.55, Wins: 1160, Losses: 958, Epsilon: 0.1867, Steps: 36754, Time: 124.81s\n",
      "Ações: Manter=11639, Comprar=17359, Vender=7756\n",
      "Ganhos Totais: 34128.25, Perdas Totais: -35055.50\n",
      "Episode 99/100, Total Reward: 2833.50, Win Rate: 0.55, Wins: 1224, Losses: 1001, Epsilon: 0.1849, Steps: 36754, Time: 125.05s\n",
      "Ações: Manter=9868, Comprar=18574, Vender=8312\n",
      "Ganhos Totais: 37104.50, Perdas Totais: -34271.00\n",
      "Episode 100/100, Total Reward: 1254.50, Win Rate: 0.55, Wins: 1173, Losses: 961, Epsilon: 0.1830, Steps: 36754, Time: 125.40s\n",
      "Ações: Manter=11971, Comprar=17552, Vender=7231\n",
      "Ganhos Totais: 35442.00, Perdas Totais: -34187.50\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 92, Total Reward: 5555.25, Win Rate: 0.57, Wins: 1259, Losses: 935, Ações: {0: 9744, 1: 17185, 2: 9825}, Steps: 36754, Time: 125.75s\n",
      "Rank 2: Episode 62, Total Reward: 5071.00, Win Rate: 0.54, Wins: 1236, Losses: 1035, Ações: {0: 11981, 1: 13409, 2: 11364}, Steps: 36754, Time: 153.02s\n",
      "Rank 3: Episode 74, Total Reward: 4527.50, Win Rate: 0.56, Wins: 1215, Losses: 943, Ações: {0: 10984, 1: 13895, 2: 11875}, Steps: 36754, Time: 139.82s\n",
      "Rank 4: Episode 32, Total Reward: 4141.75, Win Rate: 0.52, Wins: 1332, Losses: 1218, Ações: {0: 11360, 1: 12896, 2: 12498}, Steps: 36754, Time: 108.65s\n",
      "Rank 5: Episode 8, Total Reward: 3959.25, Win Rate: 0.52, Wins: 1447, Losses: 1333, Ações: {0: 11733, 1: 13193, 2: 11828}, Steps: 36754, Time: 107.43s\n",
      "Rank 6: Episode 69, Total Reward: 3459.75, Win Rate: 0.55, Wins: 1357, Losses: 1130, Ações: {0: 13333, 1: 12717, 2: 10704}, Steps: 36754, Time: 156.52s\n",
      "Rank 7: Episode 37, Total Reward: 3363.25, Win Rate: 0.52, Wins: 1264, Losses: 1165, Ações: {0: 12496, 1: 13060, 2: 11198}, Steps: 36754, Time: 109.88s\n",
      "Rank 8: Episode 56, Total Reward: 3079.50, Win Rate: 0.54, Wins: 1329, Losses: 1127, Ações: {0: 13514, 1: 13538, 2: 9702}, Steps: 36754, Time: 159.42s\n",
      "Rank 9: Episode 55, Total Reward: 2974.50, Win Rate: 0.54, Wins: 1231, Losses: 1044, Ações: {0: 15658, 1: 10627, 2: 10469}, Steps: 36754, Time: 161.18s\n",
      "Rank 10: Episode 26, Total Reward: 2972.00, Win Rate: 0.53, Wins: 1361, Losses: 1217, Ações: {0: 12157, 1: 13499, 2: 11098}, Steps: 36754, Time: 109.20s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.4\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
