{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -3976.25, Win Rate: 0.49, Wins: 1294, Losses: 1325, Epsilon: 0.4950, Steps: 36754, Time: 157.28s\n",
      "Ações: Manter=12229, Comprar=12834, Vender=11691\n",
      "Ganhos Totais: 33782.25, Perdas Totais: -37758.50\n",
      "Modelo e log do episódio 1 salvos em: 4.7.7\\model_episode_1.pth e 4.7.7\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -1703.00, Win Rate: 0.51, Wins: 1329, Losses: 1279, Epsilon: 0.4900, Steps: 36754, Time: 156.38s\n",
      "Ações: Manter=10133, Comprar=13301, Vender=13320\n",
      "Ganhos Totais: 35855.25, Perdas Totais: -37558.25\n",
      "Modelo e log do episódio 2 salvos em: 4.7.7\\model_episode_2.pth e 4.7.7\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -1274.50, Win Rate: 0.52, Wins: 1367, Losses: 1269, Epsilon: 0.4851, Steps: 36754, Time: 174.15s\n",
      "Ações: Manter=12098, Comprar=13381, Vender=11275\n",
      "Ganhos Totais: 37235.75, Perdas Totais: -38510.25\n",
      "Modelo e log do episódio 3 salvos em: 4.7.7\\model_episode_3.pth e 4.7.7\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -2642.25, Win Rate: 0.51, Wins: 1368, Losses: 1307, Epsilon: 0.4803, Steps: 36754, Time: 178.94s\n",
      "Ações: Manter=12166, Comprar=13024, Vender=11564\n",
      "Ganhos Totais: 36245.00, Perdas Totais: -38887.25\n",
      "Modelo e log do episódio 4 salvos em: 4.7.7\\model_episode_4.pth e 4.7.7\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -2463.25, Win Rate: 0.52, Wins: 1404, Losses: 1318, Epsilon: 0.4755, Steps: 36754, Time: 182.59s\n",
      "Ações: Manter=11980, Comprar=13345, Vender=11429\n",
      "Ganhos Totais: 37022.75, Perdas Totais: -39486.00\n",
      "Modelo e log do episódio 5 salvos em: 4.7.7\\model_episode_5.pth e 4.7.7\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: 616.00, Win Rate: 0.54, Wins: 1435, Losses: 1241, Epsilon: 0.4707, Steps: 36754, Time: 183.03s\n",
      "Ações: Manter=12379, Comprar=12741, Vender=11634\n",
      "Ganhos Totais: 36375.75, Perdas Totais: -35759.75\n",
      "Modelo e log do episódio 6 salvos em: 4.7.7\\model_episode_6.pth e 4.7.7\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -2760.25, Win Rate: 0.53, Wins: 1507, Losses: 1322, Epsilon: 0.4660, Steps: 36754, Time: 183.95s\n",
      "Ações: Manter=11861, Comprar=13355, Vender=11538\n",
      "Ganhos Totais: 37259.75, Perdas Totais: -40020.00\n",
      "Modelo e log do episódio 7 salvos em: 4.7.7\\model_episode_7.pth e 4.7.7\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -1023.50, Win Rate: 0.54, Wins: 1486, Losses: 1271, Epsilon: 0.4614, Steps: 36754, Time: 182.78s\n",
      "Ações: Manter=12045, Comprar=12614, Vender=12095\n",
      "Ganhos Totais: 37917.75, Perdas Totais: -38941.25\n",
      "Modelo e log do episódio 8 salvos em: 4.7.7\\model_episode_8.pth e 4.7.7\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -5413.50, Win Rate: 0.51, Wins: 1381, Losses: 1324, Epsilon: 0.4568, Steps: 36754, Time: 181.51s\n",
      "Ações: Manter=11800, Comprar=12467, Vender=12487\n",
      "Ganhos Totais: 35435.50, Perdas Totais: -40849.00\n",
      "Modelo e log do episódio 9 salvos em: 4.7.7\\model_episode_9.pth e 4.7.7\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: 3629.75, Win Rate: 0.55, Wins: 1461, Losses: 1183, Epsilon: 0.4522, Steps: 36754, Time: 185.85s\n",
      "Ações: Manter=12586, Comprar=12332, Vender=11836\n",
      "Ganhos Totais: 40337.25, Perdas Totais: -36707.50\n",
      "Modelo e log do episódio 10 salvos em: 4.7.7\\model_episode_10.pth e 4.7.7\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: 5426.75, Win Rate: 0.56, Wins: 1565, Losses: 1220, Epsilon: 0.4477, Steps: 36754, Time: 182.76s\n",
      "Ações: Manter=11412, Comprar=12872, Vender=12470\n",
      "Ganhos Totais: 41447.00, Perdas Totais: -36020.25\n",
      "Modelo e log do episódio 11 salvos em: 4.7.7\\model_episode_11.pth e 4.7.7\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: 589.25, Win Rate: 0.54, Wins: 1615, Losses: 1401, Epsilon: 0.4432, Steps: 36754, Time: 181.54s\n",
      "Ações: Manter=10887, Comprar=13494, Vender=12373\n",
      "Ganhos Totais: 38540.75, Perdas Totais: -37951.50\n",
      "Modelo e log do episódio 12 salvos em: 4.7.7\\model_episode_12.pth e 4.7.7\\log_episode_12.csv\n",
      "\n",
      "Episode 13/100, Total Reward: 640.75, Win Rate: 0.54, Wins: 1470, Losses: 1232, Epsilon: 0.4388, Steps: 36754, Time: 181.57s\n",
      "Ações: Manter=13144, Comprar=11592, Vender=12018\n",
      "Ganhos Totais: 36449.25, Perdas Totais: -35808.50\n",
      "Modelo e log do episódio 13 salvos em: 4.7.7\\model_episode_13.pth e 4.7.7\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -2345.00, Win Rate: 0.52, Wins: 1455, Losses: 1321, Epsilon: 0.4344, Steps: 36754, Time: 121.22s\n",
      "Ações: Manter=12057, Comprar=12385, Vender=12312\n",
      "Ganhos Totais: 37623.00, Perdas Totais: -39968.00\n",
      "Modelo e log do episódio 14 salvos em: 4.7.7\\model_episode_14.pth e 4.7.7\\log_episode_14.csv\n",
      "\n",
      "Episode 15/100, Total Reward: -2277.25, Win Rate: 0.52, Wins: 1390, Losses: 1262, Epsilon: 0.4300, Steps: 36754, Time: 118.46s\n",
      "Ações: Manter=12364, Comprar=12712, Vender=11678\n",
      "Ganhos Totais: 36051.25, Perdas Totais: -38328.50\n",
      "Modelo e log do episódio 15 salvos em: 4.7.7\\model_episode_15.pth e 4.7.7\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: -1484.75, Win Rate: 0.54, Wins: 1455, Losses: 1247, Epsilon: 0.4257, Steps: 36754, Time: 121.67s\n",
      "Ações: Manter=12957, Comprar=11731, Vender=12066\n",
      "Ganhos Totais: 37026.50, Perdas Totais: -38511.25\n",
      "Modelo e log do episódio 16 salvos em: 4.7.7\\model_episode_16.pth e 4.7.7\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -1348.25, Win Rate: 0.53, Wins: 1412, Losses: 1253, Epsilon: 0.4215, Steps: 36754, Time: 125.95s\n",
      "Ações: Manter=12507, Comprar=12806, Vender=11441\n",
      "Ganhos Totais: 38710.75, Perdas Totais: -40059.00\n",
      "Modelo e log do episódio 17 salvos em: 4.7.7\\model_episode_17.pth e 4.7.7\\log_episode_17.csv\n",
      "\n",
      "Episode 18/100, Total Reward: 587.50, Win Rate: 0.53, Wins: 1467, Losses: 1281, Epsilon: 0.4173, Steps: 36754, Time: 126.65s\n",
      "Ações: Manter=12729, Comprar=11965, Vender=12060\n",
      "Ganhos Totais: 37426.75, Perdas Totais: -36839.25\n",
      "Modelo e log do episódio 18 salvos em: 4.7.7\\model_episode_18.pth e 4.7.7\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -4270.00, Win Rate: 0.52, Wins: 1391, Losses: 1290, Epsilon: 0.4131, Steps: 36754, Time: 126.40s\n",
      "Ações: Manter=13791, Comprar=11937, Vender=11026\n",
      "Ganhos Totais: 36299.25, Perdas Totais: -40569.25\n",
      "Episode 20/100, Total Reward: -1122.00, Win Rate: 0.53, Wins: 1356, Losses: 1195, Epsilon: 0.4090, Steps: 36754, Time: 124.59s\n",
      "Ações: Manter=12354, Comprar=11852, Vender=12548\n",
      "Ganhos Totais: 36816.00, Perdas Totais: -37938.00\n",
      "Modelo e log do episódio 20 salvos em: 4.7.7\\model_episode_20.pth e 4.7.7\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: -4946.50, Win Rate: 0.53, Wins: 1375, Losses: 1200, Epsilon: 0.4049, Steps: 36754, Time: 118.80s\n",
      "Ações: Manter=12676, Comprar=12561, Vender=11517\n",
      "Ganhos Totais: 34966.50, Perdas Totais: -39913.00\n",
      "Episode 22/100, Total Reward: 104.25, Win Rate: 0.54, Wins: 1291, Losses: 1093, Epsilon: 0.4008, Steps: 36754, Time: 119.73s\n",
      "Ações: Manter=13206, Comprar=12857, Vender=10691\n",
      "Ganhos Totais: 36579.75, Perdas Totais: -36475.50\n",
      "Modelo e log do episódio 22 salvos em: 4.7.7\\model_episode_22.pth e 4.7.7\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: 1359.25, Win Rate: 0.54, Wins: 1314, Losses: 1124, Epsilon: 0.3968, Steps: 36754, Time: 134.01s\n",
      "Ações: Manter=13040, Comprar=13330, Vender=10384\n",
      "Ganhos Totais: 36935.75, Perdas Totais: -35576.50\n",
      "Modelo e log do episódio 23 salvos em: 4.7.7\\model_episode_23.pth e 4.7.7\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: 2210.75, Win Rate: 0.53, Wins: 1283, Losses: 1146, Epsilon: 0.3928, Steps: 36754, Time: 141.28s\n",
      "Ações: Manter=11981, Comprar=13951, Vender=10822\n",
      "Ganhos Totais: 37811.00, Perdas Totais: -35600.25\n",
      "Modelo e log do episódio 24 salvos em: 4.7.7\\model_episode_24.pth e 4.7.7\\log_episode_24.csv\n",
      "\n",
      "Episode 25/100, Total Reward: -1131.75, Win Rate: 0.53, Wins: 1275, Losses: 1146, Epsilon: 0.3889, Steps: 36754, Time: 143.65s\n",
      "Ações: Manter=12941, Comprar=13651, Vender=10162\n",
      "Ganhos Totais: 36638.00, Perdas Totais: -37769.75\n",
      "Episode 26/100, Total Reward: 2145.50, Win Rate: 0.54, Wins: 1281, Losses: 1104, Epsilon: 0.3850, Steps: 36754, Time: 149.09s\n",
      "Ações: Manter=13426, Comprar=13424, Vender=9904\n",
      "Ganhos Totais: 37628.25, Perdas Totais: -35482.75\n",
      "Modelo e log do episódio 26 salvos em: 4.7.7\\model_episode_26.pth e 4.7.7\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: 2592.75, Win Rate: 0.55, Wins: 1313, Losses: 1084, Epsilon: 0.3812, Steps: 36754, Time: 129.40s\n",
      "Ações: Manter=13876, Comprar=13509, Vender=9369\n",
      "Ganhos Totais: 36601.50, Perdas Totais: -34008.75\n",
      "Modelo e log do episódio 27 salvos em: 4.7.7\\model_episode_27.pth e 4.7.7\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: 2255.75, Win Rate: 0.55, Wins: 1408, Losses: 1163, Epsilon: 0.3774, Steps: 36754, Time: 137.73s\n",
      "Ações: Manter=12163, Comprar=15152, Vender=9439\n",
      "Ganhos Totais: 38664.75, Perdas Totais: -36409.00\n",
      "Modelo e log do episódio 28 salvos em: 4.7.7\\model_episode_28.pth e 4.7.7\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: 2359.00, Win Rate: 0.53, Wins: 1336, Losses: 1164, Epsilon: 0.3736, Steps: 36754, Time: 131.28s\n",
      "Ações: Manter=12649, Comprar=14282, Vender=9823\n",
      "Ganhos Totais: 36609.00, Perdas Totais: -34250.00\n",
      "Modelo e log do episódio 29 salvos em: 4.7.7\\model_episode_29.pth e 4.7.7\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: -970.25, Win Rate: 0.53, Wins: 1283, Losses: 1121, Epsilon: 0.3699, Steps: 36754, Time: 136.35s\n",
      "Ações: Manter=12881, Comprar=14185, Vender=9688\n",
      "Ganhos Totais: 35870.75, Perdas Totais: -36841.00\n",
      "Episode 31/100, Total Reward: -1656.00, Win Rate: 0.53, Wins: 1257, Losses: 1094, Epsilon: 0.3662, Steps: 36754, Time: 141.39s\n",
      "Ações: Manter=14243, Comprar=12698, Vender=9813\n",
      "Ganhos Totais: 34520.75, Perdas Totais: -36176.75\n",
      "Episode 32/100, Total Reward: 2375.25, Win Rate: 0.53, Wins: 1293, Losses: 1138, Epsilon: 0.3625, Steps: 36754, Time: 134.50s\n",
      "Ações: Manter=12842, Comprar=13426, Vender=10486\n",
      "Ganhos Totais: 37542.25, Perdas Totais: -35167.00\n",
      "Modelo e log do episódio 32 salvos em: 4.7.7\\model_episode_32.pth e 4.7.7\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 379.75, Win Rate: 0.54, Wins: 1281, Losses: 1073, Epsilon: 0.3589, Steps: 36754, Time: 124.26s\n",
      "Ações: Manter=14321, Comprar=12071, Vender=10362\n",
      "Ganhos Totais: 37155.50, Perdas Totais: -36775.75\n",
      "Episode 34/100, Total Reward: -1852.50, Win Rate: 0.54, Wins: 1209, Losses: 1050, Epsilon: 0.3553, Steps: 36754, Time: 123.62s\n",
      "Ações: Manter=13141, Comprar=14093, Vender=9520\n",
      "Ganhos Totais: 35214.50, Perdas Totais: -37067.00\n",
      "Episode 35/100, Total Reward: -2998.00, Win Rate: 0.55, Wins: 1212, Losses: 998, Epsilon: 0.3517, Steps: 36754, Time: 118.98s\n",
      "Ações: Manter=13255, Comprar=13200, Vender=10299\n",
      "Ganhos Totais: 34179.50, Perdas Totais: -37177.50\n",
      "Episode 36/100, Total Reward: -556.00, Win Rate: 0.53, Wins: 1331, Losses: 1158, Epsilon: 0.3482, Steps: 36754, Time: 119.19s\n",
      "Ações: Manter=11178, Comprar=14952, Vender=10624\n",
      "Ganhos Totais: 36340.00, Perdas Totais: -36896.00\n",
      "Episode 37/100, Total Reward: 2303.75, Win Rate: 0.55, Wins: 1387, Losses: 1115, Epsilon: 0.3447, Steps: 36754, Time: 119.26s\n",
      "Ações: Manter=11761, Comprar=14717, Vender=10276\n",
      "Ganhos Totais: 37260.25, Perdas Totais: -34956.50\n",
      "Modelo e log do episódio 37 salvos em: 4.7.7\\model_episode_37.pth e 4.7.7\\log_episode_37.csv\n",
      "\n",
      "Episode 38/100, Total Reward: -2951.50, Win Rate: 0.54, Wins: 1334, Losses: 1144, Epsilon: 0.3413, Steps: 36754, Time: 119.23s\n",
      "Ações: Manter=13879, Comprar=12033, Vender=10842\n",
      "Ganhos Totais: 35690.75, Perdas Totais: -38642.25\n",
      "Episode 39/100, Total Reward: -2047.25, Win Rate: 0.53, Wins: 1200, Losses: 1075, Epsilon: 0.3379, Steps: 36754, Time: 118.87s\n",
      "Ações: Manter=13185, Comprar=13673, Vender=9896\n",
      "Ganhos Totais: 34366.50, Perdas Totais: -36413.75\n",
      "Episode 40/100, Total Reward: -3034.25, Win Rate: 0.55, Wins: 1408, Losses: 1160, Epsilon: 0.3345, Steps: 36754, Time: 119.82s\n",
      "Ações: Manter=13423, Comprar=13943, Vender=9388\n",
      "Ganhos Totais: 34706.75, Perdas Totais: -37741.00\n",
      "Episode 41/100, Total Reward: 381.50, Win Rate: 0.55, Wins: 1476, Losses: 1211, Epsilon: 0.3311, Steps: 36754, Time: 119.48s\n",
      "Ações: Manter=10778, Comprar=15541, Vender=10435\n",
      "Ganhos Totais: 37379.75, Perdas Totais: -36998.25\n",
      "Episode 42/100, Total Reward: 2091.25, Win Rate: 0.55, Wins: 1332, Losses: 1102, Epsilon: 0.3278, Steps: 36754, Time: 119.22s\n",
      "Ações: Manter=14000, Comprar=13141, Vender=9613\n",
      "Ganhos Totais: 37905.25, Perdas Totais: -35814.00\n",
      "Modelo e log do episódio 42 salvos em: 4.7.7\\model_episode_42.pth e 4.7.7\\log_episode_42.csv\n",
      "\n",
      "Episode 43/100, Total Reward: -592.25, Win Rate: 0.56, Wins: 1396, Losses: 1116, Epsilon: 0.3246, Steps: 36754, Time: 119.64s\n",
      "Ações: Manter=12049, Comprar=15529, Vender=9176\n",
      "Ganhos Totais: 35752.00, Perdas Totais: -36344.25\n",
      "Episode 44/100, Total Reward: 651.25, Win Rate: 0.55, Wins: 1277, Losses: 1057, Epsilon: 0.3213, Steps: 36754, Time: 119.53s\n",
      "Ações: Manter=12793, Comprar=13445, Vender=10516\n",
      "Ganhos Totais: 34907.00, Perdas Totais: -34255.75\n",
      "Episode 45/100, Total Reward: 670.00, Win Rate: 0.55, Wins: 1272, Losses: 1027, Epsilon: 0.3181, Steps: 36754, Time: 119.29s\n",
      "Ações: Manter=13707, Comprar=11478, Vender=11569\n",
      "Ganhos Totais: 35527.00, Perdas Totais: -34857.00\n",
      "Episode 46/100, Total Reward: 2552.00, Win Rate: 0.54, Wins: 1241, Losses: 1059, Epsilon: 0.3149, Steps: 36754, Time: 119.87s\n",
      "Ações: Manter=13005, Comprar=13404, Vender=10345\n",
      "Ganhos Totais: 38110.75, Perdas Totais: -35558.75\n",
      "Modelo e log do episódio 46 salvos em: 4.7.7\\model_episode_46.pth e 4.7.7\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: -1781.75, Win Rate: 0.54, Wins: 1208, Losses: 1049, Epsilon: 0.3118, Steps: 36754, Time: 135.65s\n",
      "Ações: Manter=12651, Comprar=13146, Vender=10957\n",
      "Ganhos Totais: 35004.25, Perdas Totais: -36786.00\n",
      "Episode 48/100, Total Reward: 1535.00, Win Rate: 0.56, Wins: 1197, Losses: 956, Epsilon: 0.3086, Steps: 36754, Time: 135.12s\n",
      "Ações: Manter=15000, Comprar=11465, Vender=10289\n",
      "Ganhos Totais: 35068.25, Perdas Totais: -33533.25\n",
      "Episode 49/100, Total Reward: -2123.00, Win Rate: 0.55, Wins: 1187, Losses: 984, Epsilon: 0.3056, Steps: 36754, Time: 135.57s\n",
      "Ações: Manter=14527, Comprar=12482, Vender=9745\n",
      "Ganhos Totais: 34438.75, Perdas Totais: -36561.75\n",
      "Episode 50/100, Total Reward: 462.50, Win Rate: 0.55, Wins: 1193, Losses: 957, Epsilon: 0.3025, Steps: 36754, Time: 135.26s\n",
      "Ações: Manter=16390, Comprar=10633, Vender=9731\n",
      "Ganhos Totais: 36088.75, Perdas Totais: -35626.25\n",
      "Episode 51/100, Total Reward: -2137.00, Win Rate: 0.54, Wins: 1123, Losses: 956, Epsilon: 0.2995, Steps: 36754, Time: 134.54s\n",
      "Ações: Manter=14604, Comprar=10187, Vender=11963\n",
      "Ganhos Totais: 32808.50, Perdas Totais: -34945.50\n",
      "Episode 52/100, Total Reward: 1325.50, Win Rate: 0.54, Wins: 1115, Losses: 960, Epsilon: 0.2965, Steps: 36754, Time: 135.36s\n",
      "Ações: Manter=12680, Comprar=12155, Vender=11919\n",
      "Ganhos Totais: 35475.50, Perdas Totais: -34150.00\n",
      "Episode 53/100, Total Reward: 2187.50, Win Rate: 0.57, Wins: 1274, Losses: 964, Epsilon: 0.2935, Steps: 36754, Time: 134.94s\n",
      "Ações: Manter=12086, Comprar=13966, Vender=10702\n",
      "Ganhos Totais: 37021.50, Perdas Totais: -34834.00\n",
      "Modelo e log do episódio 53 salvos em: 4.7.7\\model_episode_53.pth e 4.7.7\\log_episode_53.csv\n",
      "\n",
      "Episode 54/100, Total Reward: 509.75, Win Rate: 0.52, Wins: 1059, Losses: 975, Epsilon: 0.2906, Steps: 36754, Time: 135.58s\n",
      "Ações: Manter=13276, Comprar=12337, Vender=11141\n",
      "Ganhos Totais: 34124.50, Perdas Totais: -33614.75\n",
      "Episode 55/100, Total Reward: 800.25, Win Rate: 0.54, Wins: 1230, Losses: 1045, Epsilon: 0.2877, Steps: 36754, Time: 135.89s\n",
      "Ações: Manter=14137, Comprar=11355, Vender=11262\n",
      "Ganhos Totais: 35931.00, Perdas Totais: -35130.75\n",
      "Episode 56/100, Total Reward: 347.25, Win Rate: 0.55, Wins: 1100, Losses: 909, Epsilon: 0.2848, Steps: 36754, Time: 135.57s\n",
      "Ações: Manter=15497, Comprar=10557, Vender=10700\n",
      "Ganhos Totais: 33203.75, Perdas Totais: -32856.50\n",
      "Episode 57/100, Total Reward: -69.00, Win Rate: 0.55, Wins: 1261, Losses: 1029, Epsilon: 0.2820, Steps: 36754, Time: 138.79s\n",
      "Ações: Manter=14408, Comprar=11296, Vender=11050\n",
      "Ganhos Totais: 34113.50, Perdas Totais: -34182.50\n",
      "Episode 58/100, Total Reward: -1902.00, Win Rate: 0.54, Wins: 1147, Losses: 995, Epsilon: 0.2791, Steps: 36754, Time: 121.60s\n",
      "Ações: Manter=13363, Comprar=10979, Vender=12412\n",
      "Ganhos Totais: 33217.75, Perdas Totais: -35119.75\n",
      "Episode 59/100, Total Reward: 4702.75, Win Rate: 0.57, Wins: 1282, Losses: 974, Epsilon: 0.2763, Steps: 36754, Time: 120.68s\n",
      "Ações: Manter=11774, Comprar=12693, Vender=12287\n",
      "Ganhos Totais: 37444.25, Perdas Totais: -32741.50\n",
      "Modelo e log do episódio 59 salvos em: 4.7.7\\model_episode_59.pth e 4.7.7\\log_episode_59.csv\n",
      "\n",
      "Episode 60/100, Total Reward: -1925.75, Win Rate: 0.53, Wins: 1168, Losses: 1025, Epsilon: 0.2736, Steps: 36754, Time: 121.36s\n",
      "Ações: Manter=11576, Comprar=12029, Vender=13149\n",
      "Ganhos Totais: 34744.75, Perdas Totais: -36670.50\n",
      "Episode 61/100, Total Reward: -2446.25, Win Rate: 0.52, Wins: 1021, Losses: 940, Epsilon: 0.2708, Steps: 36754, Time: 142.46s\n",
      "Ações: Manter=13490, Comprar=11285, Vender=11979\n",
      "Ganhos Totais: 32534.50, Perdas Totais: -34980.75\n",
      "Episode 62/100, Total Reward: 4035.00, Win Rate: 0.55, Wins: 1234, Losses: 1009, Epsilon: 0.2681, Steps: 36754, Time: 139.72s\n",
      "Ações: Manter=10854, Comprar=13096, Vender=12804\n",
      "Ganhos Totais: 37936.00, Perdas Totais: -33901.00\n",
      "Modelo e log do episódio 62 salvos em: 4.7.7\\model_episode_62.pth e 4.7.7\\log_episode_62.csv\n",
      "\n",
      "Episode 63/100, Total Reward: -1274.75, Win Rate: 0.55, Wins: 1147, Losses: 920, Epsilon: 0.2655, Steps: 36754, Time: 137.45s\n",
      "Ações: Manter=10012, Comprar=15172, Vender=11570\n",
      "Ganhos Totais: 35315.75, Perdas Totais: -36590.50\n",
      "Episode 64/100, Total Reward: -92.25, Win Rate: 0.54, Wins: 1091, Losses: 926, Epsilon: 0.2628, Steps: 36754, Time: 137.25s\n",
      "Ações: Manter=10216, Comprar=15499, Vender=11039\n",
      "Ganhos Totais: 35489.75, Perdas Totais: -35582.00\n",
      "Episode 65/100, Total Reward: -3530.25, Win Rate: 0.53, Wins: 1058, Losses: 942, Epsilon: 0.2602, Steps: 36754, Time: 136.79s\n",
      "Ações: Manter=7634, Comprar=16386, Vender=12734\n",
      "Ganhos Totais: 33481.25, Perdas Totais: -37011.50\n",
      "Episode 66/100, Total Reward: 4569.50, Win Rate: 0.56, Wins: 1184, Losses: 921, Epsilon: 0.2576, Steps: 36754, Time: 137.90s\n",
      "Ações: Manter=7361, Comprar=17551, Vender=11842\n",
      "Ganhos Totais: 38539.50, Perdas Totais: -33970.00\n",
      "Modelo e log do episódio 66 salvos em: 4.7.7\\model_episode_66.pth e 4.7.7\\log_episode_66.csv\n",
      "\n",
      "Episode 67/100, Total Reward: 562.00, Win Rate: 0.55, Wins: 1184, Losses: 960, Epsilon: 0.2550, Steps: 36754, Time: 137.56s\n",
      "Ações: Manter=8250, Comprar=17367, Vender=11137\n",
      "Ganhos Totais: 35115.50, Perdas Totais: -34553.50\n",
      "Episode 68/100, Total Reward: 2385.50, Win Rate: 0.56, Wins: 1205, Losses: 945, Epsilon: 0.2524, Steps: 36754, Time: 136.81s\n",
      "Ações: Manter=9399, Comprar=15142, Vender=12213\n",
      "Ganhos Totais: 37255.25, Perdas Totais: -34869.75\n",
      "Modelo e log do episódio 68 salvos em: 4.7.7\\model_episode_68.pth e 4.7.7\\log_episode_68.csv\n",
      "\n",
      "Episode 69/100, Total Reward: 360.00, Win Rate: 0.55, Wins: 1135, Losses: 916, Epsilon: 0.2499, Steps: 36754, Time: 137.80s\n",
      "Ações: Manter=8965, Comprar=16637, Vender=11152\n",
      "Ganhos Totais: 36089.25, Perdas Totais: -35729.25\n",
      "Episode 70/100, Total Reward: 3212.50, Win Rate: 0.56, Wins: 1157, Losses: 898, Epsilon: 0.2474, Steps: 36754, Time: 136.95s\n",
      "Ações: Manter=8548, Comprar=15308, Vender=12898\n",
      "Ganhos Totais: 37058.75, Perdas Totais: -33846.25\n",
      "Modelo e log do episódio 70 salvos em: 4.7.7\\model_episode_70.pth e 4.7.7\\log_episode_70.csv\n",
      "\n",
      "Episode 71/100, Total Reward: 248.25, Win Rate: 0.56, Wins: 1116, Losses: 876, Epsilon: 0.2449, Steps: 36754, Time: 137.46s\n",
      "Ações: Manter=7673, Comprar=16395, Vender=12686\n",
      "Ganhos Totais: 35356.75, Perdas Totais: -35108.50\n",
      "Episode 72/100, Total Reward: 3325.25, Win Rate: 0.57, Wins: 1100, Losses: 843, Epsilon: 0.2425, Steps: 36754, Time: 137.26s\n",
      "Ações: Manter=8446, Comprar=17425, Vender=10883\n",
      "Ganhos Totais: 36627.25, Perdas Totais: -33302.00\n",
      "Modelo e log do episódio 72 salvos em: 4.7.7\\model_episode_72.pth e 4.7.7\\log_episode_72.csv\n",
      "\n",
      "Episode 73/100, Total Reward: -4661.00, Win Rate: 0.54, Wins: 1039, Losses: 868, Epsilon: 0.2401, Steps: 36754, Time: 135.40s\n",
      "Ações: Manter=9282, Comprar=15527, Vender=11945\n",
      "Ganhos Totais: 32291.50, Perdas Totais: -36952.50\n",
      "Episode 74/100, Total Reward: 2554.00, Win Rate: 0.55, Wins: 1078, Losses: 874, Epsilon: 0.2377, Steps: 36754, Time: 132.25s\n",
      "Ações: Manter=8164, Comprar=16364, Vender=12226\n",
      "Ganhos Totais: 36435.00, Perdas Totais: -33881.00\n",
      "Modelo e log do episódio 74 salvos em: 4.7.7\\model_episode_74.pth e 4.7.7\\log_episode_74.csv\n",
      "\n",
      "Episode 75/100, Total Reward: 2917.00, Win Rate: 0.55, Wins: 1020, Losses: 825, Epsilon: 0.2353, Steps: 36754, Time: 137.71s\n",
      "Ações: Manter=12010, Comprar=12026, Vender=12718\n",
      "Ganhos Totais: 35107.75, Perdas Totais: -32190.75\n",
      "Modelo e log do episódio 75 salvos em: 4.7.7\\model_episode_75.pth e 4.7.7\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: 1521.50, Win Rate: 0.55, Wins: 1059, Losses: 884, Epsilon: 0.2329, Steps: 36754, Time: 137.44s\n",
      "Ações: Manter=9222, Comprar=15088, Vender=12444\n",
      "Ganhos Totais: 35905.25, Perdas Totais: -34383.75\n",
      "Episode 77/100, Total Reward: 2891.25, Win Rate: 0.55, Wins: 1004, Losses: 820, Epsilon: 0.2306, Steps: 36754, Time: 136.88s\n",
      "Ações: Manter=10131, Comprar=16144, Vender=10479\n",
      "Ganhos Totais: 35671.75, Perdas Totais: -32780.50\n",
      "Modelo e log do episódio 77 salvos em: 4.7.7\\model_episode_77.pth e 4.7.7\\log_episode_77.csv\n",
      "\n",
      "Episode 78/100, Total Reward: 2343.50, Win Rate: 0.55, Wins: 1030, Losses: 828, Epsilon: 0.2283, Steps: 36754, Time: 137.10s\n",
      "Ações: Manter=11270, Comprar=13773, Vender=11711\n",
      "Ganhos Totais: 35380.75, Perdas Totais: -33037.25\n",
      "Episode 79/100, Total Reward: -595.00, Win Rate: 0.53, Wins: 985, Losses: 879, Epsilon: 0.2260, Steps: 36754, Time: 136.50s\n",
      "Ações: Manter=9247, Comprar=14716, Vender=12791\n",
      "Ganhos Totais: 35022.00, Perdas Totais: -35617.00\n",
      "Episode 80/100, Total Reward: 808.25, Win Rate: 0.54, Wins: 1016, Losses: 872, Epsilon: 0.2238, Steps: 36754, Time: 137.26s\n",
      "Ações: Manter=6620, Comprar=18129, Vender=12005\n",
      "Ganhos Totais: 35221.25, Perdas Totais: -34413.00\n",
      "Episode 81/100, Total Reward: 838.25, Win Rate: 0.54, Wins: 921, Losses: 780, Epsilon: 0.2215, Steps: 36754, Time: 137.31s\n",
      "Ações: Manter=9700, Comprar=13943, Vender=13111\n",
      "Ganhos Totais: 34719.00, Perdas Totais: -33880.75\n",
      "Episode 82/100, Total Reward: 184.50, Win Rate: 0.52, Wins: 992, Losses: 901, Epsilon: 0.2193, Steps: 36754, Time: 136.82s\n",
      "Ações: Manter=8061, Comprar=13505, Vender=15188\n",
      "Ganhos Totais: 35848.75, Perdas Totais: -35664.25\n",
      "Episode 83/100, Total Reward: -5765.75, Win Rate: 0.51, Wins: 926, Losses: 889, Epsilon: 0.2171, Steps: 36754, Time: 137.40s\n",
      "Ações: Manter=7259, Comprar=16247, Vender=13248\n",
      "Ganhos Totais: 32529.75, Perdas Totais: -38295.50\n",
      "Episode 84/100, Total Reward: -1248.00, Win Rate: 0.54, Wins: 957, Losses: 821, Epsilon: 0.2149, Steps: 36754, Time: 137.08s\n",
      "Ações: Manter=9237, Comprar=13266, Vender=14251\n",
      "Ganhos Totais: 34893.75, Perdas Totais: -36141.75\n",
      "Episode 85/100, Total Reward: -4058.75, Win Rate: 0.51, Wins: 945, Losses: 890, Epsilon: 0.2128, Steps: 36754, Time: 137.97s\n",
      "Ações: Manter=9435, Comprar=13787, Vender=13532\n",
      "Ganhos Totais: 33112.75, Perdas Totais: -37171.50\n",
      "Episode 86/100, Total Reward: -6214.50, Win Rate: 0.52, Wins: 907, Losses: 837, Epsilon: 0.2107, Steps: 36754, Time: 137.96s\n",
      "Ações: Manter=7182, Comprar=17118, Vender=12454\n",
      "Ganhos Totais: 31623.25, Perdas Totais: -37837.75\n",
      "Episode 87/100, Total Reward: -2892.00, Win Rate: 0.54, Wins: 942, Losses: 814, Epsilon: 0.2086, Steps: 36754, Time: 137.27s\n",
      "Ações: Manter=9461, Comprar=12632, Vender=14661\n",
      "Ganhos Totais: 33790.00, Perdas Totais: -36682.00\n",
      "Episode 88/100, Total Reward: -2158.00, Win Rate: 0.52, Wins: 1005, Losses: 916, Epsilon: 0.2065, Steps: 36754, Time: 137.75s\n",
      "Ações: Manter=6829, Comprar=16028, Vender=13897\n",
      "Ganhos Totais: 34501.00, Perdas Totais: -36659.00\n",
      "Episode 89/100, Total Reward: -2862.25, Win Rate: 0.52, Wins: 911, Losses: 851, Epsilon: 0.2044, Steps: 36754, Time: 137.46s\n",
      "Ações: Manter=6579, Comprar=16112, Vender=14063\n",
      "Ganhos Totais: 32739.50, Perdas Totais: -35601.75\n",
      "Episode 90/100, Total Reward: -343.00, Win Rate: 0.54, Wins: 962, Losses: 836, Epsilon: 0.2024, Steps: 36754, Time: 137.37s\n",
      "Ações: Manter=7186, Comprar=15025, Vender=14543\n",
      "Ganhos Totais: 36026.50, Perdas Totais: -36369.50\n",
      "Episode 91/100, Total Reward: -472.50, Win Rate: 0.53, Wins: 992, Losses: 871, Epsilon: 0.2003, Steps: 36754, Time: 137.82s\n",
      "Ações: Manter=9508, Comprar=13986, Vender=13260\n",
      "Ganhos Totais: 34699.75, Perdas Totais: -35172.25\n",
      "Episode 92/100, Total Reward: 606.75, Win Rate: 0.53, Wins: 1054, Losses: 927, Epsilon: 0.1983, Steps: 36754, Time: 137.37s\n",
      "Ações: Manter=9213, Comprar=10921, Vender=16620\n",
      "Ganhos Totais: 36223.00, Perdas Totais: -35616.25\n",
      "Episode 93/100, Total Reward: -2645.00, Win Rate: 0.55, Wins: 980, Losses: 805, Epsilon: 0.1964, Steps: 36754, Time: 137.96s\n",
      "Ações: Manter=8091, Comprar=13013, Vender=15650\n",
      "Ganhos Totais: 33185.00, Perdas Totais: -35830.00\n",
      "Episode 94/100, Total Reward: 1116.75, Win Rate: 0.56, Wins: 1048, Losses: 838, Epsilon: 0.1944, Steps: 36754, Time: 138.46s\n",
      "Ações: Manter=7429, Comprar=13127, Vender=16198\n",
      "Ganhos Totais: 34870.75, Perdas Totais: -33754.00\n",
      "Episode 95/100, Total Reward: 1530.00, Win Rate: 0.54, Wins: 1087, Losses: 909, Epsilon: 0.1924, Steps: 36754, Time: 137.45s\n",
      "Ações: Manter=7786, Comprar=15015, Vender=13953\n",
      "Ganhos Totais: 37116.75, Perdas Totais: -35586.75\n",
      "Episode 96/100, Total Reward: 570.50, Win Rate: 0.54, Wins: 1051, Losses: 880, Epsilon: 0.1905, Steps: 36754, Time: 136.73s\n",
      "Ações: Manter=8525, Comprar=14225, Vender=14004\n",
      "Ganhos Totais: 35232.75, Perdas Totais: -34662.25\n",
      "Episode 97/100, Total Reward: -192.00, Win Rate: 0.56, Wins: 1039, Losses: 819, Epsilon: 0.1886, Steps: 36754, Time: 133.42s\n",
      "Ações: Manter=8059, Comprar=13511, Vender=15184\n",
      "Ganhos Totais: 34602.75, Perdas Totais: -34794.75\n",
      "Episode 98/100, Total Reward: 2925.75, Win Rate: 0.57, Wins: 1015, Losses: 780, Epsilon: 0.1867, Steps: 36754, Time: 132.84s\n",
      "Ações: Manter=7821, Comprar=13898, Vender=15035\n",
      "Ganhos Totais: 35398.75, Perdas Totais: -32473.00\n",
      "Modelo e log do episódio 98 salvos em: 4.7.7\\model_episode_98.pth e 4.7.7\\log_episode_98.csv\n",
      "\n",
      "Episode 99/100, Total Reward: 839.75, Win Rate: 0.56, Wins: 1058, Losses: 838, Epsilon: 0.1849, Steps: 36754, Time: 133.95s\n",
      "Ações: Manter=6863, Comprar=15941, Vender=13950\n",
      "Ganhos Totais: 35929.25, Perdas Totais: -35089.50\n",
      "Episode 100/100, Total Reward: 2125.25, Win Rate: 0.54, Wins: 1018, Losses: 859, Epsilon: 0.1830, Steps: 36754, Time: 133.09s\n",
      "Ações: Manter=7982, Comprar=14962, Vender=13810\n",
      "Ganhos Totais: 36413.50, Perdas Totais: -34288.25\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 11, Total Reward: 5426.75, Win Rate: 0.56, Wins: 1565, Losses: 1220, Ações: {0: 11412, 1: 12872, 2: 12470}, Steps: 36754, Time: 182.76s\n",
      "Rank 2: Episode 59, Total Reward: 4702.75, Win Rate: 0.57, Wins: 1282, Losses: 974, Ações: {0: 11774, 1: 12693, 2: 12287}, Steps: 36754, Time: 120.68s\n",
      "Rank 3: Episode 66, Total Reward: 4569.50, Win Rate: 0.56, Wins: 1184, Losses: 921, Ações: {0: 7361, 1: 17551, 2: 11842}, Steps: 36754, Time: 137.90s\n",
      "Rank 4: Episode 62, Total Reward: 4035.00, Win Rate: 0.55, Wins: 1234, Losses: 1009, Ações: {0: 10854, 1: 13096, 2: 12804}, Steps: 36754, Time: 139.72s\n",
      "Rank 5: Episode 10, Total Reward: 3629.75, Win Rate: 0.55, Wins: 1461, Losses: 1183, Ações: {0: 12586, 1: 12332, 2: 11836}, Steps: 36754, Time: 185.85s\n",
      "Rank 6: Episode 72, Total Reward: 3325.25, Win Rate: 0.57, Wins: 1100, Losses: 843, Ações: {0: 8446, 1: 17425, 2: 10883}, Steps: 36754, Time: 137.26s\n",
      "Rank 7: Episode 70, Total Reward: 3212.50, Win Rate: 0.56, Wins: 1157, Losses: 898, Ações: {0: 8548, 1: 15308, 2: 12898}, Steps: 36754, Time: 136.95s\n",
      "Rank 8: Episode 98, Total Reward: 2925.75, Win Rate: 0.57, Wins: 1015, Losses: 780, Ações: {0: 7821, 1: 13898, 2: 15035}, Steps: 36754, Time: 132.84s\n",
      "Rank 9: Episode 75, Total Reward: 2917.00, Win Rate: 0.55, Wins: 1020, Losses: 825, Ações: {0: 12010, 1: 12026, 2: 12718}, Steps: 36754, Time: 137.71s\n",
      "Rank 10: Episode 77, Total Reward: 2891.25, Win Rate: 0.55, Wins: 1004, Losses: 820, Ações: {0: 10131, 1: 16144, 2: 10479}, Steps: 36754, Time: 136.88s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.7\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
