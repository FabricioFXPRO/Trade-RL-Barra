{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range', 'SMA4', 'SMA8', 'SMA12', 'SMA20', 'SMA50', 'SMA100', 'SMA200',\n",
    "    'StochasticoK', 'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram',\n",
    "    'atr8', 'atr14', 'atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass\n",
    "        else:\n",
    "            # Fechar posição se o gatilho não estiver ativo\n",
    "            if self.position == 1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 3: Criar o Agente Rainbow DQN usando PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Implementação da Camada Noisy Linear\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma_init = sigma_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.zeros(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.zeros(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return nn.functional.linear(x, weight, bias)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt())\n",
    "\n",
    "# Implementação da Rede Rainbow DQN\n",
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(obs_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Advantage stream\n",
    "        self.advantage = nn.Sequential(\n",
    "            NoisyLinear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(256, n_actions)\n",
    "        )\n",
    "        # Value stream\n",
    "        self.value = nn.Sequential(\n",
    "            NoisyLinear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "        # Dueling Q-values\n",
    "        q_values = value + advantage - advantage.mean()\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()\n",
    "\n",
    "# Implementação do Prioritized Replay Buffer\n",
    "class PrioritizedReplayBuffer(object):\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "\n",
    "    def push(self, *args):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((*args,))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (*args,)\n",
    "\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        batch = list(zip(*samples))\n",
    "\n",
    "        states = torch.cat(batch[0]).to(device)\n",
    "        actions = torch.tensor(batch[1], dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(batch[2], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        next_states = torch.cat(batch[3]).to(device)\n",
    "        dones = torch.tensor(batch[4], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloco 4: Treinamento do Agente Rainbow DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -1645.75, Win Rate: 0.49, Wins: 200, Losses: 212, Steps: 36754, Time: 358.04s\n",
      "Ações: Manter=12344, Comprar=2052, Vender=22358\n",
      "Ganhos Totais: 19422.25, Perdas Totais: -20965.00\n",
      "Modelo e log do episódio 1 salvos em: 4.17.2\\model_episode_1.pth e 4.17.2\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -299.75, Win Rate: 0.49, Wins: 206, Losses: 212, Steps: 36754, Time: 422.13s\n",
      "Ações: Manter=23101, Comprar=4517, Vender=9136\n",
      "Ganhos Totais: 20231.00, Perdas Totais: -20426.25\n",
      "Modelo e log do episódio 2 salvos em: 4.17.2\\model_episode_2.pth e 4.17.2\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: 15.00, Win Rate: 0.49, Wins: 198, Losses: 209, Steps: 36754, Time: 441.87s\n",
      "Ações: Manter=23043, Comprar=4512, Vender=9199\n",
      "Ganhos Totais: 19590.75, Perdas Totais: -19473.75\n",
      "Modelo e log do episódio 3 salvos em: 4.17.2\\model_episode_3.pth e 4.17.2\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -993.75, Win Rate: 0.50, Wins: 213, Losses: 214, Steps: 36754, Time: 457.58s\n",
      "Ações: Manter=16897, Comprar=4511, Vender=15346\n",
      "Ganhos Totais: 21142.50, Perdas Totais: -22029.25\n",
      "Modelo e log do episódio 4 salvos em: 4.17.2\\model_episode_4.pth e 4.17.2\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -1068.25, Win Rate: 0.49, Wins: 210, Losses: 222, Steps: 36754, Time: 469.28s\n",
      "Ações: Manter=17226, Comprar=4527, Vender=15001\n",
      "Ganhos Totais: 21058.50, Perdas Totais: -22018.50\n",
      "Modelo e log do episódio 5 salvos em: 4.17.2\\model_episode_5.pth e 4.17.2\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -879.00, Win Rate: 0.50, Wins: 218, Losses: 219, Steps: 36754, Time: 465.50s\n",
      "Ações: Manter=14704, Comprar=4528, Vender=17522\n",
      "Ganhos Totais: 21469.00, Perdas Totais: -22238.75\n",
      "Modelo e log do episódio 6 salvos em: 4.17.2\\model_episode_6.pth e 4.17.2\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -2649.00, Win Rate: 0.53, Wins: 420, Losses: 369, Steps: 36754, Time: 465.35s\n",
      "Ações: Manter=15395, Comprar=8768, Vender=12591\n",
      "Ganhos Totais: 24205.00, Perdas Totais: -26656.25\n",
      "Modelo e log do episódio 7 salvos em: 4.17.2\\model_episode_7.pth e 4.17.2\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -3355.50, Win Rate: 0.54, Wins: 462, Losses: 401, Steps: 36754, Time: 463.06s\n",
      "Ações: Manter=15142, Comprar=9307, Vender=12305\n",
      "Ganhos Totais: 24956.75, Perdas Totais: -28095.50\n",
      "Modelo e log do episódio 8 salvos em: 4.17.2\\model_episode_8.pth e 4.17.2\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: -3639.50, Win Rate: 0.55, Wins: 556, Losses: 461, Steps: 36754, Time: 469.60s\n",
      "Ações: Manter=14897, Comprar=9006, Vender=12851\n",
      "Ganhos Totais: 24520.50, Perdas Totais: -27904.50\n",
      "Modelo e log do episódio 9 salvos em: 4.17.2\\model_episode_9.pth e 4.17.2\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: -7433.00, Win Rate: 0.49, Wins: 655, Losses: 670, Steps: 36754, Time: 486.56s\n",
      "Ações: Manter=11912, Comprar=7088, Vender=17754\n",
      "Ganhos Totais: 24039.25, Perdas Totais: -31140.50\n",
      "Modelo e log do episódio 10 salvos em: 4.17.2\\model_episode_10.pth e 4.17.2\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -3196.75, Win Rate: 0.53, Wins: 716, Losses: 640, Steps: 36754, Time: 489.05s\n",
      "Ações: Manter=12568, Comprar=8004, Vender=16182\n",
      "Ganhos Totais: 27107.75, Perdas Totais: -29964.25\n",
      "Modelo e log do episódio 11 salvos em: 4.17.2\\model_episode_11.pth e 4.17.2\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -3738.50, Win Rate: 0.54, Wins: 695, Losses: 603, Steps: 36754, Time: 491.20s\n",
      "Ações: Manter=12023, Comprar=5303, Vender=19428\n",
      "Ganhos Totais: 25909.00, Perdas Totais: -29321.50\n",
      "Episode 13/100, Total Reward: -7131.50, Win Rate: 0.52, Wins: 707, Losses: 642, Steps: 36754, Time: 491.34s\n",
      "Ações: Manter=12411, Comprar=7284, Vender=17059\n",
      "Ganhos Totais: 24986.25, Perdas Totais: -31779.25\n",
      "Episode 14/100, Total Reward: -8237.50, Win Rate: 0.54, Wins: 581, Losses: 487, Steps: 36754, Time: 483.64s\n",
      "Ações: Manter=15983, Comprar=6124, Vender=14647\n",
      "Ganhos Totais: 22301.00, Perdas Totais: -30270.75\n",
      "Episode 15/100, Total Reward: -6049.50, Win Rate: 0.53, Wins: 669, Losses: 587, Steps: 36754, Time: 442.39s\n",
      "Ações: Manter=11377, Comprar=4491, Vender=20886\n",
      "Ganhos Totais: 24425.25, Perdas Totais: -30160.00\n",
      "Episode 16/100, Total Reward: -8845.25, Win Rate: 0.55, Wins: 801, Losses: 664, Steps: 36754, Time: 440.50s\n",
      "Ações: Manter=11284, Comprar=7225, Vender=18245\n",
      "Ganhos Totais: 24546.00, Perdas Totais: -33024.50\n",
      "Episode 17/100, Total Reward: -4169.50, Win Rate: 0.54, Wins: 884, Losses: 755, Steps: 36754, Time: 437.42s\n",
      "Ações: Manter=14901, Comprar=6310, Vender=15543\n",
      "Ganhos Totais: 26512.75, Perdas Totais: -30271.00\n",
      "Episode 18/100, Total Reward: -5912.50, Win Rate: 0.55, Wins: 501, Losses: 405, Steps: 36754, Time: 445.06s\n",
      "Ações: Manter=17975, Comprar=1184, Vender=17595\n",
      "Ganhos Totais: 22467.75, Perdas Totais: -28153.50\n",
      "Episode 19/100, Total Reward: -5766.75, Win Rate: 0.55, Wins: 488, Losses: 402, Steps: 36754, Time: 431.21s\n",
      "Ações: Manter=12023, Comprar=904, Vender=23827\n",
      "Ganhos Totais: 23279.00, Perdas Totais: -28822.75\n",
      "Episode 20/100, Total Reward: -4724.25, Win Rate: 0.55, Wins: 983, Losses: 811, Steps: 36754, Time: 390.60s\n",
      "Ações: Manter=11157, Comprar=2502, Vender=23095\n",
      "Ganhos Totais: 28040.50, Perdas Totais: -32315.00\n",
      "Episode 21/100, Total Reward: -4387.00, Win Rate: 0.56, Wins: 830, Losses: 664, Steps: 36754, Time: 392.03s\n",
      "Ações: Manter=12427, Comprar=2383, Vender=21944\n",
      "Ganhos Totais: 27958.50, Perdas Totais: -31969.00\n",
      "Episode 22/100, Total Reward: -4369.00, Win Rate: 0.55, Wins: 880, Losses: 718, Steps: 36754, Time: 401.43s\n",
      "Ações: Manter=11588, Comprar=1960, Vender=23206\n",
      "Ganhos Totais: 28405.00, Perdas Totais: -32371.75\n",
      "Episode 23/100, Total Reward: -3077.50, Win Rate: 0.57, Wins: 751, Losses: 577, Steps: 36754, Time: 378.58s\n",
      "Ações: Manter=12123, Comprar=1417, Vender=23214\n",
      "Ganhos Totais: 27135.50, Perdas Totais: -29879.75\n",
      "Modelo e log do episódio 23 salvos em: 4.17.2\\model_episode_23.pth e 4.17.2\\log_episode_23.csv\n",
      "\n",
      "Episode 24/100, Total Reward: -3525.25, Win Rate: 0.57, Wins: 771, Losses: 590, Steps: 36754, Time: 375.41s\n",
      "Ações: Manter=9916, Comprar=2644, Vender=24194\n",
      "Ganhos Totais: 27055.75, Perdas Totais: -30239.25\n",
      "Episode 25/100, Total Reward: -1505.25, Win Rate: 0.54, Wins: 755, Losses: 632, Steps: 36754, Time: 374.92s\n",
      "Ações: Manter=11643, Comprar=3857, Vender=21254\n",
      "Ganhos Totais: 27611.50, Perdas Totais: -28769.00\n",
      "Modelo e log do episódio 25 salvos em: 4.17.2\\model_episode_25.pth e 4.17.2\\log_episode_25.csv\n",
      "\n",
      "Episode 26/100, Total Reward: -1358.50, Win Rate: 0.56, Wins: 780, Losses: 616, Steps: 36754, Time: 377.00s\n",
      "Ações: Manter=11165, Comprar=2662, Vender=22927\n",
      "Ganhos Totais: 28624.00, Perdas Totais: -29631.75\n",
      "Modelo e log do episódio 26 salvos em: 4.17.2\\model_episode_26.pth e 4.17.2\\log_episode_26.csv\n",
      "\n",
      "Episode 27/100, Total Reward: 1216.00, Win Rate: 0.58, Wins: 887, Losses: 642, Steps: 36754, Time: 376.41s\n",
      "Ações: Manter=15811, Comprar=3507, Vender=17436\n",
      "Ganhos Totais: 27901.00, Perdas Totais: -26302.25\n",
      "Modelo e log do episódio 27 salvos em: 4.17.2\\model_episode_27.pth e 4.17.2\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: -3288.50, Win Rate: 0.56, Wins: 625, Losses: 484, Steps: 36754, Time: 375.59s\n",
      "Ações: Manter=17625, Comprar=1107, Vender=18022\n",
      "Ganhos Totais: 24050.00, Perdas Totais: -27059.75\n",
      "Episode 29/100, Total Reward: -776.75, Win Rate: 0.59, Wins: 660, Losses: 467, Steps: 36754, Time: 374.26s\n",
      "Ações: Manter=15943, Comprar=1074, Vender=19737\n",
      "Ganhos Totais: 24604.50, Perdas Totais: -25098.75\n",
      "Modelo e log do episódio 29 salvos em: 4.17.2\\model_episode_29.pth e 4.17.2\\log_episode_29.csv\n",
      "\n",
      "Episode 30/100, Total Reward: -1418.75, Win Rate: 0.58, Wins: 644, Losses: 469, Steps: 36754, Time: 373.95s\n",
      "Ações: Manter=16547, Comprar=1047, Vender=19160\n",
      "Ganhos Totais: 25177.00, Perdas Totais: -26317.25\n",
      "Modelo e log do episódio 30 salvos em: 4.17.2\\model_episode_30.pth e 4.17.2\\log_episode_30.csv\n",
      "\n",
      "Episode 31/100, Total Reward: -1256.25, Win Rate: 0.58, Wins: 709, Losses: 524, Steps: 36754, Time: 373.45s\n",
      "Ações: Manter=14736, Comprar=1362, Vender=20656\n",
      "Ganhos Totais: 25885.25, Perdas Totais: -26831.75\n",
      "Modelo e log do episódio 31 salvos em: 4.17.2\\model_episode_31.pth e 4.17.2\\log_episode_31.csv\n",
      "\n",
      "Episode 32/100, Total Reward: 803.25, Win Rate: 0.56, Wins: 927, Losses: 727, Steps: 36754, Time: 374.62s\n",
      "Ações: Manter=19585, Comprar=2258, Vender=14911\n",
      "Ganhos Totais: 28636.75, Perdas Totais: -27418.00\n",
      "Modelo e log do episódio 32 salvos em: 4.17.2\\model_episode_32.pth e 4.17.2\\log_episode_32.csv\n",
      "\n",
      "Episode 33/100, Total Reward: 364.00, Win Rate: 0.58, Wins: 624, Losses: 443, Steps: 36754, Time: 375.29s\n",
      "Ações: Manter=25319, Comprar=1032, Vender=10403\n",
      "Ganhos Totais: 24548.00, Perdas Totais: -23916.75\n",
      "Modelo e log do episódio 33 salvos em: 4.17.2\\model_episode_33.pth e 4.17.2\\log_episode_33.csv\n",
      "\n",
      "Episode 34/100, Total Reward: -932.00, Win Rate: 0.58, Wins: 503, Losses: 364, Steps: 36754, Time: 375.69s\n",
      "Ações: Manter=23739, Comprar=783, Vender=12232\n",
      "Ganhos Totais: 21632.75, Perdas Totais: -22347.50\n",
      "Modelo e log do episódio 34 salvos em: 4.17.2\\model_episode_34.pth e 4.17.2\\log_episode_34.csv\n",
      "\n",
      "Episode 35/100, Total Reward: -1941.75, Win Rate: 0.55, Wins: 496, Losses: 402, Steps: 36754, Time: 375.45s\n",
      "Ações: Manter=19979, Comprar=706, Vender=16069\n",
      "Ganhos Totais: 23680.75, Perdas Totais: -25397.75\n",
      "Episode 36/100, Total Reward: -1187.25, Win Rate: 0.59, Wins: 737, Losses: 517, Steps: 36754, Time: 376.42s\n",
      "Ações: Manter=14580, Comprar=1170, Vender=21004\n",
      "Ganhos Totais: 26348.25, Perdas Totais: -27220.00\n",
      "Episode 37/100, Total Reward: -2670.25, Win Rate: 0.58, Wins: 662, Losses: 477, Steps: 36754, Time: 375.62s\n",
      "Ações: Manter=18365, Comprar=976, Vender=17413\n",
      "Ganhos Totais: 23916.75, Perdas Totais: -26300.25\n",
      "Episode 38/100, Total Reward: -4699.50, Win Rate: 0.59, Wins: 748, Losses: 524, Steps: 36754, Time: 375.68s\n",
      "Ações: Manter=12575, Comprar=1069, Vender=23110\n",
      "Ganhos Totais: 26848.75, Perdas Totais: -31229.75\n",
      "Episode 39/100, Total Reward: -1421.50, Win Rate: 0.55, Wins: 602, Losses: 496, Steps: 36754, Time: 376.33s\n",
      "Ações: Manter=19544, Comprar=972, Vender=16238\n",
      "Ganhos Totais: 25701.25, Perdas Totais: -26847.75\n",
      "Episode 40/100, Total Reward: -2506.50, Win Rate: 0.56, Wins: 603, Losses: 474, Steps: 36754, Time: 375.42s\n",
      "Ações: Manter=21167, Comprar=903, Vender=14684\n",
      "Ganhos Totais: 24285.75, Perdas Totais: -26522.25\n",
      "Episode 41/100, Total Reward: -1623.25, Win Rate: 0.58, Wins: 691, Losses: 507, Steps: 36754, Time: 377.35s\n",
      "Ações: Manter=21320, Comprar=1404, Vender=14030\n",
      "Ganhos Totais: 24106.75, Perdas Totais: -25430.00\n",
      "Episode 42/100, Total Reward: -1258.25, Win Rate: 0.57, Wins: 641, Losses: 490, Steps: 36754, Time: 397.65s\n",
      "Ações: Manter=18694, Comprar=1431, Vender=16629\n",
      "Ganhos Totais: 25074.00, Perdas Totais: -26049.00\n",
      "Episode 43/100, Total Reward: 972.25, Win Rate: 0.58, Wins: 635, Losses: 453, Steps: 36754, Time: 409.80s\n",
      "Ações: Manter=20850, Comprar=2807, Vender=13097\n",
      "Ganhos Totais: 26391.25, Perdas Totais: -25146.50\n",
      "Modelo e log do episódio 43 salvos em: 4.17.2\\model_episode_43.pth e 4.17.2\\log_episode_43.csv\n",
      "\n",
      "Episode 44/100, Total Reward: 1576.75, Win Rate: 0.59, Wins: 710, Losses: 488, Steps: 36754, Time: 399.97s\n",
      "Ações: Manter=20264, Comprar=3792, Vender=12698\n",
      "Ganhos Totais: 26895.00, Perdas Totais: -25018.00\n",
      "Modelo e log do episódio 44 salvos em: 4.17.2\\model_episode_44.pth e 4.17.2\\log_episode_44.csv\n",
      "\n",
      "Episode 45/100, Total Reward: 3494.25, Win Rate: 0.57, Wins: 852, Losses: 634, Steps: 36754, Time: 396.52s\n",
      "Ações: Manter=17259, Comprar=4488, Vender=15007\n",
      "Ganhos Totais: 29511.25, Perdas Totais: -25644.50\n",
      "Modelo e log do episódio 45 salvos em: 4.17.2\\model_episode_45.pth e 4.17.2\\log_episode_45.csv\n",
      "\n",
      "Episode 46/100, Total Reward: 4755.00, Win Rate: 0.57, Wins: 805, Losses: 595, Steps: 36754, Time: 401.98s\n",
      "Ações: Manter=21765, Comprar=3142, Vender=11847\n",
      "Ganhos Totais: 28438.75, Perdas Totais: -23332.75\n",
      "Modelo e log do episódio 46 salvos em: 4.17.2\\model_episode_46.pth e 4.17.2\\log_episode_46.csv\n",
      "\n",
      "Episode 47/100, Total Reward: 2063.50, Win Rate: 0.58, Wins: 605, Losses: 430, Steps: 36754, Time: 407.10s\n",
      "Ações: Manter=23633, Comprar=2177, Vender=10944\n",
      "Ganhos Totais: 25085.50, Perdas Totais: -22762.50\n",
      "Modelo e log do episódio 47 salvos em: 4.17.2\\model_episode_47.pth e 4.17.2\\log_episode_47.csv\n",
      "\n",
      "Episode 48/100, Total Reward: 101.75, Win Rate: 0.55, Wins: 634, Losses: 515, Steps: 36754, Time: 402.91s\n",
      "Ações: Manter=20779, Comprar=3810, Vender=12165\n",
      "Ganhos Totais: 26188.00, Perdas Totais: -25798.50\n",
      "Modelo e log do episódio 48 salvos em: 4.17.2\\model_episode_48.pth e 4.17.2\\log_episode_48.csv\n",
      "\n",
      "Episode 49/100, Total Reward: -1771.25, Win Rate: 0.56, Wins: 562, Losses: 449, Steps: 36754, Time: 401.52s\n",
      "Ações: Manter=22283, Comprar=4722, Vender=9749\n",
      "Ganhos Totais: 22567.50, Perdas Totais: -24085.25\n",
      "Episode 50/100, Total Reward: 2010.75, Win Rate: 0.56, Wins: 393, Losses: 309, Steps: 36754, Time: 397.81s\n",
      "Ações: Manter=23309, Comprar=7619, Vender=5826\n",
      "Ganhos Totais: 22377.50, Perdas Totais: -20191.25\n",
      "Modelo e log do episódio 50 salvos em: 4.17.2\\model_episode_50.pth e 4.17.2\\log_episode_50.csv\n",
      "\n",
      "Episode 51/100, Total Reward: 3032.25, Win Rate: 0.58, Wins: 429, Losses: 309, Steps: 36754, Time: 408.58s\n",
      "Ações: Manter=21899, Comprar=11831, Vender=3024\n",
      "Ganhos Totais: 24149.00, Perdas Totais: -20932.25\n",
      "Modelo e log do episódio 51 salvos em: 4.17.2\\model_episode_51.pth e 4.17.2\\log_episode_51.csv\n",
      "\n",
      "Episode 52/100, Total Reward: 3148.75, Win Rate: 0.61, Wins: 588, Losses: 376, Steps: 36754, Time: 416.53s\n",
      "Ações: Manter=15506, Comprar=19229, Vender=2019\n",
      "Ganhos Totais: 25652.75, Perdas Totais: -22262.75\n",
      "Modelo e log do episódio 52 salvos em: 4.17.2\\model_episode_52.pth e 4.17.2\\log_episode_52.csv\n",
      "\n",
      "Episode 53/100, Total Reward: 1799.75, Win Rate: 0.59, Wins: 535, Losses: 373, Steps: 36754, Time: 398.21s\n",
      "Ações: Manter=26219, Comprar=7914, Vender=2621\n",
      "Ganhos Totais: 20581.00, Perdas Totais: -18553.75\n",
      "Modelo e log do episódio 53 salvos em: 4.17.2\\model_episode_53.pth e 4.17.2\\log_episode_53.csv\n",
      "\n",
      "Episode 54/100, Total Reward: 2522.50, Win Rate: 0.59, Wins: 528, Losses: 365, Steps: 36754, Time: 350.37s\n",
      "Ações: Manter=19731, Comprar=14756, Vender=2267\n",
      "Ganhos Totais: 22263.00, Perdas Totais: -19516.75\n",
      "Modelo e log do episódio 54 salvos em: 4.17.2\\model_episode_54.pth e 4.17.2\\log_episode_54.csv\n",
      "\n",
      "Episode 55/100, Total Reward: 1093.00, Win Rate: 0.58, Wins: 519, Losses: 379, Steps: 36754, Time: 347.96s\n",
      "Ações: Manter=17308, Comprar=18132, Vender=1314\n",
      "Ganhos Totais: 23349.25, Perdas Totais: -22031.50\n",
      "Episode 56/100, Total Reward: 3460.50, Win Rate: 0.57, Wins: 412, Losses: 312, Steps: 36754, Time: 347.56s\n",
      "Ações: Manter=21987, Comprar=13808, Vender=959\n",
      "Ganhos Totais: 22014.75, Perdas Totais: -18373.00\n",
      "Modelo e log do episódio 56 salvos em: 4.17.2\\model_episode_56.pth e 4.17.2\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: 2067.50, Win Rate: 0.59, Wins: 516, Losses: 360, Steps: 36754, Time: 352.00s\n",
      "Ações: Manter=11447, Comprar=21934, Vender=3373\n",
      "Ganhos Totais: 24942.50, Perdas Totais: -22655.75\n",
      "Modelo e log do episódio 57 salvos em: 4.17.2\\model_episode_57.pth e 4.17.2\\log_episode_57.csv\n",
      "\n",
      "Episode 58/100, Total Reward: 4702.50, Win Rate: 0.57, Wins: 767, Losses: 587, Steps: 36754, Time: 440.34s\n",
      "Ações: Manter=14580, Comprar=19618, Vender=2556\n",
      "Ganhos Totais: 28042.75, Perdas Totais: -22999.75\n",
      "Modelo e log do episódio 58 salvos em: 4.17.2\\model_episode_58.pth e 4.17.2\\log_episode_58.csv\n",
      "\n",
      "Episode 59/100, Total Reward: 2747.00, Win Rate: 0.55, Wins: 873, Losses: 705, Steps: 36754, Time: 411.70s\n",
      "Ações: Manter=10505, Comprar=17461, Vender=8788\n",
      "Ganhos Totais: 29763.00, Perdas Totais: -26618.50\n",
      "Modelo e log do episódio 59 salvos em: 4.17.2\\model_episode_59.pth e 4.17.2\\log_episode_59.csv\n",
      "\n",
      "Episode 60/100, Total Reward: 7627.25, Win Rate: 0.59, Wins: 638, Losses: 445, Steps: 36754, Time: 412.33s\n",
      "Ações: Manter=14482, Comprar=14510, Vender=7762\n",
      "Ganhos Totais: 29101.00, Perdas Totais: -21200.50\n",
      "Modelo e log do episódio 60 salvos em: 4.17.2\\model_episode_60.pth e 4.17.2\\log_episode_60.csv\n",
      "\n",
      "Episode 61/100, Total Reward: 3605.75, Win Rate: 0.55, Wins: 675, Losses: 546, Steps: 36754, Time: 405.06s\n",
      "Ações: Manter=8686, Comprar=10942, Vender=17126\n",
      "Ganhos Totais: 31020.25, Perdas Totais: -27108.75\n",
      "Modelo e log do episódio 61 salvos em: 4.17.2\\model_episode_61.pth e 4.17.2\\log_episode_61.csv\n",
      "\n",
      "Episode 62/100, Total Reward: -1595.75, Win Rate: 0.54, Wins: 576, Losses: 485, Steps: 36754, Time: 406.40s\n",
      "Ações: Manter=14397, Comprar=18650, Vender=3707\n",
      "Ganhos Totais: 26076.00, Perdas Totais: -27405.25\n",
      "Episode 63/100, Total Reward: 1951.50, Win Rate: 0.59, Wins: 664, Losses: 464, Steps: 36754, Time: 430.38s\n",
      "Ações: Manter=7144, Comprar=26576, Vender=3034\n",
      "Ganhos Totais: 29845.25, Perdas Totais: -27610.25\n",
      "Episode 64/100, Total Reward: 4137.50, Win Rate: 0.60, Wins: 488, Losses: 332, Steps: 36754, Time: 414.85s\n",
      "Ações: Manter=17796, Comprar=17014, Vender=1944\n",
      "Ganhos Totais: 25898.25, Perdas Totais: -21555.00\n",
      "Modelo e log do episódio 64 salvos em: 4.17.2\\model_episode_64.pth e 4.17.2\\log_episode_64.csv\n",
      "\n",
      "Episode 65/100, Total Reward: 5133.75, Win Rate: 0.57, Wins: 478, Losses: 364, Steps: 36754, Time: 429.88s\n",
      "Ações: Manter=15730, Comprar=15891, Vender=5133\n",
      "Ganhos Totais: 26783.75, Perdas Totais: -21439.25\n",
      "Modelo e log do episódio 65 salvos em: 4.17.2\\model_episode_65.pth e 4.17.2\\log_episode_65.csv\n",
      "\n",
      "Episode 66/100, Total Reward: -691.00, Win Rate: 0.54, Wins: 389, Losses: 336, Steps: 36754, Time: 423.25s\n",
      "Ações: Manter=12447, Comprar=15968, Vender=8339\n",
      "Ganhos Totais: 24471.25, Perdas Totais: -24981.00\n",
      "Episode 67/100, Total Reward: 3152.00, Win Rate: 0.53, Wins: 546, Losses: 483, Steps: 36754, Time: 445.80s\n",
      "Ações: Manter=8664, Comprar=22557, Vender=5533\n",
      "Ganhos Totais: 28788.75, Perdas Totais: -25379.25\n",
      "Modelo e log do episódio 67 salvos em: 4.17.2\\model_episode_67.pth e 4.17.2\\log_episode_67.csv\n",
      "\n",
      "Episode 68/100, Total Reward: 2175.50, Win Rate: 0.54, Wins: 361, Losses: 305, Steps: 36754, Time: 415.35s\n",
      "Ações: Manter=11709, Comprar=21720, Vender=3325\n",
      "Ganhos Totais: 25139.25, Perdas Totais: -22797.00\n",
      "Episode 69/100, Total Reward: 1660.50, Win Rate: 0.55, Wins: 375, Losses: 312, Steps: 36754, Time: 411.93s\n",
      "Ações: Manter=14527, Comprar=17803, Vender=4424\n",
      "Ganhos Totais: 24989.75, Perdas Totais: -23157.50\n",
      "Episode 70/100, Total Reward: 432.25, Win Rate: 0.53, Wins: 300, Losses: 265, Steps: 36754, Time: 428.62s\n",
      "Ações: Manter=20506, Comprar=12887, Vender=3361\n",
      "Ganhos Totais: 20320.75, Perdas Totais: -19747.00\n",
      "Episode 71/100, Total Reward: 3588.50, Win Rate: 0.57, Wins: 402, Losses: 305, Steps: 36754, Time: 429.44s\n",
      "Ações: Manter=9786, Comprar=23737, Vender=3231\n",
      "Ganhos Totais: 26410.75, Perdas Totais: -22645.50\n",
      "Modelo e log do episódio 71 salvos em: 4.17.2\\model_episode_71.pth e 4.17.2\\log_episode_71.csv\n",
      "\n",
      "Episode 72/100, Total Reward: 4371.75, Win Rate: 0.54, Wins: 391, Losses: 331, Steps: 36754, Time: 432.91s\n",
      "Ações: Manter=25142, Comprar=5333, Vender=6279\n",
      "Ganhos Totais: 21782.25, Perdas Totais: -17229.50\n",
      "Modelo e log do episódio 72 salvos em: 4.17.2\\model_episode_72.pth e 4.17.2\\log_episode_72.csv\n",
      "\n",
      "Episode 73/100, Total Reward: 2437.25, Win Rate: 0.53, Wins: 313, Losses: 274, Steps: 36754, Time: 429.95s\n",
      "Ações: Manter=25062, Comprar=3996, Vender=7696\n",
      "Ganhos Totais: 18284.75, Perdas Totais: -15700.75\n",
      "Episode 74/100, Total Reward: 3333.50, Win Rate: 0.55, Wins: 280, Losses: 226, Steps: 36754, Time: 430.43s\n",
      "Ações: Manter=27837, Comprar=2952, Vender=5965\n",
      "Ganhos Totais: 17213.25, Perdas Totais: -13753.00\n",
      "Episode 75/100, Total Reward: 3927.25, Win Rate: 0.55, Wins: 437, Losses: 362, Steps: 36754, Time: 430.47s\n",
      "Ações: Manter=18646, Comprar=5438, Vender=12670\n",
      "Ganhos Totais: 23111.75, Perdas Totais: -18983.25\n",
      "Modelo e log do episódio 75 salvos em: 4.17.2\\model_episode_75.pth e 4.17.2\\log_episode_75.csv\n",
      "\n",
      "Episode 76/100, Total Reward: 2828.50, Win Rate: 0.54, Wins: 328, Losses: 275, Steps: 36754, Time: 430.22s\n",
      "Ações: Manter=25089, Comprar=2592, Vender=9073\n",
      "Ganhos Totais: 18747.25, Perdas Totais: -15767.25\n",
      "Episode 77/100, Total Reward: 2800.50, Win Rate: 0.53, Wins: 498, Losses: 437, Steps: 36754, Time: 526.15s\n",
      "Ações: Manter=7072, Comprar=23583, Vender=6099\n",
      "Ganhos Totais: 27515.25, Perdas Totais: -24479.75\n",
      "Episode 78/100, Total Reward: 3615.50, Win Rate: 0.54, Wins: 400, Losses: 336, Steps: 36754, Time: 528.25s\n",
      "Ações: Manter=4386, Comprar=30700, Vender=1668\n",
      "Ganhos Totais: 26156.75, Perdas Totais: -22357.25\n",
      "Modelo e log do episódio 78 salvos em: 4.17.2\\model_episode_78.pth e 4.17.2\\log_episode_78.csv\n",
      "\n",
      "Episode 79/100, Total Reward: 3512.00, Win Rate: 0.56, Wins: 356, Losses: 282, Steps: 36754, Time: 525.87s\n",
      "Ações: Manter=6861, Comprar=27673, Vender=2220\n",
      "Ganhos Totais: 24254.50, Perdas Totais: -20582.75\n",
      "Episode 80/100, Total Reward: 3854.00, Win Rate: 0.58, Wins: 351, Losses: 253, Steps: 36754, Time: 500.06s\n",
      "Ações: Manter=12794, Comprar=22049, Vender=1911\n",
      "Ganhos Totais: 22411.75, Perdas Totais: -18405.75\n",
      "Modelo e log do episódio 80 salvos em: 4.17.2\\model_episode_80.pth e 4.17.2\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: 4762.00, Win Rate: 0.58, Wins: 340, Losses: 249, Steps: 36754, Time: 445.65s\n",
      "Ações: Manter=18325, Comprar=16838, Vender=1591\n",
      "Ganhos Totais: 21773.00, Perdas Totais: -16863.50\n",
      "Modelo e log do episódio 81 salvos em: 4.17.2\\model_episode_81.pth e 4.17.2\\log_episode_81.csv\n",
      "\n",
      "Episode 82/100, Total Reward: 5761.25, Win Rate: 0.58, Wins: 409, Losses: 301, Steps: 36754, Time: 406.40s\n",
      "Ações: Manter=2823, Comprar=32199, Vender=1732\n",
      "Ganhos Totais: 26608.25, Perdas Totais: -20669.25\n",
      "Modelo e log do episódio 82 salvos em: 4.17.2\\model_episode_82.pth e 4.17.2\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: 5781.75, Win Rate: 0.57, Wins: 377, Losses: 283, Steps: 36754, Time: 406.49s\n",
      "Ações: Manter=7162, Comprar=27972, Vender=1620\n",
      "Ganhos Totais: 24422.00, Perdas Totais: -18474.75\n",
      "Modelo e log do episódio 83 salvos em: 4.17.2\\model_episode_83.pth e 4.17.2\\log_episode_83.csv\n",
      "\n",
      "Episode 84/100, Total Reward: 3253.75, Win Rate: 0.57, Wins: 374, Losses: 286, Steps: 36754, Time: 401.96s\n",
      "Ações: Manter=3279, Comprar=31493, Vender=1982\n",
      "Ganhos Totais: 24541.50, Perdas Totais: -21122.50\n",
      "Episode 85/100, Total Reward: 2695.00, Win Rate: 0.51, Wins: 321, Losses: 314, Steps: 36754, Time: 396.39s\n",
      "Ações: Manter=5368, Comprar=29015, Vender=2371\n",
      "Ganhos Totais: 24646.75, Perdas Totais: -21792.75\n",
      "Episode 86/100, Total Reward: 1695.00, Win Rate: 0.54, Wins: 264, Losses: 223, Steps: 36754, Time: 388.24s\n",
      "Ações: Manter=6376, Comprar=28911, Vender=1467\n",
      "Ganhos Totais: 22309.50, Perdas Totais: -20492.50\n",
      "Episode 87/100, Total Reward: 3362.50, Win Rate: 0.51, Wins: 270, Losses: 258, Steps: 36754, Time: 387.21s\n",
      "Ações: Manter=11266, Comprar=24841, Vender=647\n",
      "Ganhos Totais: 22903.00, Perdas Totais: -19408.50\n",
      "Episode 88/100, Total Reward: 3361.25, Win Rate: 0.54, Wins: 239, Losses: 206, Steps: 36754, Time: 387.61s\n",
      "Ações: Manter=11654, Comprar=24683, Vender=417\n",
      "Ganhos Totais: 22222.25, Perdas Totais: -18749.75\n",
      "Episode 89/100, Total Reward: 6044.25, Win Rate: 0.50, Wins: 287, Losses: 289, Steps: 36754, Time: 386.96s\n",
      "Ações: Manter=17997, Comprar=17911, Vender=846\n",
      "Ganhos Totais: 23077.50, Perdas Totais: -16889.25\n",
      "Modelo e log do episódio 89 salvos em: 4.17.2\\model_episode_89.pth e 4.17.2\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: 4166.75, Win Rate: 0.53, Wins: 365, Losses: 330, Steps: 36754, Time: 387.26s\n",
      "Ações: Manter=9294, Comprar=26733, Vender=727\n",
      "Ganhos Totais: 24852.25, Perdas Totais: -20511.50\n",
      "Modelo e log do episódio 90 salvos em: 4.17.2\\model_episode_90.pth e 4.17.2\\log_episode_90.csv\n",
      "\n",
      "Episode 91/100, Total Reward: 2544.75, Win Rate: 0.53, Wins: 237, Losses: 213, Steps: 36754, Time: 388.62s\n",
      "Ações: Manter=7936, Comprar=25614, Vender=3204\n",
      "Ganhos Totais: 23392.75, Perdas Totais: -20735.50\n",
      "Episode 92/100, Total Reward: 2305.25, Win Rate: 0.54, Wins: 260, Losses: 219, Steps: 36754, Time: 387.56s\n",
      "Ações: Manter=6022, Comprar=28789, Vender=1943\n",
      "Ganhos Totais: 23499.50, Perdas Totais: -21074.25\n",
      "Episode 93/100, Total Reward: 2858.50, Win Rate: 0.51, Wins: 312, Losses: 303, Steps: 36754, Time: 387.59s\n",
      "Ações: Manter=7746, Comprar=27951, Vender=1057\n",
      "Ganhos Totais: 23343.00, Perdas Totais: -20330.50\n",
      "Episode 94/100, Total Reward: 2750.50, Win Rate: 0.55, Wins: 278, Losses: 231, Steps: 36754, Time: 388.38s\n",
      "Ações: Manter=10785, Comprar=25253, Vender=716\n",
      "Ganhos Totais: 20995.50, Perdas Totais: -18117.00\n",
      "Episode 95/100, Total Reward: 3406.75, Win Rate: 0.54, Wins: 285, Losses: 239, Steps: 36754, Time: 387.45s\n",
      "Ações: Manter=4568, Comprar=29276, Vender=2910\n",
      "Ganhos Totais: 24559.25, Perdas Totais: -21021.50\n",
      "Episode 96/100, Total Reward: 839.00, Win Rate: 0.55, Wins: 227, Losses: 184, Steps: 36754, Time: 387.74s\n",
      "Ações: Manter=24569, Comprar=9336, Vender=2849\n",
      "Ganhos Totais: 17213.75, Perdas Totais: -16271.75\n",
      "Episode 97/100, Total Reward: 462.00, Win Rate: 0.53, Wins: 197, Losses: 176, Steps: 36754, Time: 391.74s\n",
      "Ações: Manter=30647, Comprar=4570, Vender=1537\n",
      "Ganhos Totais: 14816.50, Perdas Totais: -14260.50\n",
      "Episode 98/100, Total Reward: 554.00, Win Rate: 0.53, Wins: 162, Losses: 146, Steps: 36754, Time: 391.11s\n",
      "Ações: Manter=32904, Comprar=3710, Vender=140\n",
      "Ganhos Totais: 13532.75, Perdas Totais: -12901.25\n",
      "Episode 99/100, Total Reward: 1005.00, Win Rate: 0.55, Wins: 79, Losses: 64, Steps: 36754, Time: 388.38s\n",
      "Ações: Manter=35590, Comprar=542, Vender=622\n",
      "Ganhos Totais: 4978.75, Perdas Totais: -3937.75\n",
      "Episode 100/100, Total Reward: -1096.25, Win Rate: 0.53, Wins: 122, Losses: 109, Steps: 36754, Time: 387.69s\n",
      "Ações: Manter=34532, Comprar=2160, Vender=62\n",
      "Ganhos Totais: 10789.25, Perdas Totais: -11827.25\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 60, Total Reward: 7627.25, Win Rate: 0.59, Wins: 638, Losses: 445, Ações: {0: 14482, 1: 14510, 2: 7762}, Steps: 36754, Time: 412.33s\n",
      "Rank 2: Episode 89, Total Reward: 6044.25, Win Rate: 0.50, Wins: 287, Losses: 289, Ações: {0: 17997, 1: 17911, 2: 846}, Steps: 36754, Time: 386.96s\n",
      "Rank 3: Episode 83, Total Reward: 5781.75, Win Rate: 0.57, Wins: 377, Losses: 283, Ações: {0: 7162, 1: 27972, 2: 1620}, Steps: 36754, Time: 406.49s\n",
      "Rank 4: Episode 82, Total Reward: 5761.25, Win Rate: 0.58, Wins: 409, Losses: 301, Ações: {0: 2823, 1: 32199, 2: 1732}, Steps: 36754, Time: 406.40s\n",
      "Rank 5: Episode 65, Total Reward: 5133.75, Win Rate: 0.57, Wins: 478, Losses: 364, Ações: {0: 15730, 1: 15891, 2: 5133}, Steps: 36754, Time: 429.88s\n",
      "Rank 6: Episode 81, Total Reward: 4762.00, Win Rate: 0.58, Wins: 340, Losses: 249, Ações: {0: 18325, 1: 16838, 2: 1591}, Steps: 36754, Time: 445.65s\n",
      "Rank 7: Episode 46, Total Reward: 4755.00, Win Rate: 0.57, Wins: 805, Losses: 595, Ações: {0: 21765, 1: 3142, 2: 11847}, Steps: 36754, Time: 401.98s\n",
      "Rank 8: Episode 58, Total Reward: 4702.50, Win Rate: 0.57, Wins: 767, Losses: 587, Ações: {0: 14580, 1: 19618, 2: 2556}, Steps: 36754, Time: 440.34s\n",
      "Rank 9: Episode 72, Total Reward: 4371.75, Win Rate: 0.54, Wins: 391, Losses: 331, Ações: {0: 25142, 1: 5333, 2: 6279}, Steps: 36754, Time: 432.91s\n",
      "Rank 10: Episode 90, Total Reward: 4166.75, Win Rate: 0.53, Wins: 365, Losses: 330, Ações: {0: 9294, 1: 26733, 2: 727}, Steps: 36754, Time: 387.26s\n"
     ]
    }
   ],
   "source": [
    "# Configurações do treinamento\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "memory_size = 10000\n",
    "target_update = 1000\n",
    "beta_start = 0.4\n",
    "beta_frames = num_episodes * len(data)\n",
    "alpha = 0.6\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net = RainbowDQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Inicializar o Prioritized Replay Buffer\n",
    "replay_buffer = PrioritizedReplayBuffer(memory_size, alpha=alpha)\n",
    "\n",
    "# Inicializar a lista de melhores episódios\n",
    "best_episodes = []\n",
    "\n",
    "# Função para selecionar ação usando Noisy Nets\n",
    "def select_action(state):\n",
    "    with torch.no_grad():\n",
    "        q_values = q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "save_dir = \"4.17.2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "beta = beta_start\n",
    "frame_idx = 0  # Contador de frames para ajustar beta\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        frame_idx += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        replay_buffer.push(obs, action, reward, obs_next, done)\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Atualizar recompensa total\n",
    "        total_reward += reward\n",
    "\n",
    "        # Resetar ruído das Noisy Nets\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(replay_buffer.buffer) >= batch_size:\n",
    "            beta = min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "            states, actions_batch, rewards_batch, next_states, dones, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando Double DQN\n",
    "            with torch.no_grad():\n",
    "                next_actions = q_net(next_states).argmax(1, keepdim=True)\n",
    "                next_q_values = target_net(next_states).gather(1, next_actions)\n",
    "                target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular o erro para Prioritized Replay\n",
    "            td_errors = (q_values - target_q_values).detach().cpu().numpy().flatten()\n",
    "            new_priorities = np.abs(td_errors) + 1e-6\n",
    "            replay_buffer.update_priorities(indices, new_priorities)\n",
    "\n",
    "            # Calcular a perda ponderada\n",
    "            loss = (weights * nn.functional.mse_loss(q_values, target_q_values, reduction='none')).mean()\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Atualizar a rede alvo\n",
    "            if frame_idx % target_update == 0:\n",
    "                target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista dos melhores e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log do episódio se for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "# Exibir os top 10 episódios ao final do treinamento\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
