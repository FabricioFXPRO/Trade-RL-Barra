{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -286.00, Win Rate: 0.52, Wins: 1496, Losses: 1403, Epsilon: 0.4950, Steps: 36754, Time: 155.52s\n",
      "Ações: Manter=10446, Comprar=13517, Vender=12791\n",
      "Ganhos Totais: 37796.25, Perdas Totais: -38082.25\n",
      "Modelo e log do episódio 1 salvos em: 4.7.8\\model_episode_1.pth e 4.7.8\\log_episode_1.csv\n",
      "\n",
      "Episode 2/100, Total Reward: -4364.25, Win Rate: 0.50, Wins: 1360, Losses: 1361, Epsilon: 0.4900, Steps: 36754, Time: 163.37s\n",
      "Ações: Manter=10454, Comprar=12164, Vender=14136\n",
      "Ganhos Totais: 35564.25, Perdas Totais: -39928.50\n",
      "Modelo e log do episódio 2 salvos em: 4.7.8\\model_episode_2.pth e 4.7.8\\log_episode_2.csv\n",
      "\n",
      "Episode 3/100, Total Reward: -2764.00, Win Rate: 0.50, Wins: 1286, Losses: 1291, Epsilon: 0.4851, Steps: 36754, Time: 169.01s\n",
      "Ações: Manter=11068, Comprar=12556, Vender=13130\n",
      "Ganhos Totais: 35062.00, Perdas Totais: -37826.00\n",
      "Modelo e log do episódio 3 salvos em: 4.7.8\\model_episode_3.pth e 4.7.8\\log_episode_3.csv\n",
      "\n",
      "Episode 4/100, Total Reward: -1397.75, Win Rate: 0.52, Wins: 1391, Losses: 1295, Epsilon: 0.4803, Steps: 36754, Time: 171.10s\n",
      "Ações: Manter=10615, Comprar=13302, Vender=12837\n",
      "Ganhos Totais: 36596.75, Perdas Totais: -37994.50\n",
      "Modelo e log do episódio 4 salvos em: 4.7.8\\model_episode_4.pth e 4.7.8\\log_episode_4.csv\n",
      "\n",
      "Episode 5/100, Total Reward: -1471.50, Win Rate: 0.51, Wins: 1396, Losses: 1344, Epsilon: 0.4755, Steps: 36754, Time: 172.43s\n",
      "Ações: Manter=10608, Comprar=12910, Vender=13236\n",
      "Ganhos Totais: 36643.75, Perdas Totais: -38115.25\n",
      "Modelo e log do episódio 5 salvos em: 4.7.8\\model_episode_5.pth e 4.7.8\\log_episode_5.csv\n",
      "\n",
      "Episode 6/100, Total Reward: -3275.25, Win Rate: 0.50, Wins: 1312, Losses: 1327, Epsilon: 0.4707, Steps: 36754, Time: 174.12s\n",
      "Ações: Manter=13151, Comprar=11997, Vender=11606\n",
      "Ganhos Totais: 34111.25, Perdas Totais: -37386.50\n",
      "Modelo e log do episódio 6 salvos em: 4.7.8\\model_episode_6.pth e 4.7.8\\log_episode_6.csv\n",
      "\n",
      "Episode 7/100, Total Reward: -1649.50, Win Rate: 0.51, Wins: 1375, Losses: 1307, Epsilon: 0.4660, Steps: 36754, Time: 171.66s\n",
      "Ações: Manter=11384, Comprar=11764, Vender=13606\n",
      "Ganhos Totais: 37428.00, Perdas Totais: -39077.50\n",
      "Modelo e log do episódio 7 salvos em: 4.7.8\\model_episode_7.pth e 4.7.8\\log_episode_7.csv\n",
      "\n",
      "Episode 8/100, Total Reward: -1174.50, Win Rate: 0.51, Wins: 1404, Losses: 1323, Epsilon: 0.4614, Steps: 36754, Time: 171.46s\n",
      "Ações: Manter=11889, Comprar=11832, Vender=13033\n",
      "Ganhos Totais: 35707.75, Perdas Totais: -36882.25\n",
      "Modelo e log do episódio 8 salvos em: 4.7.8\\model_episode_8.pth e 4.7.8\\log_episode_8.csv\n",
      "\n",
      "Episode 9/100, Total Reward: 359.25, Win Rate: 0.53, Wins: 1426, Losses: 1246, Epsilon: 0.4568, Steps: 36754, Time: 173.47s\n",
      "Ações: Manter=13049, Comprar=12360, Vender=11345\n",
      "Ganhos Totais: 37235.75, Perdas Totais: -36876.50\n",
      "Modelo e log do episódio 9 salvos em: 4.7.8\\model_episode_9.pth e 4.7.8\\log_episode_9.csv\n",
      "\n",
      "Episode 10/100, Total Reward: 1336.50, Win Rate: 0.53, Wins: 1501, Losses: 1338, Epsilon: 0.4522, Steps: 36754, Time: 172.61s\n",
      "Ações: Manter=11580, Comprar=12911, Vender=12263\n",
      "Ganhos Totais: 38360.50, Perdas Totais: -37024.00\n",
      "Modelo e log do episódio 10 salvos em: 4.7.8\\model_episode_10.pth e 4.7.8\\log_episode_10.csv\n",
      "\n",
      "Episode 11/100, Total Reward: -1142.00, Win Rate: 0.51, Wins: 1373, Losses: 1330, Epsilon: 0.4477, Steps: 36754, Time: 171.90s\n",
      "Ações: Manter=13083, Comprar=12157, Vender=11514\n",
      "Ganhos Totais: 36395.50, Perdas Totais: -37537.50\n",
      "Modelo e log do episódio 11 salvos em: 4.7.8\\model_episode_11.pth e 4.7.8\\log_episode_11.csv\n",
      "\n",
      "Episode 12/100, Total Reward: -5717.00, Win Rate: 0.50, Wins: 1355, Losses: 1353, Epsilon: 0.4432, Steps: 36754, Time: 172.31s\n",
      "Ações: Manter=11152, Comprar=13605, Vender=11997\n",
      "Ganhos Totais: 34530.00, Perdas Totais: -40247.00\n",
      "Episode 13/100, Total Reward: -1065.75, Win Rate: 0.51, Wins: 1371, Losses: 1299, Epsilon: 0.4388, Steps: 36754, Time: 141.90s\n",
      "Ações: Manter=11423, Comprar=13316, Vender=12015\n",
      "Ganhos Totais: 36125.50, Perdas Totais: -37191.25\n",
      "Modelo e log do episódio 13 salvos em: 4.7.8\\model_episode_13.pth e 4.7.8\\log_episode_13.csv\n",
      "\n",
      "Episode 14/100, Total Reward: -4008.50, Win Rate: 0.51, Wins: 1329, Losses: 1261, Epsilon: 0.4344, Steps: 36754, Time: 114.11s\n",
      "Ações: Manter=11958, Comprar=12628, Vender=12168\n",
      "Ganhos Totais: 34675.00, Perdas Totais: -38683.50\n",
      "Episode 15/100, Total Reward: 1511.50, Win Rate: 0.52, Wins: 1338, Losses: 1250, Epsilon: 0.4300, Steps: 36754, Time: 113.24s\n",
      "Ações: Manter=12975, Comprar=12135, Vender=11644\n",
      "Ganhos Totais: 37120.00, Perdas Totais: -35608.50\n",
      "Modelo e log do episódio 15 salvos em: 4.7.8\\model_episode_15.pth e 4.7.8\\log_episode_15.csv\n",
      "\n",
      "Episode 16/100, Total Reward: 1325.50, Win Rate: 0.53, Wins: 1472, Losses: 1310, Epsilon: 0.4257, Steps: 36754, Time: 117.62s\n",
      "Ações: Manter=11231, Comprar=12264, Vender=13259\n",
      "Ganhos Totais: 38854.25, Perdas Totais: -37528.75\n",
      "Modelo e log do episódio 16 salvos em: 4.7.8\\model_episode_16.pth e 4.7.8\\log_episode_16.csv\n",
      "\n",
      "Episode 17/100, Total Reward: -1483.75, Win Rate: 0.52, Wins: 1463, Losses: 1377, Epsilon: 0.4215, Steps: 36754, Time: 118.70s\n",
      "Ações: Manter=11146, Comprar=13865, Vender=11743\n",
      "Ganhos Totais: 37097.00, Perdas Totais: -38580.75\n",
      "Episode 18/100, Total Reward: -92.25, Win Rate: 0.51, Wins: 1278, Losses: 1211, Epsilon: 0.4173, Steps: 36754, Time: 119.82s\n",
      "Ações: Manter=12403, Comprar=12974, Vender=11377\n",
      "Ganhos Totais: 35868.50, Perdas Totais: -35960.75\n",
      "Modelo e log do episódio 18 salvos em: 4.7.8\\model_episode_18.pth e 4.7.8\\log_episode_18.csv\n",
      "\n",
      "Episode 19/100, Total Reward: -25.50, Win Rate: 0.51, Wins: 1341, Losses: 1282, Epsilon: 0.4131, Steps: 36754, Time: 120.31s\n",
      "Ações: Manter=12253, Comprar=12464, Vender=12037\n",
      "Ganhos Totais: 36968.75, Perdas Totais: -36994.25\n",
      "Modelo e log do episódio 19 salvos em: 4.7.8\\model_episode_19.pth e 4.7.8\\log_episode_19.csv\n",
      "\n",
      "Episode 20/100, Total Reward: 2855.75, Win Rate: 0.54, Wins: 1436, Losses: 1222, Epsilon: 0.4090, Steps: 36754, Time: 117.40s\n",
      "Ações: Manter=12802, Comprar=12080, Vender=11872\n",
      "Ganhos Totais: 38058.25, Perdas Totais: -35202.50\n",
      "Modelo e log do episódio 20 salvos em: 4.7.8\\model_episode_20.pth e 4.7.8\\log_episode_20.csv\n",
      "\n",
      "Episode 21/100, Total Reward: 1362.50, Win Rate: 0.53, Wins: 1462, Losses: 1305, Epsilon: 0.4049, Steps: 36754, Time: 113.25s\n",
      "Ações: Manter=11437, Comprar=12475, Vender=12842\n",
      "Ganhos Totais: 39294.50, Perdas Totais: -37932.00\n",
      "Modelo e log do episódio 21 salvos em: 4.7.8\\model_episode_21.pth e 4.7.8\\log_episode_21.csv\n",
      "\n",
      "Episode 22/100, Total Reward: -215.25, Win Rate: 0.52, Wins: 1408, Losses: 1302, Epsilon: 0.4008, Steps: 36754, Time: 113.81s\n",
      "Ações: Manter=12029, Comprar=12119, Vender=12606\n",
      "Ganhos Totais: 37472.25, Perdas Totais: -37687.50\n",
      "Modelo e log do episódio 22 salvos em: 4.7.8\\model_episode_22.pth e 4.7.8\\log_episode_22.csv\n",
      "\n",
      "Episode 23/100, Total Reward: -1377.25, Win Rate: 0.52, Wins: 1411, Losses: 1278, Epsilon: 0.3968, Steps: 36754, Time: 127.54s\n",
      "Ações: Manter=11828, Comprar=13011, Vender=11915\n",
      "Ganhos Totais: 36763.00, Perdas Totais: -38140.25\n",
      "Episode 24/100, Total Reward: -3055.00, Win Rate: 0.53, Wins: 1328, Losses: 1191, Epsilon: 0.3928, Steps: 36754, Time: 132.81s\n",
      "Ações: Manter=12874, Comprar=12119, Vender=11761\n",
      "Ganhos Totais: 34965.25, Perdas Totais: -38020.25\n",
      "Episode 25/100, Total Reward: -969.00, Win Rate: 0.53, Wins: 1319, Losses: 1184, Epsilon: 0.3889, Steps: 36754, Time: 134.60s\n",
      "Ações: Manter=12347, Comprar=11764, Vender=12643\n",
      "Ganhos Totais: 35575.25, Perdas Totais: -36544.25\n",
      "Episode 26/100, Total Reward: -2021.50, Win Rate: 0.52, Wins: 1305, Losses: 1187, Epsilon: 0.3850, Steps: 36754, Time: 140.69s\n",
      "Ações: Manter=12800, Comprar=10666, Vender=13288\n",
      "Ganhos Totais: 35559.50, Perdas Totais: -37581.00\n",
      "Episode 27/100, Total Reward: 1611.00, Win Rate: 0.53, Wins: 1311, Losses: 1183, Epsilon: 0.3812, Steps: 36754, Time: 126.83s\n",
      "Ações: Manter=12728, Comprar=12477, Vender=11549\n",
      "Ganhos Totais: 36009.75, Perdas Totais: -34398.75\n",
      "Modelo e log do episódio 27 salvos em: 4.7.8\\model_episode_27.pth e 4.7.8\\log_episode_27.csv\n",
      "\n",
      "Episode 28/100, Total Reward: 1110.00, Win Rate: 0.54, Wins: 1370, Losses: 1157, Epsilon: 0.3774, Steps: 36754, Time: 127.13s\n",
      "Ações: Manter=13512, Comprar=11333, Vender=11909\n",
      "Ganhos Totais: 36959.00, Perdas Totais: -35849.00\n",
      "Modelo e log do episódio 28 salvos em: 4.7.8\\model_episode_28.pth e 4.7.8\\log_episode_28.csv\n",
      "\n",
      "Episode 29/100, Total Reward: -2457.50, Win Rate: 0.51, Wins: 1322, Losses: 1247, Epsilon: 0.3736, Steps: 36754, Time: 128.22s\n",
      "Ações: Manter=12437, Comprar=11964, Vender=12353\n",
      "Ganhos Totais: 35809.50, Perdas Totais: -38267.00\n",
      "Episode 30/100, Total Reward: -3403.25, Win Rate: 0.53, Wins: 1307, Losses: 1181, Epsilon: 0.3699, Steps: 36754, Time: 128.55s\n",
      "Ações: Manter=12593, Comprar=11865, Vender=12296\n",
      "Ganhos Totais: 34567.00, Perdas Totais: -37970.25\n",
      "Episode 31/100, Total Reward: -340.25, Win Rate: 0.54, Wins: 1511, Losses: 1311, Epsilon: 0.3662, Steps: 36754, Time: 128.72s\n",
      "Ações: Manter=10747, Comprar=13875, Vender=12132\n",
      "Ganhos Totais: 38287.50, Perdas Totais: -38627.75\n",
      "Episode 32/100, Total Reward: -2014.00, Win Rate: 0.52, Wins: 1245, Losses: 1132, Epsilon: 0.3625, Steps: 36754, Time: 135.90s\n",
      "Ações: Manter=12216, Comprar=14020, Vender=10518\n",
      "Ganhos Totais: 36048.75, Perdas Totais: -38062.75\n",
      "Episode 33/100, Total Reward: -3384.00, Win Rate: 0.51, Wins: 1259, Losses: 1214, Epsilon: 0.3589, Steps: 36754, Time: 118.19s\n",
      "Ações: Manter=14216, Comprar=11513, Vender=11025\n",
      "Ganhos Totais: 33955.00, Perdas Totais: -37339.00\n",
      "Episode 34/100, Total Reward: -1370.50, Win Rate: 0.53, Wins: 1279, Losses: 1127, Epsilon: 0.3553, Steps: 36754, Time: 120.44s\n",
      "Ações: Manter=12613, Comprar=13745, Vender=10396\n",
      "Ganhos Totais: 35937.00, Perdas Totais: -37307.50\n",
      "Episode 35/100, Total Reward: -841.75, Win Rate: 0.53, Wins: 1276, Losses: 1122, Epsilon: 0.3517, Steps: 36754, Time: 114.92s\n",
      "Ações: Manter=12537, Comprar=13051, Vender=11166\n",
      "Ganhos Totais: 36333.75, Perdas Totais: -37175.50\n",
      "Episode 36/100, Total Reward: -3059.00, Win Rate: 0.53, Wins: 1224, Losses: 1093, Epsilon: 0.3482, Steps: 36754, Time: 113.78s\n",
      "Ações: Manter=13379, Comprar=11820, Vender=11555\n",
      "Ganhos Totais: 33166.50, Perdas Totais: -36225.50\n",
      "Episode 37/100, Total Reward: -1095.50, Win Rate: 0.53, Wins: 1211, Losses: 1084, Epsilon: 0.3447, Steps: 36754, Time: 113.86s\n",
      "Ações: Manter=14793, Comprar=11355, Vender=10606\n",
      "Ganhos Totais: 34710.75, Perdas Totais: -35806.25\n",
      "Episode 38/100, Total Reward: -2427.00, Win Rate: 0.51, Wins: 1158, Losses: 1093, Epsilon: 0.3413, Steps: 36754, Time: 114.07s\n",
      "Ações: Manter=13919, Comprar=11507, Vender=11328\n",
      "Ganhos Totais: 34727.75, Perdas Totais: -37154.75\n",
      "Episode 39/100, Total Reward: 2210.25, Win Rate: 0.56, Wins: 1438, Losses: 1143, Epsilon: 0.3379, Steps: 36754, Time: 113.97s\n",
      "Ações: Manter=10174, Comprar=12997, Vender=13583\n",
      "Ganhos Totais: 37587.25, Perdas Totais: -35377.00\n",
      "Modelo e log do episódio 39 salvos em: 4.7.8\\model_episode_39.pth e 4.7.8\\log_episode_39.csv\n",
      "\n",
      "Episode 40/100, Total Reward: -935.75, Win Rate: 0.54, Wins: 1299, Losses: 1127, Epsilon: 0.3345, Steps: 36754, Time: 113.87s\n",
      "Ações: Manter=14256, Comprar=12185, Vender=10313\n",
      "Ganhos Totais: 34233.00, Perdas Totais: -35168.75\n",
      "Episode 41/100, Total Reward: -4001.75, Win Rate: 0.50, Wins: 1206, Losses: 1189, Epsilon: 0.3311, Steps: 36754, Time: 114.45s\n",
      "Ações: Manter=12372, Comprar=12784, Vender=11598\n",
      "Ganhos Totais: 33628.25, Perdas Totais: -37630.00\n",
      "Episode 42/100, Total Reward: -978.75, Win Rate: 0.53, Wins: 1284, Losses: 1148, Epsilon: 0.3278, Steps: 36754, Time: 114.24s\n",
      "Ações: Manter=13371, Comprar=11607, Vender=11776\n",
      "Ganhos Totais: 34588.75, Perdas Totais: -35567.50\n",
      "Episode 43/100, Total Reward: -3661.75, Win Rate: 0.52, Wins: 1204, Losses: 1091, Epsilon: 0.3246, Steps: 36754, Time: 113.75s\n",
      "Ações: Manter=15401, Comprar=11250, Vender=10103\n",
      "Ganhos Totais: 32420.75, Perdas Totais: -36082.50\n",
      "Episode 44/100, Total Reward: 219.50, Win Rate: 0.53, Wins: 1234, Losses: 1114, Epsilon: 0.3213, Steps: 36754, Time: 114.33s\n",
      "Ações: Manter=13878, Comprar=10641, Vender=12235\n",
      "Ganhos Totais: 34365.75, Perdas Totais: -34146.25\n",
      "Modelo e log do episódio 44 salvos em: 4.7.8\\model_episode_44.pth e 4.7.8\\log_episode_44.csv\n",
      "\n",
      "Episode 45/100, Total Reward: -4541.50, Win Rate: 0.50, Wins: 1295, Losses: 1278, Epsilon: 0.3181, Steps: 36754, Time: 114.12s\n",
      "Ações: Manter=12741, Comprar=12019, Vender=11994\n",
      "Ganhos Totais: 33982.50, Perdas Totais: -38524.00\n",
      "Episode 46/100, Total Reward: -445.00, Win Rate: 0.53, Wins: 1229, Losses: 1070, Epsilon: 0.3149, Steps: 36754, Time: 113.86s\n",
      "Ações: Manter=14209, Comprar=11770, Vender=10775\n",
      "Ganhos Totais: 34104.50, Perdas Totais: -34549.50\n",
      "Episode 47/100, Total Reward: -183.00, Win Rate: 0.54, Wins: 1377, Losses: 1155, Epsilon: 0.3118, Steps: 36754, Time: 114.58s\n",
      "Ações: Manter=11630, Comprar=12513, Vender=12611\n",
      "Ganhos Totais: 36523.75, Perdas Totais: -36706.75\n",
      "Episode 48/100, Total Reward: -1435.50, Win Rate: 0.53, Wins: 1244, Losses: 1106, Epsilon: 0.3086, Steps: 36754, Time: 125.25s\n",
      "Ações: Manter=12637, Comprar=11378, Vender=12739\n",
      "Ganhos Totais: 34435.25, Perdas Totais: -35870.75\n",
      "Episode 49/100, Total Reward: -5484.25, Win Rate: 0.53, Wins: 1219, Losses: 1093, Epsilon: 0.3056, Steps: 36754, Time: 128.09s\n",
      "Ações: Manter=13379, Comprar=10858, Vender=12517\n",
      "Ganhos Totais: 32042.50, Perdas Totais: -37526.75\n",
      "Episode 50/100, Total Reward: 1881.00, Win Rate: 0.54, Wins: 1480, Losses: 1243, Epsilon: 0.3025, Steps: 36754, Time: 127.33s\n",
      "Ações: Manter=10900, Comprar=14303, Vender=11551\n",
      "Ganhos Totais: 37297.50, Perdas Totais: -35416.50\n",
      "Modelo e log do episódio 50 salvos em: 4.7.8\\model_episode_50.pth e 4.7.8\\log_episode_50.csv\n",
      "\n",
      "Episode 51/100, Total Reward: -1534.25, Win Rate: 0.55, Wins: 1225, Losses: 1009, Epsilon: 0.2995, Steps: 36754, Time: 127.72s\n",
      "Ações: Manter=13900, Comprar=11196, Vender=11658\n",
      "Ganhos Totais: 34208.50, Perdas Totais: -35742.75\n",
      "Episode 52/100, Total Reward: 565.75, Win Rate: 0.54, Wins: 1200, Losses: 1023, Epsilon: 0.2965, Steps: 36754, Time: 126.43s\n",
      "Ações: Manter=11262, Comprar=10999, Vender=14493\n",
      "Ganhos Totais: 35196.00, Perdas Totais: -34630.25\n",
      "Modelo e log do episódio 52 salvos em: 4.7.8\\model_episode_52.pth e 4.7.8\\log_episode_52.csv\n",
      "\n",
      "Episode 53/100, Total Reward: -3387.75, Win Rate: 0.53, Wins: 1184, Losses: 1031, Epsilon: 0.2935, Steps: 36754, Time: 127.20s\n",
      "Ações: Manter=13765, Comprar=9453, Vender=13536\n",
      "Ganhos Totais: 33144.25, Perdas Totais: -36532.00\n",
      "Episode 54/100, Total Reward: 277.00, Win Rate: 0.54, Wins: 1203, Losses: 1013, Epsilon: 0.2906, Steps: 36754, Time: 127.44s\n",
      "Ações: Manter=13021, Comprar=9951, Vender=13782\n",
      "Ganhos Totais: 34756.50, Perdas Totais: -34479.50\n",
      "Episode 55/100, Total Reward: -2305.50, Win Rate: 0.54, Wins: 1271, Losses: 1080, Epsilon: 0.2877, Steps: 36754, Time: 127.03s\n",
      "Ações: Manter=10385, Comprar=14160, Vender=12209\n",
      "Ganhos Totais: 34638.00, Perdas Totais: -36943.50\n",
      "Episode 56/100, Total Reward: 1326.25, Win Rate: 0.54, Wins: 1153, Losses: 993, Epsilon: 0.2848, Steps: 36754, Time: 127.82s\n",
      "Ações: Manter=15705, Comprar=8979, Vender=12070\n",
      "Ganhos Totais: 34358.00, Perdas Totais: -33031.75\n",
      "Modelo e log do episódio 56 salvos em: 4.7.8\\model_episode_56.pth e 4.7.8\\log_episode_56.csv\n",
      "\n",
      "Episode 57/100, Total Reward: -2315.50, Win Rate: 0.54, Wins: 1164, Losses: 972, Epsilon: 0.2820, Steps: 36754, Time: 127.56s\n",
      "Ações: Manter=14096, Comprar=8471, Vender=14187\n",
      "Ganhos Totais: 33514.75, Perdas Totais: -35830.25\n",
      "Episode 58/100, Total Reward: -2394.00, Win Rate: 0.54, Wins: 1324, Losses: 1122, Epsilon: 0.2791, Steps: 36754, Time: 126.70s\n",
      "Ações: Manter=10636, Comprar=11663, Vender=14455\n",
      "Ganhos Totais: 35549.25, Perdas Totais: -37943.25\n",
      "Episode 59/100, Total Reward: -1701.75, Win Rate: 0.53, Wins: 1179, Losses: 1034, Epsilon: 0.2763, Steps: 36754, Time: 129.24s\n",
      "Ações: Manter=13550, Comprar=9029, Vender=14175\n",
      "Ganhos Totais: 33590.25, Perdas Totais: -35292.00\n",
      "Episode 60/100, Total Reward: -5366.75, Win Rate: 0.52, Wins: 1089, Losses: 1001, Epsilon: 0.2736, Steps: 36754, Time: 115.14s\n",
      "Ações: Manter=14526, Comprar=8858, Vender=13370\n",
      "Ganhos Totais: 31486.50, Perdas Totais: -36853.25\n",
      "Episode 61/100, Total Reward: 2674.25, Win Rate: 0.55, Wins: 1398, Losses: 1154, Epsilon: 0.2708, Steps: 36754, Time: 114.99s\n",
      "Ações: Manter=10542, Comprar=15019, Vender=11193\n",
      "Ganhos Totais: 36269.50, Perdas Totais: -33595.25\n",
      "Modelo e log do episódio 61 salvos em: 4.7.8\\model_episode_61.pth e 4.7.8\\log_episode_61.csv\n",
      "\n",
      "Episode 62/100, Total Reward: 718.50, Win Rate: 0.54, Wins: 1267, Losses: 1088, Epsilon: 0.2681, Steps: 36754, Time: 115.61s\n",
      "Ações: Manter=10105, Comprar=14351, Vender=12298\n",
      "Ganhos Totais: 36730.00, Perdas Totais: -36011.50\n",
      "Episode 63/100, Total Reward: -4020.75, Win Rate: 0.52, Wins: 1231, Losses: 1138, Epsilon: 0.2655, Steps: 36754, Time: 133.70s\n",
      "Ações: Manter=11890, Comprar=11972, Vender=12892\n",
      "Ganhos Totais: 32815.75, Perdas Totais: -36836.50\n",
      "Episode 64/100, Total Reward: -147.75, Win Rate: 0.54, Wins: 1267, Losses: 1101, Epsilon: 0.2628, Steps: 36754, Time: 131.27s\n",
      "Ações: Manter=9742, Comprar=13768, Vender=13244\n",
      "Ganhos Totais: 35797.00, Perdas Totais: -35944.75\n",
      "Episode 65/100, Total Reward: 2.00, Win Rate: 0.53, Wins: 1115, Losses: 981, Epsilon: 0.2602, Steps: 36754, Time: 128.75s\n",
      "Ações: Manter=14758, Comprar=9507, Vender=12489\n",
      "Ganhos Totais: 32836.75, Perdas Totais: -32834.75\n",
      "Episode 66/100, Total Reward: 473.75, Win Rate: 0.54, Wins: 1171, Losses: 1015, Epsilon: 0.2576, Steps: 36754, Time: 128.99s\n",
      "Ações: Manter=13305, Comprar=11689, Vender=11760\n",
      "Ganhos Totais: 34653.50, Perdas Totais: -34179.75\n",
      "Episode 67/100, Total Reward: 829.25, Win Rate: 0.55, Wins: 1263, Losses: 1040, Epsilon: 0.2550, Steps: 36754, Time: 128.34s\n",
      "Ações: Manter=12800, Comprar=13642, Vender=10312\n",
      "Ganhos Totais: 35455.25, Perdas Totais: -34626.00\n",
      "Episode 68/100, Total Reward: -94.50, Win Rate: 0.55, Wins: 1353, Losses: 1113, Epsilon: 0.2524, Steps: 36754, Time: 128.70s\n",
      "Ações: Manter=10625, Comprar=14521, Vender=11608\n",
      "Ganhos Totais: 36104.75, Perdas Totais: -36199.25\n",
      "Episode 69/100, Total Reward: -3250.75, Win Rate: 0.54, Wins: 1143, Losses: 960, Epsilon: 0.2499, Steps: 36754, Time: 129.48s\n",
      "Ações: Manter=13089, Comprar=11015, Vender=12650\n",
      "Ganhos Totais: 32648.00, Perdas Totais: -35898.75\n",
      "Episode 70/100, Total Reward: -521.25, Win Rate: 0.54, Wins: 1226, Losses: 1052, Epsilon: 0.2474, Steps: 36754, Time: 128.30s\n",
      "Ações: Manter=13677, Comprar=12338, Vender=10739\n",
      "Ganhos Totais: 35081.50, Perdas Totais: -35602.75\n",
      "Episode 71/100, Total Reward: -3061.25, Win Rate: 0.53, Wins: 1230, Losses: 1078, Epsilon: 0.2449, Steps: 36754, Time: 128.40s\n",
      "Ações: Manter=13083, Comprar=14076, Vender=9595\n",
      "Ganhos Totais: 34158.75, Perdas Totais: -37220.00\n",
      "Episode 72/100, Total Reward: -3307.50, Win Rate: 0.54, Wins: 1093, Losses: 934, Epsilon: 0.2425, Steps: 36754, Time: 128.65s\n",
      "Ações: Manter=14470, Comprar=10096, Vender=12188\n",
      "Ganhos Totais: 31607.25, Perdas Totais: -34914.75\n",
      "Episode 73/100, Total Reward: 629.75, Win Rate: 0.56, Wins: 1183, Losses: 929, Epsilon: 0.2401, Steps: 36754, Time: 128.49s\n",
      "Ações: Manter=12101, Comprar=12502, Vender=12151\n",
      "Ganhos Totais: 34963.75, Perdas Totais: -34334.00\n",
      "Episode 74/100, Total Reward: -2055.50, Win Rate: 0.56, Wins: 1225, Losses: 974, Epsilon: 0.2377, Steps: 36754, Time: 128.72s\n",
      "Ações: Manter=13217, Comprar=11679, Vender=11858\n",
      "Ganhos Totais: 34301.50, Perdas Totais: -36357.00\n",
      "Episode 75/100, Total Reward: -3818.25, Win Rate: 0.53, Wins: 1066, Losses: 960, Epsilon: 0.2353, Steps: 36754, Time: 128.77s\n",
      "Ações: Manter=12849, Comprar=12266, Vender=11639\n",
      "Ganhos Totais: 31845.00, Perdas Totais: -35663.25\n",
      "Episode 76/100, Total Reward: -2653.00, Win Rate: 0.54, Wins: 1067, Losses: 922, Epsilon: 0.2329, Steps: 36754, Time: 125.25s\n",
      "Ações: Manter=11678, Comprar=12566, Vender=12510\n",
      "Ganhos Totais: 32149.25, Perdas Totais: -34802.25\n",
      "Episode 77/100, Total Reward: -2897.00, Win Rate: 0.54, Wins: 1144, Losses: 972, Epsilon: 0.2306, Steps: 36754, Time: 126.87s\n",
      "Ações: Manter=10481, Comprar=13710, Vender=12563\n",
      "Ganhos Totais: 34383.00, Perdas Totais: -37280.00\n",
      "Episode 78/100, Total Reward: -1992.50, Win Rate: 0.54, Wins: 1096, Losses: 918, Epsilon: 0.2283, Steps: 36754, Time: 129.14s\n",
      "Ações: Manter=13876, Comprar=11337, Vender=11541\n",
      "Ganhos Totais: 32812.25, Perdas Totais: -34804.75\n",
      "Episode 79/100, Total Reward: -2475.25, Win Rate: 0.52, Wins: 1016, Losses: 927, Epsilon: 0.2260, Steps: 36754, Time: 128.65s\n",
      "Ações: Manter=13333, Comprar=11487, Vender=11934\n",
      "Ganhos Totais: 31492.00, Perdas Totais: -33967.25\n",
      "Episode 80/100, Total Reward: 2478.25, Win Rate: 0.55, Wins: 1116, Losses: 916, Epsilon: 0.2238, Steps: 36754, Time: 128.76s\n",
      "Ações: Manter=12642, Comprar=14749, Vender=9363\n",
      "Ganhos Totais: 34484.00, Perdas Totais: -32005.75\n",
      "Modelo e log do episódio 80 salvos em: 4.7.8\\model_episode_80.pth e 4.7.8\\log_episode_80.csv\n",
      "\n",
      "Episode 81/100, Total Reward: -1721.25, Win Rate: 0.55, Wins: 1240, Losses: 1008, Epsilon: 0.2215, Steps: 36754, Time: 128.52s\n",
      "Ações: Manter=11715, Comprar=13318, Vender=11721\n",
      "Ganhos Totais: 34469.00, Perdas Totais: -36190.25\n",
      "Episode 82/100, Total Reward: 3302.75, Win Rate: 0.55, Wins: 1138, Losses: 947, Epsilon: 0.2193, Steps: 36754, Time: 128.45s\n",
      "Ações: Manter=14968, Comprar=13266, Vender=8520\n",
      "Ganhos Totais: 34917.75, Perdas Totais: -31615.00\n",
      "Modelo e log do episódio 82 salvos em: 4.7.8\\model_episode_82.pth e 4.7.8\\log_episode_82.csv\n",
      "\n",
      "Episode 83/100, Total Reward: 1500.75, Win Rate: 0.54, Wins: 1070, Losses: 915, Epsilon: 0.2171, Steps: 36754, Time: 129.15s\n",
      "Ações: Manter=13992, Comprar=12343, Vender=10419\n",
      "Ganhos Totais: 34481.75, Perdas Totais: -32981.00\n",
      "Modelo e log do episódio 83 salvos em: 4.7.8\\model_episode_83.pth e 4.7.8\\log_episode_83.csv\n",
      "\n",
      "Episode 84/100, Total Reward: -571.00, Win Rate: 0.53, Wins: 1028, Losses: 913, Epsilon: 0.2149, Steps: 36754, Time: 129.43s\n",
      "Ações: Manter=12712, Comprar=15421, Vender=8621\n",
      "Ganhos Totais: 32442.50, Perdas Totais: -33013.50\n",
      "Episode 85/100, Total Reward: -1537.50, Win Rate: 0.53, Wins: 964, Losses: 843, Epsilon: 0.2128, Steps: 36754, Time: 128.31s\n",
      "Ações: Manter=10934, Comprar=16946, Vender=8874\n",
      "Ganhos Totais: 31606.00, Perdas Totais: -33143.50\n",
      "Episode 86/100, Total Reward: 422.00, Win Rate: 0.54, Wins: 977, Losses: 820, Epsilon: 0.2107, Steps: 36754, Time: 128.73s\n",
      "Ações: Manter=13342, Comprar=13665, Vender=9747\n",
      "Ganhos Totais: 31771.50, Perdas Totais: -31349.50\n",
      "Episode 87/100, Total Reward: 570.50, Win Rate: 0.55, Wins: 984, Losses: 804, Epsilon: 0.2086, Steps: 36754, Time: 128.87s\n",
      "Ações: Manter=13092, Comprar=13439, Vender=10223\n",
      "Ganhos Totais: 32052.25, Perdas Totais: -31481.75\n",
      "Episode 88/100, Total Reward: -3174.00, Win Rate: 0.52, Wins: 991, Losses: 916, Epsilon: 0.2065, Steps: 36754, Time: 129.07s\n",
      "Ações: Manter=12185, Comprar=13411, Vender=11158\n",
      "Ganhos Totais: 29939.75, Perdas Totais: -33113.75\n",
      "Episode 89/100, Total Reward: 2272.00, Win Rate: 0.57, Wins: 1023, Losses: 782, Epsilon: 0.2044, Steps: 36754, Time: 129.84s\n",
      "Ações: Manter=12572, Comprar=14706, Vender=9476\n",
      "Ganhos Totais: 34095.75, Perdas Totais: -31823.75\n",
      "Modelo e log do episódio 89 salvos em: 4.7.8\\model_episode_89.pth e 4.7.8\\log_episode_89.csv\n",
      "\n",
      "Episode 90/100, Total Reward: 274.75, Win Rate: 0.56, Wins: 1052, Losses: 815, Epsilon: 0.2024, Steps: 36754, Time: 129.42s\n",
      "Ações: Manter=12500, Comprar=14795, Vender=9459\n",
      "Ganhos Totais: 32502.50, Perdas Totais: -32227.75\n",
      "Episode 91/100, Total Reward: -1166.25, Win Rate: 0.54, Wins: 1033, Losses: 873, Epsilon: 0.2003, Steps: 36754, Time: 129.04s\n",
      "Ações: Manter=11005, Comprar=14863, Vender=10886\n",
      "Ganhos Totais: 33117.00, Perdas Totais: -34283.25\n",
      "Episode 92/100, Total Reward: 1494.50, Win Rate: 0.55, Wins: 981, Losses: 818, Epsilon: 0.1983, Steps: 36754, Time: 129.43s\n",
      "Ações: Manter=12380, Comprar=13877, Vender=10497\n",
      "Ganhos Totais: 34042.00, Perdas Totais: -32547.50\n",
      "Episode 93/100, Total Reward: 2719.50, Win Rate: 0.57, Wins: 1005, Losses: 758, Epsilon: 0.1964, Steps: 36754, Time: 129.33s\n",
      "Ações: Manter=14139, Comprar=14979, Vender=7636\n",
      "Ganhos Totais: 33229.00, Perdas Totais: -30509.50\n",
      "Modelo e log do episódio 93 salvos em: 4.7.8\\model_episode_93.pth e 4.7.8\\log_episode_93.csv\n",
      "\n",
      "Episode 94/100, Total Reward: -770.25, Win Rate: 0.56, Wins: 1074, Losses: 856, Epsilon: 0.1944, Steps: 36754, Time: 129.43s\n",
      "Ações: Manter=10133, Comprar=16860, Vender=9761\n",
      "Ganhos Totais: 33394.00, Perdas Totais: -34164.25\n",
      "Episode 95/100, Total Reward: 2555.00, Win Rate: 0.55, Wins: 1065, Losses: 854, Epsilon: 0.1924, Steps: 36754, Time: 129.81s\n",
      "Ações: Manter=14192, Comprar=13662, Vender=8900\n",
      "Ganhos Totais: 33906.75, Perdas Totais: -31351.75\n",
      "Modelo e log do episódio 95 salvos em: 4.7.8\\model_episode_95.pth e 4.7.8\\log_episode_95.csv\n",
      "\n",
      "Episode 96/100, Total Reward: -2585.25, Win Rate: 0.54, Wins: 958, Losses: 831, Epsilon: 0.1905, Steps: 36754, Time: 129.05s\n",
      "Ações: Manter=12684, Comprar=15184, Vender=8886\n",
      "Ganhos Totais: 29747.50, Perdas Totais: -32332.75\n",
      "Episode 97/100, Total Reward: -101.75, Win Rate: 0.55, Wins: 1145, Losses: 931, Epsilon: 0.1886, Steps: 36754, Time: 129.96s\n",
      "Ações: Manter=10733, Comprar=15686, Vender=10335\n",
      "Ganhos Totais: 35695.25, Perdas Totais: -35797.00\n",
      "Episode 98/100, Total Reward: -85.50, Win Rate: 0.54, Wins: 1024, Losses: 874, Epsilon: 0.1867, Steps: 36754, Time: 129.83s\n",
      "Ações: Manter=13922, Comprar=11759, Vender=11073\n",
      "Ganhos Totais: 33396.25, Perdas Totais: -33481.75\n",
      "Episode 99/100, Total Reward: 3197.75, Win Rate: 0.54, Wins: 1012, Losses: 848, Epsilon: 0.1849, Steps: 36754, Time: 129.05s\n",
      "Ações: Manter=14020, Comprar=12598, Vender=10136\n",
      "Ganhos Totais: 33619.50, Perdas Totais: -30421.75\n",
      "Modelo e log do episódio 99 salvos em: 4.7.8\\model_episode_99.pth e 4.7.8\\log_episode_99.csv\n",
      "\n",
      "Episode 100/100, Total Reward: -4086.25, Win Rate: 0.53, Wins: 1020, Losses: 905, Epsilon: 0.1830, Steps: 36754, Time: 129.68s\n",
      "Ações: Manter=12275, Comprar=13561, Vender=10918\n",
      "Ganhos Totais: 29962.50, Perdas Totais: -34048.75\n",
      "\n",
      "Treinamento finalizado.\n",
      "Top 10 Melhores Episódios:\n",
      "Rank 1: Episode 82, Total Reward: 3302.75, Win Rate: 0.55, Wins: 1138, Losses: 947, Ações: {0: 14968, 1: 13266, 2: 8520}, Steps: 36754, Time: 128.45s\n",
      "Rank 2: Episode 99, Total Reward: 3197.75, Win Rate: 0.54, Wins: 1012, Losses: 848, Ações: {0: 14020, 1: 12598, 2: 10136}, Steps: 36754, Time: 129.05s\n",
      "Rank 3: Episode 20, Total Reward: 2855.75, Win Rate: 0.54, Wins: 1436, Losses: 1222, Ações: {0: 12802, 1: 12080, 2: 11872}, Steps: 36754, Time: 117.40s\n",
      "Rank 4: Episode 93, Total Reward: 2719.50, Win Rate: 0.57, Wins: 1005, Losses: 758, Ações: {0: 14139, 1: 14979, 2: 7636}, Steps: 36754, Time: 129.33s\n",
      "Rank 5: Episode 61, Total Reward: 2674.25, Win Rate: 0.55, Wins: 1398, Losses: 1154, Ações: {0: 10542, 1: 15019, 2: 11193}, Steps: 36754, Time: 114.99s\n",
      "Rank 6: Episode 95, Total Reward: 2555.00, Win Rate: 0.55, Wins: 1065, Losses: 854, Ações: {0: 14192, 1: 13662, 2: 8900}, Steps: 36754, Time: 129.81s\n",
      "Rank 7: Episode 80, Total Reward: 2478.25, Win Rate: 0.55, Wins: 1116, Losses: 916, Ações: {0: 12642, 1: 14749, 2: 9363}, Steps: 36754, Time: 128.76s\n",
      "Rank 8: Episode 89, Total Reward: 2272.00, Win Rate: 0.57, Wins: 1023, Losses: 782, Ações: {0: 12572, 1: 14706, 2: 9476}, Steps: 36754, Time: 129.84s\n",
      "Rank 9: Episode 39, Total Reward: 2210.25, Win Rate: 0.56, Wins: 1438, Losses: 1143, Ações: {0: 10174, 1: 12997, 2: 13583}, Steps: 36754, Time: 113.97s\n",
      "Rank 10: Episode 50, Total Reward: 1881.00, Win Rate: 0.54, Wins: 1480, Losses: 1243, Ações: {0: 10900, 1: 14303, 2: 11551}, Steps: 36754, Time: 127.33s\n"
     ]
    }
   ],
   "source": [
    "# Bloco 1: Preparar os Dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import os\n",
    "data = pd.read_csv('D:\\\\dados\\\\bar_M15_V02_data_01-01-2023_a_31-08-2024.csv')\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "# filtra o dataframe para pegar apenas o mês de 08 de 2024\n",
    "#data = data[(data['DateTime'] >= '2024-08-01') & (data['DateTime'] <= '2024-08-31')]\n",
    "\n",
    "# Criar a coluna \"Valor\", que é uma cópia de \"Close\" e não será normalizada\n",
    "data['Valor'] = data['Close']\n",
    "\n",
    "# Normalizar as colunas necessárias (exceto \"Valor\" e \"Gatilho\")\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'PavioSuperior', 'PavioInferior',\n",
    "    'Corpo', 'Range','SMA4','SMA8','SMA12','SMA20', 'SMA50', 'SMA100', 'SMA200', 'StochasticoK',\n",
    "    'StochasticoD', 'RSI', 'MACD', 'MACDSignal', 'MACDHistogram','atr8','atr14','atr28'\n",
    "]\n",
    "data[cols_to_normalize] = scaler.fit_transform(data[cols_to_normalize])\n",
    "\n",
    "# Converter todos os valores para tipo float32 para evitar problemas de tipo\n",
    "data = data.astype({col: 'float32' for col in cols_to_normalize + ['Valor']})\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # 0 = neutro, 1 = comprado, -1 = vendido\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None  # Novo atributo para armazenar o DateTime de entrada\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(len(data.columns) - 3 + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.trades = []  # Lista para armazenar as operações realizadas\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0\n",
    "        self.entry_step = None\n",
    "        self.entry_datetime = None\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step].drop(['Valor', 'DateTime', 'Gatilho']).values\n",
    "        obs = np.append(obs, self.position)  # Incluir a posição atual na observação\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = self.current_step >= len(self.data) - 2  # Ajustado para evitar índice fora do intervalo\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        # Obter o valor atual e o próximo valor\n",
    "        current_price = self.data['Valor'].iloc[self.current_step]\n",
    "        next_price = self.data['Valor'].iloc[self.current_step + 1]\n",
    "        price_change = next_price - current_price\n",
    "\n",
    "        # Obter o valor do gatilho no passo atual\n",
    "        gatilho = int(self.data['Gatilho'].iloc[self.current_step])\n",
    "\n",
    "        # Se o gatilho estiver ativo, o agente pode executar todas as ações\n",
    "        if gatilho == 1:\n",
    "            if action == 1:  # Comprar\n",
    "                if self.position == 0:\n",
    "                    self.position = 1  # Abrir posição comprada\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == -1:\n",
    "                    # Fechar posição vendida\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_short',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            elif action == 2:  # Vender\n",
    "                if self.position == 0:\n",
    "                    self.position = -1  # Abrir posição vendida\n",
    "                    self.entry_price = current_price\n",
    "                    self.entry_step = self.current_step\n",
    "                    self.entry_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward -= 0.25  # Custo de operação\n",
    "                    info['trade'] = {\n",
    "                        'type': 'sell',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime\n",
    "                    }\n",
    "                elif self.position == 1:\n",
    "                    # Fechar posição comprada\n",
    "                    self.exit_price = current_price\n",
    "                    profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                    self.exit_step = self.current_step\n",
    "                    self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                    reward += profit\n",
    "                    info['trade'] = {\n",
    "                        'type': 'close_long',\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    }\n",
    "                    # Registrar a operação\n",
    "                    self.trades.append({\n",
    "                        'type': 'buy',\n",
    "                        'entry_step': self.entry_step,\n",
    "                        'entry_price': self.entry_price,\n",
    "                        'entry_datetime': self.entry_datetime,\n",
    "                        'exit_step': self.exit_step,\n",
    "                        'exit_price': self.exit_price,\n",
    "                        'exit_datetime': self.exit_datetime,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "                    # Resetar posição\n",
    "                    self.position = 0\n",
    "                    self.entry_step = None\n",
    "                    self.entry_datetime = None\n",
    "            else:  # Manter\n",
    "                pass  # Nenhuma ação necessária\n",
    "        else:  # Gatilho == 0, nenhuma posição deve ser mantida\n",
    "            # Se há uma posição aberta, fechá-la\n",
    "            if self.position == 1:  # Fechar posição comprada\n",
    "                self.exit_price = current_price\n",
    "                profit = self.exit_price - self.entry_price - 0.25  # Ganho da posição comprada\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_long',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'buy',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "            elif self.position == -1:  # Fechar posição vendida\n",
    "                self.exit_price = current_price\n",
    "                profit = self.entry_price - self.exit_price - 0.25  # Ganho da posição vendida\n",
    "                self.exit_step = self.current_step\n",
    "                self.exit_datetime = self.data['DateTime'].iloc[self.current_step]\n",
    "                reward += profit\n",
    "                info['trade'] = {\n",
    "                    'type': 'close_short',\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                }\n",
    "                # Registrar a operação\n",
    "                self.trades.append({\n",
    "                    'type': 'sell',\n",
    "                    'entry_step': self.entry_step,\n",
    "                    'entry_price': self.entry_price,\n",
    "                    'entry_datetime': self.entry_datetime,\n",
    "                    'exit_step': self.exit_step,\n",
    "                    'exit_price': self.exit_price,\n",
    "                    'exit_datetime': self.exit_datetime,\n",
    "                    'profit': profit\n",
    "                })\n",
    "                # Resetar posição\n",
    "                self.position = 0\n",
    "                self.entry_step = None\n",
    "                self.entry_datetime = None\n",
    "\n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "# Bloco 3: Criar o Agente DQN usando PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Configurações do dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(data)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Definir a rede DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instanciar a rede\n",
    "q_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net = DQN(obs_size, n_actions).to(device)\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "# Definir o otimizador\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Hiperparâmetros para DQN\n",
    "memory_size = 10000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.99\n",
    "target_update = 10  # Atualizar a rede alvo a cada 10 episódios\n",
    "\n",
    "# Inicializar a memória de replay\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "# Função para selecionar ação usando epsilon-greedy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([0, 1, 2])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "# Bloco 4: Treinamento do Agente DQN com Salvamento dos Melhores Episódios Após Cada Episódio\n",
    "\n",
    "num_episodes = 100  # Defina o número de episódios de treinamento\n",
    "epsilon = epsilon_start\n",
    "best_episodes = []\n",
    "\n",
    "save_dir = \"4.7.8\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    obs = env.reset()\n",
    "    obs = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    actions_count = {0: 0, 1: 0, 2: 0}\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    win_total = 0\n",
    "    lose_total = 0\n",
    "    trades = []\n",
    "    current_trade = None\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        # Selecionar ação\n",
    "        action = select_action(obs, epsilon)\n",
    "\n",
    "        # Executar ação no ambiente\n",
    "        obs_next, reward, done, info = env.step(action)\n",
    "        obs_next = torch.FloatTensor(obs_next).unsqueeze(0).to(device)\n",
    "\n",
    "        # Armazenar na memória de replay\n",
    "        memory.append((obs, action, reward, obs_next, done))\n",
    "\n",
    "        # Atualizar o estado\n",
    "        obs = obs_next\n",
    "\n",
    "        # Atualizar contagem de ações\n",
    "        actions_count[action] += 1\n",
    "\n",
    "        # Processar informações de trade\n",
    "        if 'trade' in info:\n",
    "            trade_info = info['trade']\n",
    "            if trade_info['type'] in ['buy', 'sell']:\n",
    "                # Início de uma nova operação\n",
    "                current_trade = {\n",
    "                    'type': trade_info['type'],\n",
    "                    'entry_step': trade_info['entry_step'],\n",
    "                    'entry_price': trade_info['entry_price'],\n",
    "                    'entry_datetime': trade_info['entry_datetime'],\n",
    "                    'exit_step': None,\n",
    "                    'exit_price': None,\n",
    "                    'exit_datetime': None,\n",
    "                    'profit': None\n",
    "                }\n",
    "            elif trade_info['type'] in ['close_long', 'close_short']:\n",
    "                # Fechamento de uma operação existente\n",
    "                current_trade['exit_step'] = trade_info['exit_step']\n",
    "                current_trade['exit_price'] = trade_info['exit_price']\n",
    "                current_trade['exit_datetime'] = trade_info['exit_datetime']\n",
    "                current_trade['profit'] = trade_info['profit']\n",
    "                trades.append(current_trade.copy())\n",
    "                # Atualizar ganhos e perdas\n",
    "                if current_trade['profit'] > 0:\n",
    "                    wins += 1\n",
    "                    win_total += current_trade['profit']\n",
    "                elif current_trade['profit'] < 0:\n",
    "                    losses += 1\n",
    "                    lose_total += current_trade['profit']\n",
    "                # Atualizar recompensa total\n",
    "                total_reward += current_trade['profit']\n",
    "                current_trade = None\n",
    "\n",
    "        # Treinar a rede se a memória tiver tamanho suficiente\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions_batch, rewards_batch, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.cat(states).to(device)\n",
    "            actions_batch = torch.tensor(actions_batch, dtype=torch.long, device=device).unsqueeze(1)\n",
    "            rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            # Computar Q-valor atual\n",
    "            q_values = q_net(states).gather(1, actions_batch)\n",
    "\n",
    "            # Computar Q-valor alvo usando a rede alvo\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards_batch + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            # Calcular a perda\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "            # Otimizar a rede\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decaimento de epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "    # Atualizar a rede alvo\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Cálculo do tempo de treinamento do episódio\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "\n",
    "    win_rate = wins / (wins + losses) if (wins + losses) > 0 else 0\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Win Rate: {win_rate:.2f}, \"\n",
    "          f\"Wins: {wins}, Losses: {losses}, Epsilon: {epsilon:.4f}, Steps: {steps}, Time: {episode_time:.2f}s\")\n",
    "    print(f\"Ações: Manter={actions_count[0]}, Comprar={actions_count[1]}, Vender={actions_count[2]}\")\n",
    "    print(f\"Ganhos Totais: {win_total:.2f}, Perdas Totais: {lose_total:.2f}\")\n",
    "\n",
    "    # Salvar informações do episódio\n",
    "    episode_info = {\n",
    "        'episode': episode + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'win_rate': win_rate,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'actions_count': actions_count.copy(),\n",
    "        'win_total': win_total,\n",
    "        'lose_total': lose_total,\n",
    "        'steps': steps,\n",
    "        'episode_time': episode_time,\n",
    "        'model_state_dict': q_net.state_dict(),\n",
    "        'trades': trades.copy()  # Salvar as operações do episódio\n",
    "    }\n",
    "\n",
    "    # Adicionar o episódio à lista e manter os top 10\n",
    "    best_episodes.append(episode_info)\n",
    "    best_episodes = sorted(best_episodes, key=lambda x: x['total_reward'], reverse=True)[:10]\n",
    "\n",
    "    # Salvar o modelo e log se o episódio for um dos top 10\n",
    "    if episode_info in best_episodes:\n",
    "        model_path = os.path.join(save_dir, f\"model_episode_{episode_info['episode']}.pth\")\n",
    "        torch.save(episode_info['model_state_dict'], model_path)\n",
    "        episode_info['model_path'] = model_path\n",
    "\n",
    "        # Salvar o log completo das operações\n",
    "        log_path = os.path.join(save_dir, f\"log_episode_{episode_info['episode']}.csv\")\n",
    "        trades_df = pd.DataFrame(episode_info['trades'])\n",
    "        trades_df.to_csv(log_path, index=False)\n",
    "        episode_info['log_path'] = log_path\n",
    "\n",
    "        print(f\"Modelo e log do episódio {episode_info['episode']} salvos em: {model_path} e {log_path}\\n\")\n",
    "\n",
    "print(\"\\nTreinamento finalizado.\")\n",
    "print(\"Top 10 Melhores Episódios:\")\n",
    "for idx, ep in enumerate(best_episodes, 1):\n",
    "    print(f\"Rank {idx}: Episode {ep['episode']}, Total Reward: {ep['total_reward']:.2f}, \"\n",
    "          f\"Win Rate: {ep['win_rate']:.2f}, Wins: {ep['wins']}, Losses: {ep['losses']}, \"\n",
    "          f\"Ações: {ep['actions_count']}, Steps: {ep['steps']}, Time: {ep['episode_time']:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
