{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 15.6095\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 1.2619\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 1.4199\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 1.9803\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 1.2762\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 1.0038\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 1.0587\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 1.2315\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 1.0045\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 0.8387\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Actual",
         "type": "scatter",
         "x": [
          "2024-03-22T00:00:00",
          "2024-03-23T00:00:00",
          "2024-03-24T00:00:00",
          "2024-03-25T00:00:00",
          "2024-03-26T00:00:00",
          "2024-03-27T00:00:00",
          "2024-03-28T00:00:00",
          "2024-03-29T00:00:00",
          "2024-03-30T00:00:00",
          "2024-03-31T00:00:00",
          "2024-04-01T00:00:00",
          "2024-04-02T00:00:00",
          "2024-04-03T00:00:00",
          "2024-04-04T00:00:00",
          "2024-04-05T00:00:00",
          "2024-04-06T00:00:00",
          "2024-04-07T00:00:00",
          "2024-04-08T00:00:00",
          "2024-04-09T00:00:00"
         ],
         "y": [
          -3.4176886209760697,
          -3.002186006678059,
          -3.9257306633912767,
          -4.121757975699242,
          -4.712527794562378,
          -5.012239031890205,
          -3.7153538391635323,
          -2.1857742057703766,
          -1.5163560123607156,
          -0.9676108925823861,
          -0.2909819030265003,
          -0.3032240896329434,
          -0.37888755112000966,
          -1.0525327384960836,
          -1.1084001885461576,
          1.1515467980801037,
          2.0205861273339294,
          1.6784691039070663,
          1.2065424517735448
         ]
        },
        {
         "mode": "lines",
         "name": "Predicted",
         "type": "scatter",
         "x": [
          "2024-03-22T00:00:00",
          "2024-03-23T00:00:00",
          "2024-03-24T00:00:00",
          "2024-03-25T00:00:00",
          "2024-03-26T00:00:00",
          "2024-03-27T00:00:00",
          "2024-03-28T00:00:00",
          "2024-03-29T00:00:00",
          "2024-03-30T00:00:00",
          "2024-03-31T00:00:00",
          "2024-04-01T00:00:00",
          "2024-04-02T00:00:00",
          "2024-04-03T00:00:00",
          "2024-04-04T00:00:00",
          "2024-04-05T00:00:00",
          "2024-04-06T00:00:00",
          "2024-04-07T00:00:00",
          "2024-04-08T00:00:00",
          "2024-04-09T00:00:00"
         ],
         "y": [
          -4.474661350250244,
          -3.653541088104248,
          -3.233538866043091,
          -3.8530592918395996,
          -4.223348140716553,
          -4.671383380889893,
          -5.0096025466918945,
          -4.05344295501709,
          -2.5088486671447754,
          -1.7246973514556885,
          -1.129453182220459,
          -0.5504215359687805,
          -0.55418860912323,
          -0.6924287676811218,
          -1.1414096355438232,
          -1.3816219568252563,
          0.5165267586708069,
          0.707277238368988,
          0.4545385241508484
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Financial Data Prediction"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "title": {
          "text": "Price"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sim some financial data\n",
    "np.random.seed(7)\n",
    "dates = pd.date_range(start='2024-01-01', periods=100)\n",
    "data = np.random.randn(100).cumsum()\n",
    "df = pd.DataFrame(data, columns=['Price'], index=dates)\n",
    "\n",
    "# Prepare dataset for training\n",
    "window_size = 5\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for i in range(window_size, len(df)):\n",
    "    features.append(df.iloc[i-window_size:i, 0])\n",
    "    labels.append(df.iloc[i, 0])\n",
    "\n",
    "features, labels = np.array(features), np.array(labels)\n",
    "\n",
    "# Split the dataset into training & testing\n",
    "split = int(len(features) * 0.8)\n",
    "train_features, test_features = features[:split], features[split:]\n",
    "train_labels, test_labels = labels[:split], labels[split:]\n",
    "\n",
    "# Build a simple LSTM model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(window_size,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_features, train_labels, epochs=10, batch_size=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Visualize the results\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=dates[split+window_size:], y=test_labels, mode='lines', name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=dates[split+window_size:], y=predictions.flatten(), mode='lines', name='Predicted'))\n",
    "\n",
    "fig.update_layout(title='Financial Data Prediction',\n",
    "                                  xaxis_title='Date',\n",
    "                                  yaxis_title='Price')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning:\n",
      "\n",
      "You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 484      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Ação: 1, Recompensa: 0.0\n",
      "Ação: 2, Recompensa: 0.9995340626294591\n",
      "Ação: 0, Recompensa: -0.000465937370540928\n",
      "Ação: 0, Recompensa: -0.000465937370540928\n",
      "Ação: 2, Recompensa: -0.000465937370540928\n",
      "Ação: 2, Recompensa: -0.000465937370540928\n",
      "Ação: 2, Recompensa: -0.000465937370540928\n",
      "Ação: 1, Recompensa: -0.000465937370540928\n",
      "Ação: 2, Recompensa: 1.0005517206351227\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 0, Recompensa: 0.0005517206351225923\n",
      "Ação: 0, Recompensa: 0.0005517206351225923\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 2, Recompensa: 0.0005517206351225923\n",
      "Ação: 1, Recompensa: 0.0005517206351225923\n",
      "Ação: 0, Recompensa: 0.0022024203263090385\n",
      "Ação: 2, Recompensa: 1.0023567558617654\n",
      "Ação: 1, Recompensa: 0.0023567558617653504\n",
      "Ação: 0, Recompensa: 0.004385828082526473\n",
      "Ação: 0, Recompensa: 0.004340442052665821\n",
      "Ação: 2, Recompensa: 1.0028897633535192\n",
      "Ação: 2, Recompensa: 0.0028897633535192426\n",
      "Ação: 2, Recompensa: 0.0028897633535192426\n",
      "Ação: 1, Recompensa: 0.0028897633535192426\n",
      "Ação: 1, Recompensa: 0.0024732890349991976\n",
      "Ação: 0, Recompensa: 0.000988181984590142\n",
      "Ação: 0, Recompensa: 0.003133122247741085\n",
      "Ação: 0, Recompensa: -0.0001690289310705566\n",
      "Ação: 2, Recompensa: 1.0009018297815817\n",
      "Ação: 1, Recompensa: -0.0011625850215395985\n",
      "Ação: 1, Recompensa: -0.0024869037008731993\n",
      "Ação: 0, Recompensa: -0.006099563237672328\n",
      "Ação: 1, Recompensa: -0.0017136363560316568\n",
      "Ação: 2, Recompensa: 1.0053510071556855\n",
      "Ação: 0, Recompensa: 0.004362765899946453\n",
      "Ação: 0, Recompensa: 0.006884965626376925\n",
      "Ação: 0, Recompensa: 0.006345006422619918\n",
      "Ação: 0, Recompensa: 0.008049192084603874\n",
      "Ação: 2, Recompensa: 1.0057906804953396\n",
      "Ação: 2, Recompensa: 1.0023740020890055\n",
      "Ação: 2, Recompensa: 1.0005709034305257\n",
      "Ação: 2, Recompensa: 0.0005709034305257319\n",
      "Ação: 1, Recompensa: 0.0005709034305257319\n",
      "Ação: 2, Recompensa: 1.0008403150610337\n",
      "Ação: 2, Recompensa: 0.000840315061033607\n",
      "Ação: 1, Recompensa: 0.000840315061033607\n",
      "Ação: 0, Recompensa: 0.0010776169076708584\n",
      "Ação: 2, Recompensa: 1.0011790508928826\n",
      "Ação: 2, Recompensa: 0.0011790508928825147\n",
      "Ação: 2, Recompensa: 0.0011790508928825147\n",
      "Ação: 1, Recompensa: 0.0011790508928825147\n",
      "Ação: 2, Recompensa: 0.9997440874268471\n",
      "Ação: 2, Recompensa: -0.0002559125731529548\n",
      "Ação: 0, Recompensa: -0.0002559125731529548\n",
      "Ação: 0, Recompensa: -0.0002559125731529548\n",
      "Ação: 1, Recompensa: -0.0002559125731529548\n",
      "Ação: 2, Recompensa: 0.9978377175483165\n",
      "Ação: 1, Recompensa: -0.0021622824516834955\n",
      "Ação: 2, Recompensa: 0.9995372548450632\n",
      "Ação: 0, Recompensa: -0.0004627451549367834\n",
      "Ação: 1, Recompensa: -0.0004627451549367834\n",
      "Ação: 2, Recompensa: 0.9983436629228051\n",
      "Ação: 2, Recompensa: -0.0016563370771949621\n",
      "Ação: 0, Recompensa: -0.0016563370771949621\n",
      "Ação: 1, Recompensa: -0.0016563370771948485\n",
      "Ação: 1, Recompensa: -0.00015869796159722683\n",
      "Ação: 0, Recompensa: -0.0007239684337480412\n",
      "Ação: 1, Recompensa: -0.0005066716907585942\n",
      "Ação: 1, Recompensa: 0.0038080468714426843\n",
      "Ação: 1, Recompensa: 0.00982132095311033\n",
      "Ação: 0, Recompensa: 0.00875765611802251\n",
      "Ação: 2, Recompensa: 1.010417527195465\n",
      "Ação: 2, Recompensa: 1.0133576335329744\n",
      "Ação: 0, Recompensa: 0.012779067151925005\n",
      "Ação: 0, Recompensa: 0.007445028593049528\n",
      "Ação: 1, Recompensa: 0.009409145704158845\n",
      "Ação: 1, Recompensa: 0.012986554923440963\n",
      "Ação: 0, Recompensa: 0.015064067994930952\n",
      "Ação: 1, Recompensa: 0.010446344711364873\n",
      "Ação: 2, Recompensa: 1.009270180837517\n",
      "Ação: 0, Recompensa: 0.006316331743201431\n",
      "Ação: 2, Recompensa: 1.0048177755565624\n",
      "Ação: 0, Recompensa: 0.010005316327469017\n",
      "Ação: 0, Recompensa: 0.01612363486104164\n",
      "Ação: 2, Recompensa: 1.0188013076346802\n",
      "Ação: 2, Recompensa: 1.0204475429940152\n",
      "Ação: 0, Recompensa: 0.021800800973127026\n",
      "Ação: 0, Recompensa: 0.021776316599914254\n",
      "Ação: 1, Recompensa: 0.021624989676940005\n",
      "Ação: 2, Recompensa: 1.0196040541148117\n",
      "Ação: 2, Recompensa: 1.0194923192147116\n",
      "Ação: 1, Recompensa: 0.021752266201337987\n",
      "Ação: 2, Recompensa: 1.0234903448598456\n",
      "Ação: 2, Recompensa: 1\n"
     ]
    }
   ],
   "source": [
    "# Instalar bibliotecas necessárias (no terminal):\n",
    "# !pip install stable-baselines3 gym pandas numpy\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from gym import spaces\n",
    "\n",
    "# Criar o ambiente personalizado\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000  # Saldo inicial\n",
    "        self.position = 0  # Posição atual (quantidade de ações compradas)\n",
    "        \n",
    "        # Definir espaços de ação e observação\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000\n",
    "        self.position = 0\n",
    "        return [self.df.iloc[self.current_step]]\n",
    "\n",
    "    def step(self, action):\n",
    "        # Aplicar a ação escolhida\n",
    "        current_price = self.df.iloc[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 1:  # Comprar\n",
    "            self.position += 1\n",
    "            self.balance -= current_price\n",
    "        elif action == 2:  # Vender\n",
    "            if self.position > 0:\n",
    "                self.position -= 1\n",
    "                self.balance += current_price\n",
    "                reward = 1  # Recompensa por obter lucro ao vender\n",
    "        \n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "\n",
    "        # Calcular o valor da carteira\n",
    "        total_value = self.balance + self.position * current_price\n",
    "        \n",
    "        # Recompensa baseada no valor da carteira\n",
    "        if not done:\n",
    "            reward += (total_value - 1000) / 1000  # Recompensa relativa ao saldo inicial\n",
    "\n",
    "        # Obter próximo estado\n",
    "        next_state = [self.df.iloc[self.current_step]]\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Simular dados financeiros\n",
    "np.random.seed(7)\n",
    "dates = pd.date_range(start='2024-01-01', periods=100)\n",
    "data = np.random.randn(100).cumsum() + 100\n",
    "df = pd.Series(data, index=dates)\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(df)\n",
    "\n",
    "# Treinar o agente com PPO\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# Testar o agente\n",
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    print(f\"Ação: {action}, Recompensa: {rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0.0\n",
      "Ação: 0, Recompensa: 0\n"
     ]
    }
   ],
   "source": [
    "# Instalar bibliotecas necessárias (no terminal):\n",
    "# !pip install gym pandas numpy torch\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym import spaces\n",
    "\n",
    "# Criar o ambiente personalizado\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000  # Saldo inicial\n",
    "        self.position = 0  # Posição atual (quantidade de ações compradas)\n",
    "        \n",
    "        # Definir espaços de ação e observação\n",
    "        self.action_space = spaces.Discrete(3)  # 0 = Manter, 1 = Comprar, 2 = Vender\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(len(df.columns),), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000\n",
    "        self.position = 0\n",
    "        return self.df.iloc[self.current_step].values  # Retornar todos os indicadores como observação\n",
    "\n",
    "    def step(self, action):\n",
    "        # Aplicar a ação escolhida\n",
    "        current_price = self.df['Price'].iloc[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 1:  # Comprar\n",
    "            self.position += 1\n",
    "            self.balance -= current_price\n",
    "        elif action == 2:  # Vender\n",
    "            if self.position > 0:\n",
    "                self.position -= 1\n",
    "                self.balance += current_price\n",
    "                reward = current_price / 100  # Recompensa proporcional ao preço de venda\n",
    "        \n",
    "        # Atualizar o passo atual\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "\n",
    "        # Calcular o valor da carteira\n",
    "        total_value = self.balance + self.position * current_price\n",
    "        \n",
    "        # Recompensa baseada no valor da carteira\n",
    "        if not done:\n",
    "            reward += (total_value - 1000) / 1000  # Recompensa relativa ao saldo inicial\n",
    "\n",
    "        # Obter próximo estado\n",
    "        next_state = self.df.iloc[self.current_step].values\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Simular dados financeiros com múltiplos indicadores\n",
    "np.random.seed(7)\n",
    "dates = pd.date_range(start='2024-01-01', periods=100)\n",
    "data = np.random.randn(100).cumsum() + 100\n",
    "volume = np.random.randint(100, 200, size=100)\n",
    "ma = pd.Series(data).rolling(window=5).mean().fillna(0)\n",
    "df = pd.DataFrame({'Price': data, 'Volume': volume, 'Moving_Avg': ma}, index=dates)\n",
    "\n",
    "# Criar o ambiente\n",
    "env = TradingEnv(df)\n",
    "\n",
    "# Criar o modelo de rede neural em PyTorch\n",
    "class TradingAgent(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TradingAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.output = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Definir parâmetros do modelo\n",
    "input_dim = len(df.columns)\n",
    "output_dim = 3  # 3 ações possíveis\n",
    "model = TradingAgent(input_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Treinar o agente manualmente\n",
    "num_episodes = 10\n",
    "epsilon = 1.0  # Fator de exploração\n",
    "epsilon_decay = 0.99\n",
    "for episode in range(num_episodes):\n",
    "    obs = torch.tensor(env.reset(), dtype=torch.float32).unsqueeze(0)  # Ajustar a dimensão para (1, input_dim)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done and steps < 200:\n",
    "        # Escolher uma ação usando epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Exploração\n",
    "        else:\n",
    "            q_values = model(obs)\n",
    "            action = torch.argmax(q_values).item()  # Exploração dirigida\n",
    "        \n",
    "        # Executar a ação no ambiente\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_obs = torch.tensor(next_obs, dtype=torch.float32).unsqueeze(0)  # Ajustar a dimensão para (1, input_dim)\n",
    "        \n",
    "        # Atualizar o modelo usando o erro TD\n",
    "        target = reward + 0.95 * torch.max(model(next_obs)).item() if not done else reward\n",
    "        target_f = model(obs).detach().clone()\n",
    "        target_f[0, action] = target\n",
    "        \n",
    "        # Calcular a perda e fazer o backpropagation\n",
    "        output = model(obs)\n",
    "        loss = loss_fn(output, target_f)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        obs = next_obs\n",
    "        steps += 1\n",
    "    \n",
    "    # Decaimento do epsilon\n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "# Testar o agente\n",
    "done = False\n",
    "obs = torch.tensor(env.reset(), dtype=torch.float32).unsqueeze(0)  # Ajustar a dimensão para (1, input_dim)\n",
    "steps = 0\n",
    "while not done and steps < 200:\n",
    "    q_values = model(obs)\n",
    "    action = torch.argmax(q_values).item()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Ajustar a dimensão para (1, input_dim)\n",
    "    print(f\"Ação: {action}, Recompensa: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
