# Testes de Aprendizado por Refor√ßo para Trading Algor√≠timo

# Relat√≥rio do Projeto
https://raspy-cheek-f77.notion.site/1257dc6c82b58007a423ee423dadb8bd?v=1257dc6c82b581d08cc7000cf3c71662

## 1. Introdu√ß√£o

### 1.1 In√≠cio do Projeto e Objetivo

O projeto teve in√≠cio em **20 de outubro de 2024**, com o objetivo de **estudar e desenvolver uma estrat√©gia de trading com aprendizado por refor√ßo em barras de 1 minuto no Nasdaq NQ**. O foco √© criar um agente que aprenda a operar com base em **gatilhos de trade na janela operacional dos primeiros 30 minutos da abertura**. Devido ao tempo de treinamento, os testes foram expandidos para **M5, M15 e M30**, e a janela operacional ajustada para **9:40 √†s 17:00h (ver√£o) e 10:40 √†s 18:00 (inverno) EUA**.

### 1.2 Estrat√©gia de Desenvolvimento

- Implementa√ß√£o de um ambiente de aprendizado por refor√ßo.
- Defini√ß√£o de regras para execu√ß√£o de trades com base em gatilhos.
- Ajustes progressivos de hiperpar√¢metros.
- Testes em diferentes timeframes e an√°lise de desempenho.

Pasta do projeto: **D:\Aprendizado_por_Reforco\TradeM1**

## 2. Detalhes do Ambiente

- **Biblioteca Python:** 3.9.13
- Keras
    
- **Dados Utilizados:** M1 com colunas **Open, High, Low, Close, m√©dias m√≥veis, indicadores t√©cnicos (RSI, MACD, Estoc√°stico, Volume), e vari√°veis personalizadas (PavioSuperior, PavioInferior, Corpo, Delta, etc.)**.
- **Janela Operacional:** O "Gatilho" determina se o agente pode operar. Nos primeiros 30 minutos, o valor √© **1** (opera√ß√µes ativas). Fora desse per√≠odo, o valor √© **0**, encerrando qualquer posi√ß√£o aberta.

Colunas detalhadas:

```python
DateTime,Open,High,Low,Close,Volume,PavioSuperior,PavioInferior,Corpo,Range,D_Open,D_High,D_Low,D_Close,D_PavSup,D_PavInf,D_Corpo,
SMA4,SMA8,SMA12,SMA20,SMA50,SMA100,SMA200,StochasticoK,StochasticoD,RSI,MACD,MACDSignal,MACDHistogram

```

## 3. Agente de Aprendizado por Refor√ßo

- **A√ß√µes:**
    - Com "Gatilho" = **1**, o agente decide entre **comprar, vender ou manter**.
    - Se j√° estiver em opera√ß√£o, decide entre **continuar ou sair**.
    - Com "Gatilho" = **0**, qualquer posi√ß√£o aberta √© encerrada automaticamente.

### 3.1 Sistema de Recompensas

- **Recompensa Base:** Calculada pelo ganho real da opera√ß√£o.
- **Recompensa Extra por Alta Assertividade:** B√¥nus progressivo para sequ√™ncias de opera√ß√µes lucrativas.
- **Penalidade Progressiva por Sequ√™ncia de Erros:** Penaliza m√∫ltiplas perdas consecutivas.
- **Fator de Balanceamento:** Ajusta a rela√ß√£o entre recompensas e penaliza√ß√µes para manter o aprendizado eficiente.
- **Limita√ß√£o Din√¢mica:** O agente pode abrir uma nova opera√ß√£o se a anterior foi lucrativa.

### 3.2 Alternativas de Recompensa Testadas

### 1. **Recompensa do Agente (Usada no Treinamento)**

- Baseada em assertividade (n√∫mero de acertos recentes).
- **Recompensas Extras para Alta Assertividade:** B√¥nus por 3 acertos consecutivos.
- **Penaliza√ß√£o Progressiva:** Penalidades escalonadas para sequ√™ncias de perdas.
- **Fator de Balanceamento:** Ajustado dinamicamente para evitar aprendizado tendencioso.

### 2. **Recompensa Real (Monitoramento do Desempenho)**

- Calculada com base no **ganho real** em pontos.
- **Monitoramento e Avalia√ß√£o:** Serve como m√©trica de refer√™ncia sem influenciar diretamente o aprendizado.

## 4. Regras para Opera√ß√µes

- **Com "Gatilho" = 1:** O agente pode operar.
- **Com "Gatilho" = 0:** O agente deve encerrar todas as posi√ß√µes.

## 5. An√°lise de Resultados

- Melhor desempenho do agente ap√≥s ajustes na penaliza√ß√£o progressiva.
- Ajustes em hiperpar√¢metros **batch_size (32-512), gamma (0.99 a 0.48), learning_rate** melhoraram a estabilidade.
- Timeframes **M15 e M30** mostraram melhor consist√™ncia nos padr√µes.
- Implementa√ß√£o de **Rainbow DQN** ajudou a reduzir a variabilidade das decis√µes.

## 6. Conclus√£o e Pr√≥ximos Passos

üìå **Conclus√£o**: O modelo evoluiu significativamente com ajustes progressivos e penaliza√ß√£o de erros. A transi√ß√£o para **PyTorch + Stable Baselines3** proporcionou maior controle e efici√™ncia.

üîπ **Pr√≥ximos Passos:**

1. Refinamento baseado nos melhores hiperpar√¢metros.
2. Testes com **stop loss din√¢mico**.
3. Avalia√ß√£o e valida√ß√£o do modelo em cen√°rios simulados e reais.

üöÄ O projeto segue para otimiza√ß√£o e testes, visando uma abordagem mais robusta para decis√µes automatizadas no trading.
